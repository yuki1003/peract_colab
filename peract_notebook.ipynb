{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PerAct**\n",
    "\n",
    "**Important: Before starting, change the runtime to GPU.**\n",
    "\n",
    "### Modified\n",
    "This notebook is a modified version of the original `PerAct_Tutorial.ipynb` that explains the training of [Perceiver-Actor (PerAct)](https://peract.github.io/). We will look at training a single-task agent on the `...` task.  The tutorial will start from loading calibrated RGB-D images, and end with visualizing *action detections* in voxelized observations. Overall, this guide is\n",
    "meant to complement the [paper](https://peract.github.io/) by providing concrete implementation details. Moreover, this modified notebook allows for storing the model weights.\n",
    "\n",
    "### Full Code\n",
    "See [this Github repository](https://github.com/peract/peract) for the full code, pre-trained checkpoints, and pre-generated datasets. You should be able to use the pre-generated datasets with this notebook.\n",
    "\n",
    "### Credit\n",
    "This notebook heavily builds on data-loading and pre-preprocessing code from [`ARM`](https://github.com/stepjam/ARM), [`YARR`](https://github.com/stepjam/YARR), [`PyRep`](https://github.com/stepjam/PyRep), [`RLBench`](https://github.com/stepjam/RLBench) by [James et al.](https://stepjam.github.io/) The [PerceiverIO](https://arxiv.org/abs/2107.14795) code is adapted from [`perceiver-pytorch`](https://github.com/lucidrains/perceiver-pytorch) by [Phil Wang (lucidrains)](https://github.com/lucidrains). The optimizer is based on [this LAMB implementation](https://github.com/cybertronai/pytorch-lamb). See the corresponding licenses below.\n",
    "\n",
    "<img src=\"https://peract.github.io/media/figures/sim_task.jpg\" alt=\"drawing\" style=\"width:100px;\"/>\n",
    "\n",
    "### Licenses\n",
    "- [PerAct License (Apache 2.0)](https://github.com/peract/peract/blob/main/LICENSE) - Perceiver-Actor Transformer\n",
    "- [ARM License](https://github.com/peract/peract/blob/main/ARM_LICENSE) - Voxelization and Data Preprocessing\n",
    "- [YARR Licence (Apache 2.0)](https://github.com/stepjam/YARR/blob/main/LICENSE)\n",
    "- [RLBench Licence](https://github.com/stepjam/RLBench/blob/master/LICENSE)\n",
    "- [PyRep License (MIT)](https://github.com/stepjam/PyRep/blob/master/LICENSE)\n",
    "- [Perceiver PyTorch License (MIT)](https://github.com/lucidrains/perceiver-pytorch/blob/main/LICENSE)\n",
    "- [LAMB License (MIT)](https://github.com/cybertronai/pytorch-lamb/blob/master/LICENSE)\n",
    "- [CLIP License (MIT)](https://github.com/openai/CLIP/blob/main/LICENSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab or Kaggle\n",
    "Due to computation constraints of my GPU, some parts have to been run through support of a GPUcloud. Here, we can use [Google Colab](https://colab.research.google.com/) or [Kaggle](https://www.kaggle.com/) to use their GPUs to run training. For this reason, some dependencies or environment variables have to be set in order to make this notebook work. This option is set through the flag `colab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some setting for Colab/Kaggle vs. Computer\n",
    "colab = False\n",
    "data_origin = \"handoversim\" # Choose {\"example\", \"real\", \"handoversim\"}\n",
    "visualize_image = True\n",
    "visualize_voxel = True\n",
    "visualize_pcd = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    !pip install scipy ftfy regex tqdm torch open3d tensorboard natsort transformers git+https://github.com/openai/CLIP.git einops pyrender==0.1.45 trimesh==3.9.34 pycollada==0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Repo and Setup\n",
    "\n",
    "Clone [https://github.com/peract/peract_colab.git](github.com/peract/peract_colab.git), if not cloned yet.\n",
    "\n",
    "This repo contains barebones code from [`ARM`](https://github.com/stepjam/ARM), [`YARR`](https://github.com/stepjam/YARR), [`PyRep`](https://github.com/stepjam/PyRep), [`RLBench`](https://github.com/stepjam/RLBench) to get started with  PerAct without the actual [V-REP](https://www.coppeliarobotics.com/) simulator.\n",
    "\n",
    "The repo also contains a pre-generated RLBench dataset of 10 expert demonstrations for the `open_drawer` task. This task has three variations: \"open the top drawer\", \"open the middle drawer\", and \"open the bottom drawer\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_ # bad trick to fix numpy version issue :(\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "from natsort import natsorted\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set `PYOPENGL_PLATFORM=egl` for pyrender visualizations\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "\n",
    "if not colab:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\" # Depends on your computer and available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Depending on your workspace, you may already have this repository installe, otherwise clone once again\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'peract_colab')):\n",
    "    !git clone https://github.com/yuki1003/peract_colab.git\n",
    "\n",
    "sys.path = [p for p in sys.path if '/peract/' not in p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you fork-off this repo, you might want to pull the latest changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd peract_colab && git pull origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some constants and setting variables.\n",
    "\n",
    "The `BATCH_SIZE` is 1 to fit the model on a single GPU. But you can play around with the voxel sizes and Transformer layers to increase this.  \n",
    "\n",
    "In the paper, we use `NUM_LATENTS=2048` by default, but smaller latents like `512` are also fine (see Appendix G)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WORKSPACE_DIR = os.getcwd()\n",
    "\n",
    "# Replaybuffer related constants\n",
    "LOW_DIM_SIZE = 4    # 4 dimensions - proprioception: {gripper_open, left_finger_joint, right_finger_joint, timestep}\n",
    "IMAGE_SIZE =  128  # 128x128 - if you want to use higher voxel resolutions like 200^3, you might want to regenerate the dataset with larger images\n",
    "DEMO_AUGMENTATION_EVERY_N = 5 # Only select every n-th frame to use for replaybuffer from demo\n",
    "ROTATION_RESOLUTION = 5 # degree increments per axis\n",
    "TARGET_OBJ_KEYPOINTS=False # Real - (changed later)\n",
    "TARGET_OBJ_USE_LAST_KP=False # Real - (changed later)\n",
    "TARGET_OBJ_IS_AVAIL = False # HandoverSim - (changed later)\n",
    "\n",
    "## Paths for collected data & Assigned tasks\n",
    "if data_origin in [\"real\", \"handoversim\"]: ## Custom Data paths of task demos\n",
    "\n",
    "    DEPTH_SCALE = 1000\n",
    "    STOPPING_DELTA = 0.001\n",
    "\n",
    "    # Choose Task\n",
    "    TASK = 'handing_over_banana'\n",
    "    EPISODES_FOLDER = f'{TASK}/all_variations/episodes'\n",
    "    \n",
    "    if colab:\n",
    "        DATA_FOLDER = \"/kaggle/input/peract-task-data\"\n",
    "    else:\n",
    "        if data_origin == \"handoversim\":\n",
    "            DATA_FOLDER = os.path.join(WORKSPACE_DIR, \"task_data\", \"handoversim_v2\") # Change directory\n",
    "            # DATA_FOLDER = \"/media/ywatabe/ESD-USB/task_data/handoversim_v4\"\n",
    "            CAMERAS = [f\"view_{camera_i}\" for camera_i in range(3)]#+ ['wrist']#,'shoulder']#,'wrist']  # TODO: Depends on available cameras from collected data\n",
    "            SCENE_BOUNDS = [0.11, -0.5, 0.8, 1.11, 0.5, 1.8] #NOTE: must be 1m each\n",
    "            TARGET_OBJ_IS_AVAIL = True # NOTE: Object locations are available for HandoverSim\n",
    "        elif data_origin == \"real\":\n",
    "            DATA_FOLDER = os.path.join(WORKSPACE_DIR, \"task_data\", \"real\")\n",
    "            CAMERAS = ['front']\n",
    "            SCENE_BOUNDS = [0.0, -0.5, -0.2, 1., 0.5, 0.8] #NOTE: must be 1m each\n",
    "            TARGET_OBJ_KEYPOINTS=True # TODO: Choose based on task (commonly True)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "\n",
    "elif data_origin in [\"example\"]: ## Running the tutorial data from Colab PerAct_Tutorial\n",
    "    DEPTH_SCALE = 2**24 -1\n",
    "    STOPPING_DELTA = 0.05\n",
    "\n",
    "    # Example Task provided by Tutorial\n",
    "    TASK = 'open_drawer'\n",
    "    EPISODES_FOLDER = f'colab_dataset/{TASK}/all_variations/episodes'\n",
    "\n",
    "    DATA_FOLDER = os.path.join(WORKSPACE_DIR, 'peract_colab', 'data') # Change directory\n",
    "\n",
    "    CAMERAS = ['front', 'left_shoulder', 'right_shoulder', 'wrist'] # TODO: Depends on available cameras from collected data\n",
    "\n",
    "    TARGET_OBJ_USE_LAST_KP=True # TODO: Choose based on task (commonly True)\n",
    "    SCENE_BOUNDS = [-0.3, -0.5, 0.6, 0.7, 0.5, 1.6] # [x_min, y_min, z_min, x_max, y_max, z_max] - the metric volume to be voxelized\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# === Data Handling ===\n",
    "# Construct paths for data processing\n",
    "data_path = os.path.join(DATA_FOLDER, EPISODES_FOLDER)\n",
    "EPISODE_FOLDER = 'episode%d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Settings\n",
    "BATCH_SIZE = 1\n",
    "TRAINING_ITERATIONS = 10000 # 2400\n",
    "LEARNING_RATE = 0.001\n",
    "LR_SCHEDULER = False\n",
    "NUM_WARMUP_STEPS = 300 #FOR LR_SCHEDULER - I tend to see that after ~300 iterations, the losses seems to fluctuate\n",
    "NUM_CYCLES = 1 #TODO: FOR LR_SCHEDULER - same as paper: https://github.com/peract/peract/blob/02fb87681c5a47be9dbf20141bedb836ee2d3ef9/agents/peract_bc/qattention_peract_bc_agent.py#L232\n",
    "TRANSFORM_AUGMENTATION = True\n",
    "RGB_AUGMENTATION = \"None\" #\"partial\" # \"None\", \"full\", \"partial\"\n",
    "FILL_REPLAY_UNIFORM = False\n",
    "VOXEL_SIZES = [100] # 100x100x100 voxels\n",
    "NUM_LATENTS = 512 # PerceiverIO latents (This is a lower-dim space/features capturing the input data)\n",
    "\n",
    "print(\"RUN PROPERTIES\")\n",
    "print(f\"task: {TASK}\")\n",
    "print(f\"transform augmentation: {TRANSFORM_AUGMENTATION}\")\n",
    "print(f\"rgb augmentation: {RGB_AUGMENTATION}\")\n",
    "print(f\"learning_rate: {LEARNING_RATE}\")\n",
    "print(f\"cameras: {CAMERAS}\")\n",
    "print(f\"fill_replay_uniform: {FILL_REPLAY_UNIFORM}\")\n",
    "\n",
    "# === Data Handling ===\n",
    "# Construct paths for data processing\n",
    "data_path = os.path.join(DATA_FOLDER, EPISODES_FOLDER)\n",
    "if data_origin == \"handoversim\":\n",
    "    data_path = os.path.join(DATA_FOLDER, \"train_s1\", EPISODES_FOLDER)\n",
    "EPISODE_FOLDER = 'episode%d'\n",
    "\n",
    "## TRAIN/VALIDATE/TEST - here VALIDATE = TEST & TEST = TEST_TASK\n",
    "TOTAL_DEMOS = len(os.listdir(data_path))\n",
    "## Determine Training/Testing set\n",
    "TRAIN_FRAC = 0.8\n",
    "if data_origin == \"handoversim\":\n",
    "    TRAIN_FRAC = 1\n",
    "SPLIT_POINT = int(TOTAL_DEMOS * TRAIN_FRAC) # #TODO OR CHOOSE FIXED\n",
    "INDEXES = [int(episode_nr.replace(\"episode\",\"\")) for episode_nr in natsorted(os.listdir(data_path))] #np.arange(0, TOTAL_DEMOS)\n",
    "shuffle = False\n",
    "if shuffle:\n",
    "    random_seed = 10\n",
    "    random.Random(random_seed).shuffle(INDEXES) # IF you want to randomise the demos\n",
    "TRAIN_INDEXES, TEST_INDEXES = np.split(INDEXES, [SPLIT_POINT]) # Split demos to train/test sets\n",
    "TEST_INDEXES=TEST_INDEXES\n",
    "\n",
    "print(f\"DEMOS | Total #: {len(INDEXES)}, indexes: {INDEXES}\")\n",
    "print(f\"Split index: {SPLIT_POINT}, shuffle: {shuffle}\")\n",
    "print(f\"TRAIN | Total #: {len(TRAIN_INDEXES)}, indices: {TRAIN_INDEXES}\")\n",
    "print(f\"TEST | Total #: {len(TEST_INDEXES)}, indices:\", TEST_INDEXES)\n",
    "\n",
    "\n",
    "## Testing (USED AT LAST CELL)\n",
    "test_on_diff_set = True\n",
    "TEST_TASK = TASK\n",
    "\n",
    "print(f\"Data TRAIN/TEST: {data_path}\")\n",
    "\n",
    "if not test_on_diff_set:\n",
    "    test_data_path = data_path\n",
    "\n",
    "else: # Choose a different task for testing\n",
    "    test_task = TEST_TASK\n",
    "    test_task_folder = f'{test_task}/all_variations/episodes'    \n",
    "    test_data_path = os.path.join(DATA_FOLDER, \"val_s1\", test_task_folder)\n",
    "    TEST_INDEXES = [int(episode_nr.replace(\"episode\",\"\")) for episode_nr in natsorted(os.listdir(test_data_path))]\n",
    "    print(f\"TEST_DIFF | Total #: {TEST_INDEXES}\")\n",
    "    print(f\"Data TEST (updated): {data_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `peract_colab` to the system path and make a directory for storing the replay buffer.  For now, we will store the replay buffer on disk to avoid memory issues with putting everthing on RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if modules in peract_colab repository are recognized.\n",
    "try: # test import\n",
    "    from rlbench.utils import get_stored_demo\n",
    "except ImportError as error_message:\n",
    "    print(error_message)\n",
    "    print(\"Adding peract_colab repository to system path.\")\n",
    "    sys.path.append('peract_colab')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "train_replay_storage_dir = os.path.join(WORKSPACE_DIR,'replay_train')\n",
    "if os.path.exists(train_replay_storage_dir):\n",
    "    print(f\"Emptying {train_replay_storage_dir}\")\n",
    "    shutil.rmtree(train_replay_storage_dir)\n",
    "if not os.path.exists(train_replay_storage_dir):\n",
    "    print(f\"Could not find {train_replay_storage_dir}, creating directory.\")\n",
    "    os.mkdir(train_replay_storage_dir)\n",
    "\n",
    "test_replay_storage_dir = os.path.join(WORKSPACE_DIR,'replay_test')\n",
    "if os.path.exists(test_replay_storage_dir):\n",
    "    print(f\"Emptying {test_replay_storage_dir}\")\n",
    "    shutil.rmtree(test_replay_storage_dir)\n",
    "if not os.path.exists(test_replay_storage_dir):\n",
    "    print(f\"Could not find {test_replay_storage_dir}, creating directory.\")\n",
    "    os.mkdir(test_replay_storage_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "\n",
    "An expert demonstration recorded at ~20Hz contains 100s of individual timesteps in a sequence. Each timestep contains observations recorded from 4 calibrated cameras (front, left_shoulder, right_shoulder, and wrist) and other proprioception sensors. \"Calibrated\" means we know the extrinsics and intrinsics.\n",
    "\n",
    "Let's take a look at what these observations look like. Play around with different `episode_idx_to_visualize` and timesteps `ts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs\n",
    "\n",
    "if visualize_image:\n",
    "    # what to visualize\n",
    "    episode_idx_to_visualize = INDEXES[0] # out of 10 demos\n",
    "    ts = 21 # timestep out of total timesteps\n",
    "\n",
    "    # get demo\n",
    "    demo = get_stored_demo(data_path=data_path,\n",
    "                        index=episode_idx_to_visualize,\n",
    "                        cameras=CAMERAS,\n",
    "                        depth_scale=DEPTH_SCALE)\n",
    "\n",
    "    # extract obs at timestep\n",
    "    obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "    gripper_pose = demo[ts].gripper_pose\n",
    "    gripper_open = demo[ts].gripper_open\n",
    "\n",
    "    # total timesteps in demo\n",
    "    print(f\"Demo {episode_idx_to_visualize} | {len(demo._observations)} total steps\\n\")\n",
    "    print(f\"The gripper is at: {gripper_pose[:3]}\")\n",
    "    print(f\"gripper_open: {gripper_open}\")\n",
    "\n",
    "    # plot rgb and depth at timestep\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    rows, cols = 2, len(CAMERAS)\n",
    "\n",
    "    plot_idx = 1\n",
    "    for camera in CAMERAS:\n",
    "        # rgb\n",
    "        rgb_name = \"%s_%s\" % (camera, 'rgb')\n",
    "        rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
    "        fig.add_subplot(rows, cols, plot_idx)\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"%s_rgb | step %s\" % (camera, ts))\n",
    "\n",
    "        # depth\n",
    "        depth_name = \"%s_%s\" % (camera, 'depth')\n",
    "        # depth = np.transpose(obs_dict[depth_name], (1, 2, 0)).reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
    "        depth = np.transpose(obs_dict[depth_name], (1, 2, 0))\n",
    "        fig.add_subplot(rows, cols, plot_idx+len(CAMERAS))\n",
    "        plt.imshow(depth)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"%s_depth | step %s\" % (camera, ts))\n",
    "\n",
    "        # # mask\n",
    "        # mask_name = \"%s_%s\" % (camera, 'mask')\n",
    "        # # depth = np.transpose(obs_dict[depth_name], (1, 2, 0)).reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
    "        # mask = np.transpose(obs_dict[mask_name], (1, 2, 0))\n",
    "        # fig.add_subplot(rows, cols, plot_idx+len(CAMERAS))\n",
    "        # plt.imshow(mask)\n",
    "        # plt.axis('off')\n",
    "        # plt.title(\"%s_mask | step %s\" % (camera, ts))\n",
    "\n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Replay Buffer\n",
    "\n",
    "As described in **Section 3.4** of the paper, PerAct is trained with discrete-time input-action tuples from a dataset of demonstrations. These tuples are stored in a Replay Buffer following the [`ARM`](https://github.com/stepjam/ARM) codebase. You can use your own storage format, but here we follow `ARM` to benchmark against baselines and other methods by James et al.\n",
    "\n",
    "This replay buffer stores **<observation, language goal, keyframe action>** tuples sampled from demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arm.replay_buffer import create_replay\n",
    "\n",
    "\n",
    "train_replay_buffer = create_replay(batch_size=BATCH_SIZE,\n",
    "                                    timesteps=1,\n",
    "                                    save_dir=train_replay_storage_dir,\n",
    "                                    cameras=CAMERAS,\n",
    "                                    voxel_sizes=VOXEL_SIZES,\n",
    "                                    image_size=IMAGE_SIZE,\n",
    "                                    low_dim_size=LOW_DIM_SIZE)\n",
    "\n",
    "test_replay_buffer = create_replay(batch_size=BATCH_SIZE,\n",
    "                                   timesteps=1,\n",
    "                                   save_dir=test_replay_storage_dir,\n",
    "                                   cameras=CAMERAS,\n",
    "                                   voxel_sizes=VOXEL_SIZES,\n",
    "                                   image_size=IMAGE_SIZE,\n",
    "                                   low_dim_size=LOW_DIM_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Replay with Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyframe Extraction\n",
    "\n",
    "Instead of directly trying to predict every action in the demonstration, which could be very noisy and inefficient, we extract keyframe actions that capture **bottleneck** poses \\[[James et al.](https://arxiv.org/abs/2105.14829)\\]. This extraction is done with a simple heuristic: an action is a keyframe action if (1) the joint-velocities are near zero and (2) the gripper open state has not changed. Then every timestep in the demonstration can be cast as a predict \"the next (best) keyframe\" classification task, like the orange points in this figure:  \n",
    "\n",
    "<div>\n",
    "<img src=\"https://peract.github.io/media/figures/keypoints.jpg\" alt=\"drawing\"  width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what these keyframe actions look like.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arm.demo import _keypoint_discovery, _keypoint_discovery_available, _target_object_discovery\n",
    "\n",
    "# Display for every demo (i.e. episode)\n",
    "# Display Demo 1 like last time\n",
    "if visualize_image:\n",
    "    for i in INDEXES:\n",
    "        episode_idx_to_visualize = i#INDEXES#746#INDEXES[0]#746#146#INDEXES[0]\n",
    "\n",
    "        demo = get_stored_demo(data_path=data_path,\n",
    "                                index=episode_idx_to_visualize,\n",
    "                                cameras=CAMERAS,\n",
    "                                depth_scale=DEPTH_SCALE)\n",
    "\n",
    "        # total timesteps\n",
    "        print(\"Demo %s | %s total steps\" % (episode_idx_to_visualize, len(demo._observations)))\n",
    "\n",
    "        # use the heuristic to extract keyframes (aka keypoints) NOTE: the absolute-difference (per joint) that was at STOPPING_DELTA.\n",
    "        \n",
    "\n",
    "        if data_origin in [\"example\"]:\n",
    "            episode_keypoints = _keypoint_discovery(demo, stopping_delta=STOPPING_DELTA)\n",
    "            episode_target_object = _target_object_discovery(demo)\n",
    "        elif data_origin in [\"real\"]:\n",
    "            episode_target_object = _target_object_discovery(demo, keypoints=TARGET_OBJ_KEYPOINTS, stopping_delta=STOPPING_DELTA, last_kp=TARGET_OBJ_USE_LAST_KP)\n",
    "        elif data_origin in [\"handoversim\"]:\n",
    "            episode_keypoints = _keypoint_discovery(demo, stopping_delta=STOPPING_DELTA)#_keypoint_discovery_available(demo, 0.3)\n",
    "            episode_target_object = _target_object_discovery(demo, is_available=True)\n",
    "\n",
    "        # visualize rgb observations from these keyframes\n",
    "        for kp_idx, kp in enumerate(episode_keypoints):\n",
    "            \n",
    "            obs_dict = extract_obs(demo._observations[kp], CAMERAS, t=kp)\n",
    "            gripper_pose = demo[kp].gripper_pose\n",
    "            gripper_open = demo[kp].gripper_open\n",
    "            target_object = episode_target_object[kp]\n",
    "\n",
    "            # plot rgb and depth at timestep\n",
    "            fig = plt.figure(figsize=(10, 5))\n",
    "            rows, cols = 1, len(CAMERAS)\n",
    "\n",
    "            plot_idx = 1\n",
    "\n",
    "            for camera in CAMERAS:\n",
    "\n",
    "                rgb_name = \"%s_%s\" % (camera, 'rgb')\n",
    "                rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
    "                fig.add_subplot(rows, cols, plot_idx)\n",
    "                plt.imshow(rgb)\n",
    "                plt.axis('off')\n",
    "                fig.suptitle(\"step %s | \\n gripper at %s \\n gripper open %s \\n object at %s\" % (kp, gripper_pose[:3], gripper_open, target_object[:3]))\n",
    "\n",
    "                plot_idx += 1\n",
    "            \n",
    "            fig.tight_layout()\n",
    "            fig.subplots_adjust(top=0.88)\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the motion-planner used to generate demonstrations might take various paths to execute the \"opening\" motion, but all paths strictly pass through these **bottleneck** poses, since that's how the expert demonstrations were collected in RLBench. This essentially circuments the issue of training directly on randomized motion paths from sampling-based motion planners, which can be quite noisy to learn from for end-to-end methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the replay buffer\n",
    "\n",
    "Load a [pre-trained CLIP model](https://arxiv.org/abs/2103.00020) to extract language features. You can probably swap this with other language models, but CLIP's language features were trained to be aligned with image features, which might give it a multi-modal edge over text-only models 🤷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally fill the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "from arm.replay_buffer import fill_replay, uniform_fill_replay\n",
    "from yarr.replay_buffer.wrappers.pytorch_replay_buffer import PyTorchReplayBuffer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"RN50\", device=device) # CLIP-ResNet50\n",
    "\n",
    "# if FILL_REPLAY_UNIFORM:\n",
    "#     fill_replay = uniform_fill_replay\n",
    "\n",
    "print(\"-- Train Buffer --\")\n",
    "fill_replay( # fill_replay_copy_with_crop_from_approach\n",
    "            data_path=data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=train_replay_buffer,\n",
    "            # start_idx=0,\n",
    "            # num_demos=NUM_DEMOS,\n",
    "            d_indexes=TRAIN_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=True,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    \n",
    "print(\"-- Test Buffer --\")\n",
    "fill_replay( # fill_replay_copy_with_crop_from_approach\n",
    "            data_path=test_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=test_replay_buffer,\n",
    "            # start_idx=start_idx,\n",
    "            # num_demos=num_demos,\n",
    "            d_indexes=TEST_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=True,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "\n",
    "# delete the CLIP model since we have already extracted language features\n",
    "del clip_model\n",
    "\n",
    "# wrap buffer with PyTorch dataset and make iterator\n",
    "train_wrapped_replay = PyTorchReplayBuffer(train_replay_buffer)\n",
    "train_dataset = train_wrapped_replay.dataset()\n",
    "train_data_iter = iter(train_dataset)\n",
    "\n",
    "test_wrapped_replay = PyTorchReplayBuffer(test_replay_buffer)\n",
    "test_dataset = test_wrapped_replay.dataset()\n",
    "test_data_iter = iter(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training PerAct\n",
    "\n",
    "### Voxelization\n",
    "\n",
    "Now we define a class for voxelizing calibrated RGB-D observations following [C2FARM \\(James et al.\\)](https://arxiv.org/pdf/2106.12534.pdf)\n",
    "\n",
    "The input to the voxelizer is:\n",
    "- Flattened RGB images\n",
    "- Flattened global-coordinate point clouds\n",
    "- Scene bounds in metric units that specify the volume to be voxelized\n",
    "\n",
    "The output is a 10-dimensional voxel grid (see Appendix B for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use this voxelizer on observation samples from the replay buffer.\n",
    "\n",
    "But first, lets define some helper functions to normalize and format RGB and pointcloud input:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rgb and pointcloud inputs have to be flattened before feeding them into the voxelizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voxel: Original (no-augmentation vs. translation/rotation-augmentation vs. RGB-augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from agent.utils import _preprocess_inputs, pcd_bbox\n",
    "from agent.voxel_grid import VoxelGrid\n",
    "from arm.utils import visualise_voxel, point_to_voxel_index, voxel_index_to_point\n",
    "from arm.augmentation import apply_se3_augmentation, perturb_se3\n",
    "\n",
    "RGB_AUGMENTATION = \"None\"\n",
    "\n",
    "if visualize_voxel or visualize_pcd:\n",
    "\n",
    "    # initialize voxelizer\n",
    "    vox_grid = VoxelGrid(\n",
    "        coord_bounds=SCENE_BOUNDS,\n",
    "        voxel_size=VOXEL_SIZES[0],\n",
    "        device=device,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        feature_size=3,\n",
    "        max_num_coords=np.prod([IMAGE_SIZE, IMAGE_SIZE]) * len(CAMERAS),\n",
    "    )\n",
    "\n",
    "    # sample from dataset\n",
    "    batch = next(train_data_iter)\n",
    "    lang_goal = batch['lang_goal'][0][0][0]\n",
    "    print(batch[\"view_0_rgb\"].shape)\n",
    "    batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "    print(batch[\"view_0_rgb\"].shape)\n",
    "\n",
    "    # preprocess observations\n",
    "    rgbs_pcds, _ = _preprocess_inputs(batch, CAMERAS)\n",
    "    pcds = [rp[1] for rp in rgbs_pcds]\n",
    "\n",
    "    # batch_size\n",
    "    bs = rgbs_pcds[0][0].shape[0]\n",
    "\n",
    "    # metric scene bounds\n",
    "    bounds = torch.tensor(SCENE_BOUNDS,device=device).unsqueeze(0)\n",
    "\n",
    "    # identity matrix\n",
    "    identity_4x4 = torch.eye(4).unsqueeze(0).repeat(bs, 1, 1).to(device=device)\n",
    "    \n",
    "    # sample\n",
    "    action_trans = batch['trans_action_indicies'][:, -1, :3].int()\n",
    "    action_rot_grip = batch['rot_grip_action_indicies'][:, -1].int()\n",
    "    action_ignore_collisions = batch['ignore_collisions'][:, -1].int()\n",
    "    action_gripper_pose = batch['gripper_pose'][:, -1]\n",
    "    gripper_pose = batch['gripper_state'][:, -1]\n",
    "    object_pose = batch['object_state'][:, -1]\n",
    "    lang_goal_embs = batch['lang_goal_embs'][:, -1].float()\n",
    "\n",
    "    # Get the pose voxel location\n",
    "    gripper_pcd = pcd_bbox(gripper_pose, 1, VOXEL_SIZES[0], bounds, bs, device)\n",
    "    object_pcd = pcd_bbox(object_pose, 1, VOXEL_SIZES[0], bounds, bs, device)\n",
    "    \n",
    "    if RGB_AUGMENTATION.lower() == \"partial\":\n",
    "        \n",
    "        gripper_bbox_pcd = pcd_bbox(gripper_pose, 10, VOXEL_SIZES[0], bounds, bs, device)\n",
    "        object_bbox_pcd = pcd_bbox(object_pose, 10, VOXEL_SIZES[0], bounds, bs, device)\n",
    "        \n",
    "    if TRANSFORM_AUGMENTATION:\n",
    "        action_trans, \\\n",
    "        action_rot_grip, \\\n",
    "        pcds, \\\n",
    "        trans_shift_4x4, \\\n",
    "        rot_shift_4x4, \\\n",
    "        action_gripper_4x4 = apply_se3_augmentation(pcds,\n",
    "                                                    action_gripper_pose,\n",
    "                                                    action_trans,\n",
    "                                                    action_rot_grip,\n",
    "                                                    bounds,\n",
    "                                                    0,  \n",
    "                                                    [0.125,0.125,0.125],#[0.125,0.125,0.125],#[0.125,0.125,0.125],#[0.125, 0.125, 0.125], # Translation Range\n",
    "                                                    [0.0,0.0,45.0],#[0.0, 0.0, 45.0], # Rotation Range # was [0.0, 0.0, 45.0]\n",
    "                                                    5, # Increments of rotation\n",
    "                                                    VOXEL_SIZES[0],\n",
    "                                                    ROTATION_RESOLUTION, # Resolution to fix to\n",
    "                                                    device)\n",
    "        \n",
    "        gripper_pcd = perturb_se3(gripper_pcd, trans_shift_4x4, rot_shift_4x4, action_gripper_4x4, bounds)\n",
    "        object_pcd = perturb_se3(object_pcd, trans_shift_4x4, rot_shift_4x4, action_gripper_4x4, bounds)\n",
    "        \n",
    "        if RGB_AUGMENTATION.lower() == \"partial\":\n",
    "            gripper_bbox_pcd = perturb_se3(gripper_bbox_pcd, trans_shift_4x4, rot_shift_4x4, action_gripper_4x4, bounds)\n",
    "            object_bbox_pcd = perturb_se3(object_bbox_pcd, trans_shift_4x4, rot_shift_4x4, action_gripper_4x4, bounds)\n",
    "\n",
    "    # flatten observations\n",
    "    pcd_flat = torch.cat([p.permute(0, 2, 3, 1).reshape(bs, -1, 3) for p in pcds], 1)\n",
    "\n",
    "    rgb = [rp[0] for rp in rgbs_pcds] # Loop per camera\n",
    "    \n",
    "    feat_size = rgb[0].shape[1]\n",
    "    flat_imag_features = torch.cat(\n",
    "        [p.permute(0, 2, 3, 1).reshape(bs, -1, feat_size) for p in rgb], 1)\n",
    "    \n",
    "    # voxelize!\n",
    "    voxel_grid = vox_grid.coords_to_bounding_voxel_grid(pcd_flat,\n",
    "                                                        flat_imag_features,\n",
    "                                                        coord_bounds=bounds)\n",
    "    # swap to channels fist\n",
    "    voxel_grid = voxel_grid.permute(0, 4, 1, 2, 3).detach()#.cpu().numpy() #(B, (RGB, point_robot_cs, occupancy, position_index), V_depth, V_height, V_width)\n",
    "    \n",
    "    \n",
    "    if RGB_AUGMENTATION.lower() in [\"full\", \"partial\"]:\n",
    "        transform = T.Compose([\n",
    "            T.ColorJitter(brightness=10.2, contrast=10.2, saturation=10.2, hue=0.1),  # Create transformation for brightness/contrast/saturation/hue\n",
    "        ])\n",
    "        image_features_scaled = torch.cat([(rgb_batch + 1) / 2 for rgb_batch in rgb]) # Conversion [-1,1] -> [0, 1] & Split batches\n",
    "        image_features_scaled_augmented = transform(image_features_scaled) # Do transformation\n",
    "        rgb_unscaled = image_features_scaled_augmented * 2 - 1\n",
    "\n",
    "        rgb_augmented = [rgb_unscaled[i:i + bs] for i in range(0, len(rgb_unscaled), bs)] # Merge back to batches\n",
    "        flat_imag_features_augmented = torch.cat(\n",
    "            [p.permute(0, 2, 3, 1).reshape(bs, -1, feat_size) for p in rgb_augmented], 1)\n",
    "        \n",
    "        voxel_grid_augmented = vox_grid.coords_to_bounding_voxel_grid(pcd_flat,\n",
    "                                                                    coord_features=flat_imag_features_augmented,\n",
    "                                                                    coord_bounds=bounds)\n",
    "\n",
    "        # swap to channels fist\n",
    "        voxel_grid_augmented = voxel_grid_augmented.permute(0, 4, 1, 2, 3).detach()#.cpu().numpy() #(B, (point_robot_cs, RGB, occupancy, position_index), V_depth, V_height, V_width) NOTE: ORDER IS DIFFERENT?\n",
    "        \n",
    "        if RGB_AUGMENTATION.lower() == \"partial\":\n",
    "\n",
    "            bounding_box_pcd = [torch.cat((gripper_bbox_pcd[0], object_bbox_pcd[0]), dim=2)]\n",
    "            bounding_box_pcd_flat = torch.cat([p.permute(0, 2, 3, 1).reshape(bs, -1, 3) for p in bounding_box_pcd], 1)\n",
    "            \n",
    "            bounding_box_indices = []\n",
    "            for b, bounding_box_pcd_flat_b in enumerate(bounding_box_pcd_flat):\n",
    "                bounding_box_indices_b = point_to_voxel_index(bounding_box_pcd_flat_b.cpu().numpy(),\n",
    "                                                            VOXEL_SIZES[0],\n",
    "                                                            bounds[0].cpu().numpy())\n",
    "                bounding_box_indices.append(bounding_box_indices_b)\n",
    "                voxel_grid_augmented[b, 3:6, \n",
    "                                        bounding_box_indices_b[:, 0], \n",
    "                                        bounding_box_indices_b[:, 1], \n",
    "                                        bounding_box_indices_b[:, 2]] = voxel_grid[b, 3:6, \n",
    "                                                                                bounding_box_indices_b[:, 0], \n",
    "                                                                                bounding_box_indices_b[:, 1], \n",
    "                                                                                bounding_box_indices_b[:, 2]]\n",
    "\n",
    "    # expert action voxel indicies and coord\n",
    "    vis_gt_coords = action_trans[:, :3].int().detach().cpu().numpy()\n",
    "    gt_coord_pcd = np.array([voxel_index_to_point(vis_gt_coord, VOXEL_SIZES[0], bounds[0].cpu().numpy()) \n",
    "                             for vis_gt_coord in vis_gt_coords])\n",
    "\n",
    "    # Current gripper and (estimated) target object location\n",
    "    # gripper_object_pcd = [torch.cat((gripper_pcd[0], object_pcd[0]), dim=2)]\n",
    "    gripper_object_pcd = [object_pcd[0]]\n",
    "    gripper_object_pcd_flat = torch.cat([p.permute(0, 2, 3, 1).reshape(bs, -1, 3) for p in gripper_object_pcd], 1)\n",
    "    \n",
    "    gripper_object_indices = []\n",
    "    for b, gripper_object_pcd_flat_b in enumerate(gripper_object_pcd_flat):\n",
    "        gripper_object_indices_b = point_to_voxel_index(gripper_object_pcd_flat_b.cpu().numpy(),\n",
    "                                                    VOXEL_SIZES[0],\n",
    "                                                    bounds[0].cpu().numpy())\n",
    "        gripper_object_indices.append(gripper_object_indices_b)\n",
    "\n",
    "    for b in range(bs):\n",
    "        print(f\"Expert Action Voxel Indices: {vis_gt_coords[b]}\")\n",
    "        gripper_object_indices_b = gripper_object_indices[b]\n",
    "        idx_gripper_object = int(len(gripper_object_indices_b)/2)\n",
    "        print(f\"Gripper Voxel Indices: {gripper_object_indices_b[:idx_gripper_object]}\")\n",
    "        print(f\"Object (estimated) Voxel Indices: {gripper_object_indices_b[idx_gripper_object:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing voxel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not colab and visualize_voxel: # Does NOT work within colab\n",
    "    # render voxel grid with expert action (blue)\n",
    "    #@markdown #### Show voxel grid and expert action (blue)\n",
    "    #@markdown Adjust `rotation_amount` to change the camera yaw angle for rendering.\n",
    "    # rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "    gripper_pose_indices = [None]        \n",
    "    # gripper_pose_indices = [gripper_object_indices[0]] if RGB_AUGMENTATION == \"full\" else [bounding_box_indices[0]] # bounding box\n",
    "\n",
    "    angle = 0\n",
    "\n",
    "    rendered_img_0 = visualise_voxel(voxel_grid[0].cpu().numpy(),\n",
    "                                None,\n",
    "                                gripper_pose_indices[0],\n",
    "                                vis_gt_coords[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=0.03,\n",
    "                                rotation_amount=np.deg2rad(0-angle),\n",
    "                                perspective = False)\n",
    "\n",
    "    rendered_img_90 = visualise_voxel(voxel_grid[0].cpu().numpy(),\n",
    "                                None,\n",
    "                                gripper_pose_indices[0],\n",
    "                                vis_gt_coords[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=0.03,\n",
    "                                rotation_amount=np.deg2rad(90-angle),\n",
    "                                perspective = False)\n",
    "    \n",
    "    rendered_img_180 = visualise_voxel(voxel_grid[0].cpu().numpy(),\n",
    "                                None,\n",
    "                                gripper_pose_indices[0],\n",
    "                                vis_gt_coords[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=0.03,\n",
    "                                rotation_amount=np.deg2rad(180-angle),\n",
    "                                perspective = False)\n",
    "    \n",
    "    rendered_img_270 = visualise_voxel(voxel_grid[0].cpu().numpy(),\n",
    "                                None,\n",
    "                                gripper_pose_indices[0],\n",
    "                                vis_gt_coords[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=0.03,\n",
    "                                rotation_amount=np.deg2rad(270-angle),\n",
    "                                perspective = False)\n",
    "    \n",
    "    rendered_img_0_persp = visualise_voxel(voxel_grid[0].cpu().numpy(),\n",
    "                                None,\n",
    "                                gripper_pose_indices[0],\n",
    "                                vis_gt_coords[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=0.03,\n",
    "                                rotation_amount=np.deg2rad(0-angle))\n",
    "    \n",
    "    rendered_img_side_persp = visualise_voxel(voxel_grid[0].cpu().numpy(),\n",
    "                                None,\n",
    "                                gripper_pose_indices[0],\n",
    "                                vis_gt_coords[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=0.03,\n",
    "                                rotation_amount=np.deg2rad(45-angle))\n",
    "    \n",
    "                    \n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(3, 2, 1)\n",
    "    plt.imshow(rendered_img_0)\n",
    "    plt.title(\"0-degree view\")\n",
    "    fig.add_subplot(3, 2, 2)\n",
    "    plt.imshow(rendered_img_90)\n",
    "    plt.title(\"90-degree view\")\n",
    "    fig.add_subplot(3, 2, 3)\n",
    "    plt.imshow(rendered_img_180)\n",
    "    plt.title(\"180-degree view\")\n",
    "    fig.add_subplot(3, 2, 4)\n",
    "    plt.imshow(rendered_img_270)\n",
    "    plt.title(\"270-degree view\")\n",
    "    fig.add_subplot(3, 2, 5)\n",
    "    plt.imshow(rendered_img_0_persp)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"00-degree view\")\n",
    "    fig.add_subplot(3, 2, 6)\n",
    "    plt.imshow(rendered_img_side_persp)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"side view\")\n",
    "\n",
    "    print(f\"Lang goal: {lang_goal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not colab and visualize_voxel and (RGB_AUGMENTATION.lower() in [\"full\", \"partial\"]): # Does NOT work within colab\n",
    "    # render voxel grid with expert action (blue)\n",
    "    #@markdown #### Show voxel grid and expert action (blue)\n",
    "    #@markdown Adjust `rotation_amount` to change the camera yaw angle for rendering.\n",
    "    # rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "    gripper_pose_indices = [None]\n",
    "    # gripper_pose_indices = [gripper_object_indices[0]] # gripper and object location highlights\n",
    "\n",
    "    rendered_img_0 = visualise_voxel(voxel_grid_augmented[0].cpu().numpy(),\n",
    "                                None,\n",
    "                                gripper_pose_indices[0],\n",
    "                                vis_gt_coords[0],\n",
    "                                # highlight_alpha=0.3,\n",
    "                                voxel_size=0.03,\n",
    "                                rotation_amount=np.deg2rad(0))\n",
    "\n",
    "    rendered_img_side = visualise_voxel(voxel_grid_augmented[0].cpu().numpy(),\n",
    "                                None,\n",
    "                                gripper_pose_indices[0],\n",
    "                                vis_gt_coords[0],\n",
    "                                # highlight_alpha=0.3,\n",
    "                                voxel_size=0.03,\n",
    "                                rotation_amount=np.deg2rad(45))\n",
    "                    \n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(rendered_img_0)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Front view\")\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(rendered_img_side)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Side view\")\n",
    "\n",
    "    print(f\"Lang goal: {lang_goal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_pcd:\n",
    "\n",
    "    import open3d as o3d\n",
    "    \n",
    "    def create_geometry_at_points(points, radius = 0.03):\n",
    "        geometries = o3d.geometry.TriangleMesh()\n",
    "        for point in points:\n",
    "            sphere = o3d.geometry.TriangleMesh.create_sphere(radius=radius) #create a small sphere to represent point\n",
    "            sphere.translate(point) #translate this sphere to point\n",
    "            geometries += sphere\n",
    "        geometries.paint_uniform_color([1.0, 1.0, 0.0])\n",
    "        return geometries\n",
    "\n",
    "    def points_in_scene_bounds(points, scene_bounds=np.array(SCENE_BOUNDS)):\n",
    "        x_min, y_min, z_min, x_max, y_max, z_max = scene_bounds\n",
    "        mask = (\n",
    "            (points[:, 0] >= x_min) & (points[:, 0] <= x_max) &  # x bounds\n",
    "            (points[:, 1] >= y_min) & (points[:, 1] <= y_max) &  # y bounds\n",
    "            (points[:, 2] >= z_min) & (points[:, 2] <= z_max)    # z bounds\n",
    "        )\n",
    "        return points[mask]\n",
    "    \n",
    "    # General point cloud from camera(s)\n",
    "    point_cloud_o3d = o3d.geometry.PointCloud()\n",
    "    pcd_np = pcd_flat[0].cpu().numpy()\n",
    "    pcd_np_filtered = points_in_scene_bounds(pcd_np)\n",
    "    point_cloud_o3d.points = o3d.utility.Vector3dVector(pcd_np_filtered)\n",
    "\n",
    "    # Ground truth to next action\n",
    "    gt_coord_cloud_o3d = o3d.geometry.PointCloud()\n",
    "    gt_coord_cloud_o3d.points = o3d.utility.Vector3dVector(gt_coord_pcd)\n",
    "    gt_coord_o3d = create_geometry_at_points(gt_coord_cloud_o3d.points)\n",
    "\n",
    "    # Gripper and object location\n",
    "    gripper_object_coord_cloud_o3d = o3d.geometry.PointCloud()\n",
    "    gripper_object_pcd_np = gripper_object_pcd_flat[0].cpu().numpy()\n",
    "    gripper_object_coord_cloud_o3d.points = o3d.utility.Vector3dVector(np.array(gripper_object_pcd_np))\n",
    "    gripper_object_o3d = create_geometry_at_points(gripper_object_coord_cloud_o3d.points)\n",
    "\n",
    "    # Gripper and object bbox point cloud\n",
    "    gripper_object_bbox_cloud_o3d = o3d.geometry.PointCloud()\n",
    "    # gripper_object_bbox_pcd_np = bounding_box_pcd_flat[0].cpu().numpy()\n",
    "    # gripper_object_coord_cloud_o3d.points = o3d.utility.Vector3dVector(gripper_object_bbox_pcd_np)\n",
    "    # gripper_object_coord_cloud_o3d.colors = o3d.utility.Vector3dVector(gripper_object_bbox_pcd_np)\n",
    "\n",
    "    # Used for fixed axis\n",
    "    fixed_axis_cloud_o3d = o3d.geometry.PointCloud()\n",
    "    fixed_axis_pcd_np = np.array([[-2,-2,-2],\n",
    "                                  [2, 2, 2]])\n",
    "    fixed_axis_cloud_o3d.points = o3d.utility.Vector3dVector(fixed_axis_pcd_np)\n",
    "\n",
    "    o3d.visualization.draw_plotly([point_cloud_o3d,\n",
    "                                   gt_coord_o3d,\n",
    "                                #    gripper_object_coord_cloud_o3d,\n",
    "                                #    gripper_object_o3d,\n",
    "                                   fixed_axis_cloud_o3d,\n",
    "                                   ])\n",
    "\n",
    "    # # camera position: 0.3732, -0.7225, 0.4119"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization shows a voxel grid of size 100x100x100 = 1 million voxels, and one expert keyframe action (blue voxel). These samples are what PerAct is trained with. Given a language goal and voxel grid, we train a detector to detect the next best action with supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PerceiverIO\n",
    "\n",
    "Now we can start implementing the actual Transformer backbone of PerAct from **Section 3.3**.\n",
    "\n",
    "The input grid is 100×100×100 = 1 million voxels. If we extract 5×5×5 patches, the input is still 20×20×20 = 8000 embeddings long. This sequence is way too long for a standard Transformer with O(n^2) self-attention connections. So we use the [PerceiverIO architecture](https://arxiv.org/abs/2107.14795) instead.  \n",
    "\n",
    "Perceiver uses a small set of **latent vectors** to encode the input. These latent vectors are randomly initialized and trained end-to-end. This approach decouples the depth of the Transformer self-attention layers from the dimensionality of the input space, which allows us train PerAct on very large input voxel grids. We can potentially scale the input to 200^3 voxels without increasing self-attention layer parameters.\n",
    "\n",
    "Refer to **Appendix B** in the paper for additional details.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://peract.github.io/media/figures/perceiver.png\" alt=\"drawing\"  width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PerceiverIO Transformer\n",
    "from agent.perceiver_io import PerceiverIO\n",
    "\n",
    "\n",
    "perceiver_encoder = PerceiverIO(\n",
    "    depth=6,\n",
    "    iterations=1,\n",
    "    voxel_size=VOXEL_SIZES[0],\n",
    "    initial_dim=3 + 3 + 1 + 3,\n",
    "    low_dim_size=4,\n",
    "    layer=0,\n",
    "    num_rotation_classes=72,\n",
    "    num_grip_classes=2,\n",
    "    num_collision_classes=2,\n",
    "    num_latents=NUM_LATENTS,\n",
    "    latent_dim=512,\n",
    "    cross_heads=1,\n",
    "    latent_heads=8,\n",
    "    cross_dim_head=64,\n",
    "    latent_dim_head=64,\n",
    "    weight_tie_layers=False,\n",
    "    activation='lrelu',\n",
    "    input_dropout=0.1,\n",
    "    attn_dropout=0.1,\n",
    "    decoder_dropout=0.0,\n",
    "    voxel_patch_size=5,\n",
    "    voxel_patch_stride=5,\n",
    "    final_dim=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Functions\n",
    "\n",
    "Finally we put everything together to make PerAct's Q-Functions.  \n",
    "\n",
    "This module voxelizes RGB-D input, encodes per-voxel features, and predicts discretized actions.  \n",
    "\n",
    "<div>\n",
    "<img src=\"https://peract.github.io/media/figures/arch.png\" alt=\"drawing\"  width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PerAct Agent\n",
    "\n",
    "Let's initialize PerAct and define an update function for the training loop.\n",
    "\n",
    "The keyframe actions used for supervision are represented as one-hot vectors. Then we use cross-entropy loss to train PerAct, just like a standard classifier. This training method is also closely related to [Energy-Based Models (EBMs)](https://arxiv.org/abs/2109.00137)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PerceiverActor\n",
    "from agent.peract_agent import PerceiverActorAgent\n",
    "\n",
    "\n",
    "peract_agent = PerceiverActorAgent(\n",
    "    coordinate_bounds=SCENE_BOUNDS,\n",
    "    perceiver_encoder=perceiver_encoder,\n",
    "    camera_names=CAMERAS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    voxel_size=VOXEL_SIZES[0],\n",
    "    voxel_feature_size=3,\n",
    "    num_rotation_classes=72,\n",
    "    rotation_resolution=ROTATION_RESOLUTION,\n",
    "    training_iterations = TRAINING_ITERATIONS,\n",
    "    lr=LEARNING_RATE,\n",
    "    lr_scheduler=LR_SCHEDULER,\n",
    "    num_warmup_steps = NUM_WARMUP_STEPS,\n",
    "    num_cycles = NUM_CYCLES,\n",
    "    image_resolution=[IMAGE_SIZE, IMAGE_SIZE],\n",
    "    lambda_weight_l2=0.000001,\n",
    "    transform_augmentation=TRANSFORM_AUGMENTATION,\n",
    "    rgb_augmentation=RGB_AUGMENTATION,\n",
    "    optimizer_type='lamb',\n",
    ")\n",
    "peract_agent.build(training=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "The final training loop samples data from the replay buffer and trains the agent with supervised learning. 2400 iterations should take ~130mins.\n",
    "\n",
    "❗2400 iterations is probably not enough for robust performance, so you might see some weird predictions. Training for longer periods, particularly with data augmentation (see **Appendix E**), will improve performance. But we will stick with this for now to avoid Colab timeouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Tensorboard in PyTorch\n",
    "\n",
    "Let’s now try using TensorBoard with PyTorch! Before logging anything, we need to create a SummaryWriter instance. Writer will output to `logir = ./runs/` directory by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TensorBoard log\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the TensorBoard notebook extension\n",
    "\n",
    "# if not colab:\n",
    "#     %load_ext tensorboard\n",
    "# else:\n",
    "#     !load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_FREQ = 10\n",
    "SAVE_MODELS = True\n",
    "SAVE_MODEL_FREQ = 50\n",
    "calc_test_loss = True\n",
    "\n",
    "# Misc\n",
    "train_loss = 1e8\n",
    "test_loss = 1e8\n",
    "\n",
    "# Directories where to save the best models for train/test\n",
    "model_run_time = datetime.now()\n",
    "model_save_dir = os.path.join(WORKSPACE_DIR,\"outputs\", \"models\", TASK, model_run_time.strftime(\"%Y-%m-%d_%H-%M\"))\n",
    "model_save_dir_best_train = os.path.join(model_save_dir, \"best_model_train\")\n",
    "model_save_dir_best_test = os.path.join(model_save_dir, \"best_model_test\")\n",
    "metrics_save_path = os.path.join(model_save_dir, \"training_metrics.json\")  # JSON file to save metrics\n",
    "\n",
    "# Create directories\n",
    "if not os.path.exists(model_save_dir_best_train):\n",
    "    print(f\"Could not find {model_save_dir_best_train}, creating directory.\")\n",
    "    os.makedirs(model_save_dir_best_train)\n",
    "if not os.path.exists(model_save_dir_best_test):\n",
    "    print(f\"Could not find {model_save_dir_best_test}, creating directory.\")\n",
    "    os.makedirs(model_save_dir_best_test)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize metrics dictionary\n",
    "metrics = {\n",
    "    \"train\": [],\n",
    "    \"test\": []\n",
    "}\n",
    "\n",
    "for iteration in range(TRAINING_ITERATIONS):\n",
    "    batch = next(train_data_iter)\n",
    "    batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "    update_dict = peract_agent.update(iteration, batch) # Here backprop == True: for training reaons, hence training_loss == total_loss\n",
    "\n",
    "    if iteration % LOG_FREQ == 0:\n",
    "        elapsed_time = (time.time() - start_time) / 60.0\n",
    "\n",
    "        # Log training metrics\n",
    "        train_metrics = {\n",
    "            \"iteration\": iteration,\n",
    "            \"learning_rate\": update_dict['learning_rate'],\n",
    "            \"total_loss\": update_dict['total_loss'],\n",
    "            \"trans_loss\": update_dict['trans_loss'],\n",
    "            \"rot_loss\": update_dict['rot_loss'],\n",
    "            \"col_loss\": update_dict['col_loss'],\n",
    "            \"elapsed_time\": elapsed_time\n",
    "        }\n",
    "        metrics[\"train\"].append(train_metrics)\n",
    "\n",
    "        writer.add_scalar(\"Learning Rate\", update_dict['learning_rate'], iteration)\n",
    "            \n",
    "        writer.add_scalar(\"Loss/train\", update_dict['total_loss'], iteration)\n",
    "        writer.add_scalar(\"Trans Loss/train\", update_dict['trans_loss'], iteration)\n",
    "        writer.add_scalar(\"Rot Loss/train\", update_dict['rot_loss'], iteration)\n",
    "        writer.add_scalar(\"Collision Loss/train\", update_dict['col_loss'], iteration)\n",
    "\n",
    "        if calc_test_loss:\n",
    "            batch = next(test_data_iter)\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "            test_update_dict = peract_agent.update(iteration, batch, backprop=False) # Here backprop == False: for evaluation, hence test_loss == total_loss\n",
    "\n",
    "            # Log test metrics\n",
    "            test_metrics = {\n",
    "                \"iteration\": iteration,\n",
    "                \"total_loss\": test_update_dict['total_loss'],\n",
    "                \"trans_loss\": test_update_dict['trans_loss'],\n",
    "                \"rot_loss\": test_update_dict['rot_loss'],\n",
    "                \"col_loss\": test_update_dict['col_loss']\n",
    "            }\n",
    "            metrics[\"test\"].append(test_metrics)\n",
    "            \n",
    "            writer.add_scalar(\"Loss/test\", test_update_dict['total_loss'], iteration)\n",
    "            writer.add_scalar(\"Trans Loss/test\", test_update_dict['trans_loss'], iteration)\n",
    "            writer.add_scalar(\"Rot Loss/test\", test_update_dict['rot_loss'], iteration)\n",
    "            writer.add_scalar(\"Collision Loss\", test_update_dict['col_loss'], iteration)\n",
    "\n",
    "            print(\"Iteration: %d/%d | Learning Rate: %f | Train Loss [tot,trans,rot,col]: [%0.2f, %0.2f, %0.2f, %0.2f] | Test Loss [tot,trans,rot,col]: [%0.2f, %0.2f, %0.2f, %0.2f] | Elapsed Time: %0.2f mins\"\\\n",
    "                   % (iteration, TRAINING_ITERATIONS, \n",
    "                      update_dict['learning_rate'],\n",
    "                      update_dict['total_loss'], update_dict['trans_loss'], update_dict['rot_loss'], update_dict['col_loss'],\n",
    "                      test_update_dict['total_loss'], test_update_dict['trans_loss'], test_update_dict['rot_loss'], test_update_dict['col_loss'], \n",
    "                      elapsed_time))\n",
    "        else:\n",
    "            print(\"Iteration: %d/%d | Learning Rate: %f| Train Loss [tot,trans,rot,col]: [%0.2f, %0.2f, %0.2f, %0.2f] | Elapsed Time: %0.2f mins\"\\\n",
    "                   % (iteration, TRAINING_ITERATIONS, \n",
    "                      update_dict['learning_rate'],\n",
    "                      update_dict['total_loss'], update_dict['trans_loss'], update_dict['rot_loss'], update_dict['col_loss'],\n",
    "                      elapsed_time))\n",
    "            \n",
    "    if (SAVE_MODELS == True):\n",
    "        if iteration % SAVE_MODEL_FREQ == 0:\n",
    "            \n",
    "            # Only save the best if better\n",
    "            if update_dict['total_loss'] < train_loss:\n",
    "                print(\"Saving Best Model - Train\")\n",
    "                train_loss = update_dict['total_loss']\n",
    "                peract_agent.save_weights(model_save_dir_best_train)\n",
    "            if test_update_dict['total_loss'] < test_loss:\n",
    "                print(\"Saving Best Model - Test\")\n",
    "                test_loss = test_update_dict['total_loss']\n",
    "                peract_agent.save_weights(model_save_dir_best_test)\n",
    "\n",
    "# Save the last checkpoint\n",
    "model_save_dir_last = os.path.join(model_save_dir, \"last\")\n",
    "\n",
    "if not os.path.exists(model_save_dir_last):\n",
    "    print(f\"Could not find {model_save_dir_last}, creating directory.\")\n",
    "    os.makedirs(model_save_dir_last)\n",
    "\n",
    "print(\"Saving Model - Last\")\n",
    "peract_agent.save_weights(model_save_dir_last)\n",
    "\n",
    "# Save metrics to JSON file\n",
    "with open(metrics_save_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(f\"Training metrics saved to {metrics_save_path}\")\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot training and test (val) losses\n",
    "\n",
    "# if not colab:\n",
    "#     %tensorboard --logdir=runs\n",
    "# else:\n",
    "#     !tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Visualization\n",
    "\n",
    "Let's see how PerAct does on held-out test data.  \n",
    "\n",
    "PerAct should be evaluated in simulation on scenes with randomized object poses and object instances. But this Colab notebook doesn't support the V-REP simulator (for RLBench tasks). So for now we will do inference on a static test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from arm.utils import visualise_voxel, discrete_euler_to_quaternion, get_gripper_render_pose\n",
    "# from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# # create_another_test_set = True\n",
    "# create_another_test_set = False\n",
    "\n",
    "\n",
    "# if create_another_test_set:\n",
    "#     clip_model, preprocess = clip.load(\"RN50\", device=device) # CLIP-ResNet50\n",
    "#     test_task = 'go_to_apple_test'\n",
    "#     test_task_folder = f'{test_task}/all_variations/episodes'\n",
    "#     data_path = os.path.join(DATA_FOLDER, test_task_folder)\n",
    "\n",
    "#     test_replay_buffer = create_replay(batch_size=BATCH_SIZE,\n",
    "#                                     timesteps=1,\n",
    "#                                     save_dir=test_replay_storage_dir,\n",
    "#                                     cameras=CAMERAS,\n",
    "#                                     voxel_sizes=VOXEL_SIZES)\n",
    "\n",
    "#     print(\"-- Test Buffer --\")\n",
    "#     fill_replay(\n",
    "#                 data_path=data_path,\n",
    "#                 replay=test_replay_buffer,\n",
    "# #                 start_idx=0,\n",
    "# #                 num_demos=4,\n",
    "#                 demo_augmentation=True,\n",
    "#                 demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "#                 cameras=CAMERAS,\n",
    "#                 rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "#                 voxel_sizes=VOXEL_SIZES,\n",
    "#                 rotation_resolution=ROTATION_RESOLUTION,\n",
    "#                 crop_augmentation=False,\n",
    "#                 clip_model=clip_model,\n",
    "#                 device=device)\n",
    "#     del clip_model\n",
    "\n",
    "#     test_wrapped_replay = PyTorchReplayBuffer(test_replay_buffer)\n",
    "#     test_dataset = test_wrapped_replay.dataset()\n",
    "#     test_data_iter = iter(test_dataset)\n",
    "\n",
    "\n",
    "# batch = next(test_data_iter) #Change to make it either \n",
    "# lang_goal = batch['lang_goal'][0][0][0]\n",
    "# print(lang_goal)\n",
    "# batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "# update_dict = peract_agent.update(iteration, batch, backprop=False)\n",
    "\n",
    "# #things to print\n",
    "# loss = update_dict['total_loss']\n",
    "# rot_loss = update_dict['rot_loss']#.detach().cpu().numpy()\n",
    "# trans_loss = update_dict['trans_loss']#.detach().cpy().numpy()\n",
    "# col_loss = update_dict['col_loss']#.detach().cpu().numpy()\n",
    "# print('The loss of this prediction is: ', loss)\n",
    "# print('The rotational loss of this prediction is:', rot_loss)\n",
    "# print('The translational loss of this prediction is:', trans_loss)\n",
    "# print('The collision loss of this prediction is:', col_loss)\n",
    "\n",
    "# # things to visualize\n",
    "# vis_voxel_grid = update_dict['voxel_grid'][0].detach().cpu().numpy()\n",
    "# vis_trans_q = update_dict['q_trans'][0].detach().cpu().numpy()\n",
    "# vis_trans_coord = update_dict['pred_action']['trans'][0].detach().cpu().numpy()\n",
    "# vis_gt_coord = update_dict['expert_action']['action_trans'][0].detach().cpu().numpy()\n",
    "\n",
    "# # discrete to continuous\n",
    "# continuous_trans = update_dict['pred_action']['continuous_trans'][0].detach().cpu().numpy()\n",
    "# continuous_quat = discrete_euler_to_quaternion(update_dict['pred_action']['rot_and_grip'][0][:3].detach().cpu().numpy(),\n",
    "#                                                resolution=peract_agent._rotation_resolution)\n",
    "# gripper_open = bool(update_dict['pred_action']['rot_and_grip'][0][-1].detach().cpu().numpy())\n",
    "# ignore_collision = bool(update_dict['pred_action']['collision'][0][0].detach().cpu().numpy())\n",
    "\n",
    "# # gripper visualization pose\n",
    "# voxel_size = 0.045\n",
    "# voxel_scale = voxel_size * 100\n",
    "# gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "#                                            SCENE_BOUNDS[:3],\n",
    "#                                            continuous_trans,\n",
    "#                                            continuous_quat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not colab:\n",
    "#     #@markdown #### Show Q-Prediction and Best Action\n",
    "#     show_expert_action = True  #@param {type:\"boolean\"}\n",
    "#     show_q_values = True  #@param {type:\"boolean\"}\n",
    "#     render_gripper = True  #@param {type:\"boolean\"}\n",
    "#     rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "#     rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "#                                 vis_trans_q if show_q_values else None,\n",
    "#                                 vis_trans_coord,\n",
    "#                                 vis_gt_coord if show_expert_action else None,\n",
    "#                                 voxel_size=voxel_size,\n",
    "#                                 rotation_amount=np.deg2rad(0),\n",
    "#                                 render_gripper=render_gripper,\n",
    "#                                 gripper_pose=gripper_pose_mat,\n",
    "#                                 gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "#     rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "#                                 vis_trans_q if show_q_values else None,\n",
    "#                                 vis_trans_coord,\n",
    "#                                 vis_gt_coord if show_expert_action else None,\n",
    "#                                 voxel_size=voxel_size,\n",
    "#                                 rotation_amount=np.deg2rad(-90),\n",
    "#                                 render_gripper=render_gripper,\n",
    "#                                 gripper_pose=gripper_pose_mat,\n",
    "#                                 gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(20, 15))\n",
    "#     fig.add_subplot(1, 2, 1)\n",
    "#     plt.imshow(rendered_img_0)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(\"Front view\")\n",
    "#     fig.add_subplot(1, 2, 2)\n",
    "#     plt.imshow(rendered_img_270)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(\"Side view\")\n",
    "\n",
    "#     # print(f\"Lang goal: {lang_goal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained PerAct model\n",
    "# model_save_dir = os.path.join(WORKSPACE_DIR,\"outputs\", \"models\")\n",
    "\n",
    "# if not os.path.exists(model_save_dir):\n",
    "#     print(f\"Could not find {model_save_dir}, creating directory.\")\n",
    "#     os.mkdir(model_save_dir)\n",
    "\n",
    "# peract_agent.save_weights(model_save_dir)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
