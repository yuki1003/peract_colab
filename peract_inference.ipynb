{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Trained PerAct Agent on handoversim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_ # bad trick to fix numpy version issue :(\n",
    "import os\n",
    "import sys\n",
    "sys.path = [p for p in sys.path if '/peract/' not in p]\n",
    "\n",
    "# Set `PYOPENGL_PLATFORM=egl` for pyrender visualizations\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\" # Depends on your computer and available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from notebook_helpers.constants import * # Load global constant variables from constants.py\n",
    "from notebook_helpers.build_training import build_agent\n",
    "\n",
    "# Choose the run\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-10_14-59\" # Good {non-uniform 1 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad {uniform 2 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_13-34\"\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_17-25\" # {crop skip 10}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_21-23\"\n",
    "run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/\" # {crop skip 10 new}\n",
    "\n",
    "# Obtain settings\n",
    "path_settings = os.path.join(os.path.dirname(run_dir), \"training_settings.json\")\n",
    "with open(path_settings, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "CAMERAS = settings['cameras']\n",
    "\n",
    "if BATCH_SIZE != 1:\n",
    "    raise ValueError(\"For Inference, 'batch_size' must be set to 1 in constants.py.\")\n",
    "\n",
    "peract_agent = build_agent(settings, training=False)\n",
    "peract_agent.set_language_goal(\"handing over banana\")\n",
    "\n",
    "# Choose model\n",
    "iteration = \"run8000\"\n",
    "best_type = \"best_model_general\"\n",
    "model_path = os.path.join(run_dir, iteration, best_type)\n",
    "\n",
    "peract_agent.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference: Run Inference on just observation data and Save as video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs\n",
    "from arm.utils import visualise_voxel\n",
    "from arm.utils import get_gripper_render_pose\n",
    "\n",
    "\n",
    "# What to visualize\n",
    "episode_idx_to_visualize = 646  # Index of the episode to visualize\n",
    "# Video output path\n",
    "video_output_path = f\"demo{episode_idx_to_visualize}_visualization.mp4\"\n",
    "\n",
    "# Get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                       index=episode_idx_to_visualize,\n",
    "                       cameras=CAMERAS,\n",
    "                       depth_scale=DEPTH_SCALE)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "\n",
    "# Open a video writer\n",
    "with imageio.get_writer(video_output_path, fps=10) as video_writer:\n",
    "    for ts in episode_length[::5]:\n",
    "        print(ts)\n",
    "        # Extract obs at timestep\n",
    "        obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "        # gripper_pose = demo[ts].gripper_pose\n",
    "        gripper_open = demo[ts].gripper_open\n",
    "        gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "        obs_dict[\"gripper_open\"] = gripper_open\n",
    "        obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "        (continuous_trans, continuous_quat, gripper_open, _, _), \\\n",
    "        (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent.forward(obs_dict, ts)\n",
    "        print(continuous_trans, continuous_quat, gripper_open)\n",
    "\n",
    "        # Things to visualize\n",
    "        vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "        pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "\n",
    "        voxel_size = 0.045\n",
    "        voxel_scale = voxel_size * 100\n",
    "        gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                                   SCENE_BOUNDS[:3],\n",
    "                                                   continuous_trans,\n",
    "                                                   continuous_quat)\n",
    "\n",
    "        rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                         None,\n",
    "                                         [pred_trans_coord],\n",
    "                                         None,\n",
    "                                         voxel_size=voxel_size,\n",
    "                                         rotation_amount=np.deg2rad(0),\n",
    "                                         render_gripper=True,\n",
    "                                         gripper_pose=gripper_pose_mat,\n",
    "                                         gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                           None,\n",
    "                                           [pred_trans_coord],\n",
    "                                           None,\n",
    "                                           voxel_size=voxel_size,\n",
    "                                           rotation_amount=np.deg2rad(45+180),\n",
    "                                           render_gripper=True,\n",
    "                                           gripper_pose=gripper_pose_mat,\n",
    "                                           gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        # Plot figures into a NumPy array\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        plt.imshow(rendered_img_0)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Front view\")\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        plt.imshow(rendered_img_270)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Side view\")\n",
    "\n",
    "        # Add timestamp as text with white font and black background\n",
    "        fig.text(0.02, 0.95, f\"Timestep: {ts}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                 bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "\n",
    "        # Convert the matplotlib figure to a NumPy array\n",
    "        fig.canvas.draw()\n",
    "        img_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        \n",
    "        video_writer.append_data(img_array)  # Add frame to video\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "print(f\"Video saved to {video_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Run Inference on just input data with ground truth to check what happends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_from_frame(episode_keypoints_gt_obs_dict, frame_idx):\n",
    "    # Get a sorted list of dictionary keys (frames with stored data)\n",
    "    episode_keypoints = sorted(episode_keypoints_gt_obs_dict.keys())\n",
    "\n",
    "    # Iterate through the given frame indices\n",
    "    # Find the smallest key that is greater than or equal to the current frame\n",
    "    for episode_kp in episode_keypoints:\n",
    "        if frame_idx <= episode_kp:\n",
    "            episode_kp_gt_obs_dict = episode_keypoints_gt_obs_dict[episode_kp]\n",
    "            return episode_kp_gt_obs_dict\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: NEED SOME WAY TO COLLECT THE GROUND TRUTH DATA FOR ALL FRAMES\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs_gt\n",
    "\n",
    "from arm.demo import _keypoint_discovery_available\n",
    "from arm.utils import point_to_voxel_index, visualise_voxel, get_gripper_render_pose\n",
    "\n",
    "\n",
    "# What to visualize\n",
    "episode_idx_to_visualize = 645  # Index of the episode to visualize\n",
    "# Video output path\n",
    "video_output_path = f\"demo{episode_idx_to_visualize}_gt_pred_visualization.mp4\"\n",
    "\n",
    "# Get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                       index=episode_idx_to_visualize,\n",
    "                       cameras=CAMERAS,\n",
    "                       depth_scale=DEPTH_SCALE)\n",
    "\n",
    "episode_keypoints_gt_obs_dict = dict()\n",
    "episode_keypoints = _keypoint_discovery_available(demo, approach_distance=0.3) #NOTE: Approach_distance Set\n",
    "episode_keypoints = [episode_keypoints[-1]]\n",
    "for episode_keypoint in episode_keypoints:\n",
    "    episode_keypoints_gt_obs_dict[episode_keypoint] = extract_obs_gt(obs = demo._observations[episode_keypoint],\n",
    "                                                                  cameras=CAMERAS)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "# episode_length = [40]\n",
    "\n",
    "# Open a video writer\n",
    "with imageio.get_writer(video_output_path, fps=10) as video_writer:\n",
    "    for ts in episode_length[::5]: # Skip some frames\n",
    "        print(ts)\n",
    "        # Extract obs at timestep\n",
    "        obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "        # gripper_pose = demo[ts].gripper_pose\n",
    "        gripper_open = demo[ts].gripper_open\n",
    "        gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "        obs_dict[\"gripper_open\"] = gripper_open\n",
    "        obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "        (continuous_trans, continuous_quat, gripper_open, trans_confidence, _), \\\n",
    "        (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent.forward(obs_dict, ts)\n",
    "\n",
    "        pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "        \n",
    "        # Get the ground truth\n",
    "        episode_keypoint_gt_obs_dict = get_gt_from_frame(episode_keypoints_gt_obs_dict, ts)\n",
    "        if not (episode_keypoint_gt_obs_dict is None):\n",
    "            gt_gripper_pose = episode_keypoint_gt_obs_dict[\"gripper_pose\"]\n",
    "            gt_trans_coord = point_to_voxel_index(gt_gripper_pose[:3], VOXEL_SIZES, SCENE_BOUNDS)[0]\n",
    "            error = np.linalg.norm(gt_gripper_pose[:3] - continuous_trans)\n",
    "            print(f\"GT (voxel): {gt_trans_coord} - Prediction (voxel): {pred_trans_coord} - Error: {error} - Prediction-score: {round(trans_confidence,4)}\")\n",
    "        else:\n",
    "            gt_trans_coord = None\n",
    "            error = False\n",
    "            print(\"GT coordinates not available for this frame\")\n",
    "        \n",
    "        # Things to visualize\n",
    "        vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "\n",
    "        voxel_size = 0.045\n",
    "        voxel_scale = voxel_size * 100\n",
    "        gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                                   SCENE_BOUNDS[:3],\n",
    "                                                   continuous_trans,\n",
    "                                                   continuous_quat)\n",
    "\n",
    "        rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                         None,\n",
    "                                         [pred_trans_coord],\n",
    "                                         gt_trans_coord,\n",
    "                                         alpha = 0.2,\n",
    "                                         voxel_size=voxel_size,\n",
    "                                         rotation_amount=np.deg2rad(0),\n",
    "                                         render_gripper=True,\n",
    "                                         gripper_pose=gripper_pose_mat,\n",
    "                                         gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                           None,\n",
    "                                           [pred_trans_coord],\n",
    "                                           gt_trans_coord,\n",
    "                                           alpha = 0.2,\n",
    "                                           voxel_size=voxel_size,\n",
    "                                           rotation_amount=np.deg2rad(45+180),\n",
    "                                           render_gripper=True,\n",
    "                                           gripper_pose=gripper_pose_mat,\n",
    "                                           gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        # Plot figures into a NumPy array\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        plt.imshow(rendered_img_0)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Front view\")\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        plt.imshow(rendered_img_270)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Side view\")\n",
    "\n",
    "        # Add timestamp as text with white font and black background\n",
    "        if error:\n",
    "            fig.text(0.02, 0.95, f\"Timestep: {ts}, Prediction-score: {round(trans_confidence,4)}, Error: {np.round(error, 3)}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                    bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "        else:\n",
    "            fig.text(0.02, 0.95, f\"Timestep: {ts}, Prediction-score: {round(trans_confidence,4)}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                    bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "\n",
    "        # Convert the matplotlib figure to a NumPy array\n",
    "        fig.canvas.draw()\n",
    "        img_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        \n",
    "        video_writer.append_data(img_array)  # Add frame to video\n",
    "        # plt.show()\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "        \n",
    "\n",
    "print(f\"Video saved to {video_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Run Inference with same batches as during training (You can change `batch_size` in <i>constants.py</i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from notebook_helpers.build_replay import load_replay_buffer\n",
    "\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/\" # {crop skip 10 new}\n",
    "\n",
    "# path_settings = os.path.join(os.path.dirname(run_dir), \"training_settings.json\")\n",
    "# with open(path_settings, 'r') as f:\n",
    "#     settings = json.load(f)\n",
    "\n",
    "# train_data_iter, test_data_iter = load_replay_buffer(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from notebook_helpers.constants import * # Load global constant variables from constants.py\n",
    "from notebook_helpers.build_training import build_agent\n",
    "from notebook_helpers.build_replay import load_replay_buffer\n",
    "\n",
    "# Choose the run\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-10_14-59\" # Good {non-uniform 1 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad {uniform 2 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_13-34\"\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_17-25\" # {crop skip 10}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_21-23\"\n",
    "run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/\" # {crop skip 10 new}\n",
    "\n",
    "# Obtain settings\n",
    "path_settings = os.path.join(os.path.dirname(run_dir), \"training_settings.json\")\n",
    "with open(path_settings, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "train_data_iter, test_data_iter = load_replay_buffer(settings)\n",
    "\n",
    "peract_agent = build_agent(settings, training=True)\n",
    "peract_agent.set_language_goal(\"handing over banana\")\n",
    "\n",
    "# Choose model\n",
    "iteration = \"run8000\"\n",
    "best_type = \"best_model_general\"\n",
    "model_path = os.path.join(run_dir, iteration, best_type)\n",
    "\n",
    "peract_agent.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs\n",
    "\n",
    "CAMERAS = settings[\"cameras\"]\n",
    "\n",
    "batch = next(train_data_iter)\n",
    "\n",
    "# what to visualize\n",
    "episode_idx_to_visualize = 646#INDEXES[0] # out of 10 demos\n",
    "# ts = 70#25 # timestep out of total timesteps\n",
    "\n",
    "# get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                    index=episode_idx_to_visualize,\n",
    "                    cameras=CAMERAS,\n",
    "                    depth_scale=DEPTH_SCALE,)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "for ts in episode_length:\n",
    "\n",
    "    # extract obs at timestep\n",
    "    obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "    gripper_pose = demo[ts].gripper_pose\n",
    "    gripper_open = demo[ts].gripper_open\n",
    "    gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "    # obs_dict[\"gripper_pose\"] = gripper_pose\n",
    "    obs_dict[\"gripper_open\"] = gripper_open\n",
    "    obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "    # plot rgb and depth at timestep\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    rows, cols = 2, len(CAMERAS)\n",
    "\n",
    "    plot_idx = 1\n",
    "    for camera in CAMERAS:\n",
    "        # rgb\n",
    "        rgb_name = \"%s_%s\" % (camera, 'rgb')\n",
    "        rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
    "        fig.add_subplot(rows, cols, plot_idx)\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"%s_rgb | step %s\" % (camera, ts))\n",
    "\n",
    "        # depth\n",
    "        depth_name = \"%s_%s\" % (camera, 'depth')\n",
    "        depth = np.transpose(obs_dict[depth_name], (1, 2, 0))\n",
    "        fig.add_subplot(rows, cols, plot_idx+len(CAMERAS))\n",
    "        plt.imshow(depth)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"%s_depth | step %s\" % (camera, ts))\n",
    "\n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(obs_dict)\n",
    "\n",
    "    (continuous_trans, continuous_quat, gripper_open), (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent.forward(obs_dict, ts)\n",
    "\n",
    "    from arm.utils import visualise_voxel\n",
    "    from arm.utils import discrete_euler_to_quaternion, get_gripper_render_pose\n",
    "\n",
    "    # things to visualize\n",
    "    vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "    pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "\n",
    "    # discrete to continuous\n",
    "    continuous_trans = continuous_trans[0].detach().cpu().numpy()\n",
    "    continuous_quat = discrete_euler_to_quaternion(rot_and_grip_indices[0][:3].detach().cpu().numpy(),\n",
    "                                                resolution=peract_agent._rotation_resolution)\n",
    "    gripper_open = bool(rot_and_grip_indices[0][-1].detach().cpu().numpy())\n",
    "    ignore_collision = bool(test_update_dict['pred_action']['collision'][0][0].detach().cpu().numpy())\n",
    "\n",
    "    # # gripper visualization pose\n",
    "    voxel_size = 0.045\n",
    "    voxel_scale = voxel_size * 100\n",
    "    gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                            SCENE_BOUNDS[:3],\n",
    "                                            continuous_trans,\n",
    "                                            continuous_quat)\n",
    "\n",
    "    # #@markdown #### Show Q-Prediction and Best Action\n",
    "    show_expert_action = True  #@param {type:\"boolean\"}\n",
    "    show_q_values = False  #@param {type:\"boolean\"}\n",
    "    render_gripper = False  #@param {type:\"boolean\"}\n",
    "    rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "    rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(0),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale,\n",
    "                                perspective=False)\n",
    "\n",
    "    rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(45),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(rendered_img_0)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Front view\")\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(rendered_img_270)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Side view\")\n",
    "\n",
    "\n",
    "    # #@markdown #### Show Q-Prediction and Best Action\n",
    "    show_expert_action = True  #@param {type:\"boolean\"}\n",
    "    show_q_values = True  #@param {type:\"boolean\"}\n",
    "    render_gripper = True  #@param {type:\"boolean\"}\n",
    "    rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "    rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(0),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "    rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(45),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(rendered_img_0)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Front view\")\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(rendered_img_270)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Side view\")\n",
    "\n",
    "    print(f\"Lang goal: {lang_goal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs = natsorted([os.path.join(run_dir, run) for run in os.listdir(run_dir) if \"run\" in run])\n",
    "\n",
    "chosen_model = \"best_model_train\"\n",
    "\n",
    "model_run_distances = dict()\n",
    "model_run_scores = dict()\n",
    "\n",
    "for model_run_iter in model_runs:\n",
    "    \n",
    "    try:\n",
    "        peract_agent.load_weights(os.path.join(model_run_iter, chosen_model))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    distances_run = []\n",
    "    scores_run = []\n",
    "    \n",
    "    for i in range(20): # collest using 100 samples\n",
    "        print(i)\n",
    "        batch = next(train_data_iter) # NOTE: Choose which set to infer\n",
    "        lang_goal = batch['lang_goal'][0][0][0]\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "        \n",
    "        update_dict = peract_agent.update(None, batch, backprop=False)\n",
    "        prediction_score = round(update_dict[\"q_trans\"].max().item(), 4)\n",
    "        pred_trans = update_dict[\"pred_action\"][\"trans\"].detach().cpu().numpy()[0]\n",
    "        gt_trans = update_dict[\"expert_action\"][\"action_trans\"].detach().cpu().numpy()[0]\n",
    "        dist = np.round(np.linalg.norm(pred_trans-gt_trans), 4)\n",
    "        \n",
    "        distances_run.append(dist)\n",
    "        scores_run.append(prediction_score)\n",
    "    \n",
    "    zipped_lists = zip(distances_run, scores_run)\n",
    "\n",
    "    sorted_lists = sorted(zipped_lists, key=lambda x: x[0])\n",
    "\n",
    "    sorted_distances, sorted_scores = zip(*sorted_lists)\n",
    "    model_run_distances[os.path.dirname(model_run_iter)] = sorted_distances\n",
    "    model_run_scores[os.path.dirname(model_run_iter)] = sorted_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rlbench.utils import get_stored_demo\n",
    "# from rlbench.backend.utils import extract_obs\n",
    "\n",
    "# batch = next(train_data_iter)\n",
    "\n",
    "# # what to visualize\n",
    "# episode_idx_to_visualize = 846#INDEXES[0] # out of 10 demos\n",
    "# # ts = 70#25 # timestep out of total timesteps\n",
    "\n",
    "# # get demo\n",
    "# demo = get_stored_demo(data_path=test_data_path,\n",
    "#                     index=episode_idx_to_visualize,\n",
    "#                     cameras=CAMERAS,\n",
    "#                     depth_scale=DEPTH_SCALE,)\n",
    "\n",
    "# episode_length = list(range(len(demo._observations)))\n",
    "# for ts in episode_length:\n",
    "\n",
    "#     # extract obs at timestep\n",
    "#     obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "#     gripper_pose = demo[ts].gripper_pose\n",
    "#     gripper_open = demo[ts].gripper_open\n",
    "#     gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "#     # obs_dict[\"gripper_pose\"] = gripper_pose\n",
    "#     obs_dict[\"gripper_open\"] = gripper_open\n",
    "#     obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "#     # plot rgb and depth at timestep\n",
    "#     fig = plt.figure(figsize=(20, 10))\n",
    "#     rows, cols = 2, len(CAMERAS)\n",
    "\n",
    "#     plot_idx = 1\n",
    "#     for camera in CAMERAS:\n",
    "#         # rgb\n",
    "#         rgb_name = \"%s_%s\" % (camera, 'rgb')\n",
    "#         rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
    "#         fig.add_subplot(rows, cols, plot_idx)\n",
    "#         plt.imshow(rgb)\n",
    "#         plt.axis('off')\n",
    "#         plt.title(\"%s_rgb | step %s\" % (camera, ts))\n",
    "\n",
    "#         # depth\n",
    "#         depth_name = \"%s_%s\" % (camera, 'depth')\n",
    "#         depth = np.transpose(obs_dict[depth_name], (1, 2, 0))\n",
    "#         fig.add_subplot(rows, cols, plot_idx+len(CAMERAS))\n",
    "#         plt.imshow(depth)\n",
    "#         plt.axis('off')\n",
    "#         plt.title(\"%s_depth | step %s\" % (camera, ts))\n",
    "\n",
    "#         plot_idx += 1\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#     print(obs_dict)\n",
    "\n",
    "#     (continuous_trans, continuous_quat, gripper_open), (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent_inference.forward(obs_dict, ts)\n",
    "\n",
    "#     from arm.utils import visualise_voxel\n",
    "#     from arm.utils import discrete_euler_to_quaternion, get_gripper_render_pose\n",
    "\n",
    "#     # things to visualize\n",
    "#     vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "#     pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "\n",
    "#     # discrete to continuous\n",
    "#     continuous_trans = continuous_trans[0].detach().cpu().numpy()\n",
    "#     continuous_quat = discrete_euler_to_quaternion(rot_and_grip_indices[0][:3].detach().cpu().numpy(),\n",
    "#                                                 resolution=peract_agent_inference._rotation_resolution)\n",
    "#     gripper_open = bool(rot_and_grip_indices[0][-1].detach().cpu().numpy())\n",
    "#     ignore_collision = bool(test_update_dict['pred_action']['collision'][0][0].detach().cpu().numpy())\n",
    "\n",
    "#     # # gripper visualization pose\n",
    "#     voxel_size = 0.045\n",
    "#     voxel_scale = voxel_size * 100\n",
    "#     gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "#                                             SCENE_BOUNDS[:3],\n",
    "#                                             continuous_trans,\n",
    "#                                             continuous_quat)\n",
    "\n",
    "#     # #@markdown #### Show Q-Prediction and Best Action\n",
    "#     show_expert_action = True  #@param {type:\"boolean\"}\n",
    "#     show_q_values = False  #@param {type:\"boolean\"}\n",
    "#     render_gripper = False  #@param {type:\"boolean\"}\n",
    "#     rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "#     rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "#                                 None,\n",
    "#                                 [pred_trans_coord],\n",
    "#                                 None,\n",
    "#                                 voxel_size=voxel_size,\n",
    "#                                 rotation_amount=np.deg2rad(0),\n",
    "#                                 render_gripper=render_gripper,\n",
    "#                                 gripper_pose=gripper_pose_mat,\n",
    "#                                 gripper_mesh_scale=voxel_scale,\n",
    "#                                 perspective=False)\n",
    "\n",
    "#     rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "#                                 None,\n",
    "#                                 [pred_trans_coord],\n",
    "#                                 None,\n",
    "#                                 voxel_size=voxel_size,\n",
    "#                                 rotation_amount=np.deg2rad(45),\n",
    "#                                 render_gripper=render_gripper,\n",
    "#                                 gripper_pose=gripper_pose_mat,\n",
    "#                                 gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(20, 15))\n",
    "#     fig.add_subplot(1, 2, 1)\n",
    "#     plt.imshow(rendered_img_0)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(\"Front view\")\n",
    "#     fig.add_subplot(1, 2, 2)\n",
    "#     plt.imshow(rendered_img_270)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(\"Side view\")\n",
    "\n",
    "\n",
    "#     # #@markdown #### Show Q-Prediction and Best Action\n",
    "#     show_expert_action = True  #@param {type:\"boolean\"}\n",
    "#     show_q_values = True  #@param {type:\"boolean\"}\n",
    "#     render_gripper = True  #@param {type:\"boolean\"}\n",
    "#     rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "#     rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "#                                 None,\n",
    "#                                 [pred_trans_coord],\n",
    "#                                 None,\n",
    "#                                 voxel_size=voxel_size,\n",
    "#                                 rotation_amount=np.deg2rad(0),\n",
    "#                                 render_gripper=render_gripper,\n",
    "#                                 gripper_pose=gripper_pose_mat,\n",
    "#                                 gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "#     rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "#                                 None,\n",
    "#                                 [pred_trans_coord],\n",
    "#                                 None,\n",
    "#                                 voxel_size=voxel_size,\n",
    "#                                 rotation_amount=np.deg2rad(45),\n",
    "#                                 render_gripper=render_gripper,\n",
    "#                                 gripper_pose=gripper_pose_mat,\n",
    "#                                 gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(20, 15))\n",
    "#     fig.add_subplot(1, 2, 1)\n",
    "#     plt.imshow(rendered_img_0)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(\"Front view\")\n",
    "#     fig.add_subplot(1, 2, 2)\n",
    "#     plt.imshow(rendered_img_270)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(\"Side view\")\n",
    "\n",
    "#     print(f\"Lang goal: {lang_goal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from arm.utils import discrete_euler_to_quaternion, get_gripper_render_pose\n",
    "\n",
    "# batch = next(test_data_iter)\n",
    "\n",
    "# lang_goal = batch['lang_goal'][0][0][0]\n",
    "# print(lang_goal)\n",
    "\n",
    "# batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "# test_update_dict = peract_agent_inference.update(0, batch, backprop=False) # Here backprop == False: for evaluation, hence test_loss == total_loss\n",
    "\n",
    "# # Log test metrics\n",
    "# test_metrics = {\n",
    "#     \"total_loss\": test_update_dict['total_loss'],\n",
    "#     \"trans_loss\": test_update_dict['trans_loss'],\n",
    "#     \"rot_loss\": test_update_dict['rot_loss'],\n",
    "#     \"col_loss\": test_update_dict['col_loss']\n",
    "# }\n",
    "# for episode_kp, value in test_metrics.items():\n",
    "#     print(episode_kp, value)\n",
    "\n",
    "\n",
    "# from arm.utils import visualise_voxel\n",
    "\n",
    "# # # things to visualize\n",
    "# vis_voxel_grid = test_update_dict['voxel_grid'][0].detach().cpu().numpy()\n",
    "# vis_trans_q = test_update_dict['q_trans'][0].detach().cpu().numpy()\n",
    "# pred_trans_coord = test_update_dict['pred_action']['trans'][0].detach().cpu().numpy().tolist()\n",
    "# vis_gt_coord = test_update_dict['expert_action']['action_trans'][0].detach().cpu().numpy()\n",
    "\n",
    "# # discrete to continuous\n",
    "# continuous_trans = test_update_dict['pred_action']['continuous_trans'][0].detach().cpu().numpy()\n",
    "# continuous_quat = discrete_euler_to_quaternion(test_update_dict['pred_action']['rot_and_grip'][0][:3].detach().cpu().numpy(),\n",
    "#                                             resolution=peract_agent_inference._rotation_resolution)\n",
    "# gripper_open = bool(test_update_dict['pred_action']['rot_and_grip'][0][-1].detach().cpu().numpy())\n",
    "# ignore_collision = bool(test_update_dict['pred_action']['collision'][0][0].detach().cpu().numpy())\n",
    "\n",
    "# # gripper visualization pose\n",
    "# voxel_size = 0.045\n",
    "# voxel_scale = voxel_size * 100\n",
    "# # print(continuous_trans, continuous_quat)\n",
    "# gripper_pose = batch['gripper_state'][:, -1][0].detach().cpu().numpy()\n",
    "# continuous_trans = gripper_pose[:3]\n",
    "# continuous_quat = gripper_pose[3:7]\n",
    "# gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "#                                         SCENE_BOUNDS[:3],\n",
    "#                                         continuous_trans,\n",
    "#                                         continuous_quat)\n",
    "\n",
    "\n",
    "# # #@markdown #### Show Q-Prediction and Best Action\n",
    "# show_expert_action = True  #@param {type:\"boolean\"}\n",
    "# show_q_values = True  #@param {type:\"boolean\"}\n",
    "# render_gripper = True  #@param {type:\"boolean\"}\n",
    "# rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "# rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "#                             vis_trans_q if show_q_values else None,\n",
    "#                             [pred_trans_coord],\n",
    "#                             vis_gt_coord if show_expert_action else None,\n",
    "#                             voxel_size=voxel_size,\n",
    "#                             rotation_amount=np.deg2rad(0),\n",
    "#                             render_gripper=render_gripper,\n",
    "#                             gripper_pose=gripper_pose_mat,\n",
    "#                             gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "# rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "#                             vis_trans_q if show_q_values else None,\n",
    "#                             [pred_trans_coord],\n",
    "#                             vis_gt_coord if show_expert_action else None,\n",
    "#                             voxel_size=voxel_size,\n",
    "#                             rotation_amount=np.deg2rad(45),\n",
    "#                             render_gripper=render_gripper,\n",
    "#                             gripper_pose=gripper_pose_mat,\n",
    "#                             gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(20, 15))\n",
    "# fig.add_subplot(1, 2, 1)\n",
    "# plt.imshow(rendered_img_0)\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Front view\")\n",
    "# fig.add_subplot(1, 2, 2)\n",
    "# plt.imshow(rendered_img_270)\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Side view\")\n",
    "\n",
    "# # print(f\"Lang goal: {lang_goal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
