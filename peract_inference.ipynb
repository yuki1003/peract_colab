{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference, with everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_ # bad trick to fix numpy version issue :(\n",
    "import os\n",
    "import sys\n",
    "from natsort import natsorted\n",
    "\n",
    "sys.path = [p for p in sys.path if '/peract/' not in p]\n",
    "\n",
    "# Set `PYOPENGL_PLATFORM=egl` for pyrender visualizations\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\" # Depends on your computer and available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From github.com:yuki1003/peract_colab\n",
      " * branch              master     -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "### Depending on your workspace, you may already have this repository installe, otherwise clone once again\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'peract_colab')):\n",
    "    !git clone https://github.com/yuki1003/peract_colab.git\n",
    "\n",
    "!cd peract_colab && git pull origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if modules in peract_colab repository are recognized.\n",
    "try: # test import\n",
    "    from rlbench.utils import get_stored_demo\n",
    "except ImportError as error_message:\n",
    "    print(error_message)\n",
    "    print(\"Adding peract_colab repository to system path.\")\n",
    "    sys.path.append('peract_colab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from agent.perceiver_io import PerceiverIO\n",
    "from agent.peract_agent import PerceiverActorAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Total #: 32, indices: [45, 46, 47, 48, 49, 145, 146, 147, 148, 149, 245, 246, 247, 248, 249, 345, 346, 348, 349, 445, 446, 447, 448, 545, 546, 548, 549, 945, 946, 947, 948, 949]\n",
      "TEST | Total #: [645, 646, 647, 648]\n"
     ]
    }
   ],
   "source": [
    "#___DATA___\n",
    "TASK = 'handing_over_banana'\n",
    "\n",
    "# Data Constants\n",
    "WORKSPACE_DIR = os.getcwd()\n",
    "DATA_FOLDER = os.path.join(WORKSPACE_DIR, \"task_data\", \"handoversim\")\n",
    "EPISODES_FOLDER = os.path.join(TASK, \"all_variations\", \"episodes\")\n",
    "\n",
    "EPISODE_FOLDER = 'episode%d'\n",
    "SETUP = \"s1\" # Options: \"s1\"\n",
    "train_data_path = os.path.join(DATA_FOLDER, f\"train_{SETUP}\", EPISODES_FOLDER)\n",
    "TRAIN_INDEXES = [int(episode_nr.replace(\"episode\", \"\")) for episode_nr in natsorted(os.listdir(train_data_path))]\n",
    "test_data_path = os.path.join(DATA_FOLDER, f\"val_{SETUP}\", EPISODES_FOLDER)\n",
    "TEST_INDEXES = [int(episode_nr.replace(\"episode\", \"\")) for episode_nr in natsorted(os.listdir(test_data_path))]\n",
    "\n",
    "print(f\"TRAIN | Total #: {len(TRAIN_INDEXES)}, indices: {TRAIN_INDEXES}\")\n",
    "print(f\"TEST | Total #: {TEST_INDEXES}\")\n",
    "\n",
    "# Replaybuffer related constants\n",
    "LOW_DIM_SIZE = 4    # 4 dimensions - proprioception: {gripper_open, left_finger_joint, right_finger_joint, timestep}\n",
    "IMAGE_SIZE =  128  # 128x128 - if you want to use higher voxel resolutions like 200^3, you might want to regenerate the dataset with larger images\n",
    "ROTATION_RESOLUTION = 5 # degree increments per axis\n",
    "\n",
    "DEPTH_SCALE = 1000\n",
    "SCENE_BOUNDS = [0.11, -0.5, 0.8, 1.11, 0.5, 1.8]  # Must be 1m each\n",
    "\n",
    "# Training Settings Constants\n",
    "BATCH_SIZE = 1\n",
    "VOXEL_SIZES = [100]  # 100x100x100 voxels\n",
    "NUM_LATENTS = 512  # PerceiverIO latents: lower-dimension features of input data\n",
    "\n",
    "CAMERAS = [f\"view_{camera_i}\" for camera_i in range(3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#___AGENT___\n",
    "\n",
    "perceiver_encoder = PerceiverIO(\n",
    "    depth=6,\n",
    "    iterations=1,\n",
    "    voxel_size=VOXEL_SIZES[0],\n",
    "    initial_dim=3 + 3 + 1 + 3,\n",
    "    low_dim_size=4,\n",
    "    layer=0,\n",
    "    num_rotation_classes=72,\n",
    "    num_grip_classes=2,\n",
    "    num_collision_classes=2,\n",
    "    num_latents=NUM_LATENTS,\n",
    "    latent_dim=512,\n",
    "    cross_heads=1,\n",
    "    latent_heads=8,\n",
    "    cross_dim_head=64,\n",
    "    latent_dim_head=64,\n",
    "    weight_tie_layers=False,\n",
    "    activation='lrelu',\n",
    "    input_dropout=0.1,\n",
    "    attn_dropout=0.1,\n",
    "    decoder_dropout=0.0,\n",
    "    voxel_patch_size=5,\n",
    "    voxel_patch_stride=5,\n",
    "    final_dim=64,\n",
    ")\n",
    "\n",
    "peract_agent = PerceiverActorAgent(\n",
    "    coordinate_bounds=SCENE_BOUNDS,\n",
    "    perceiver_encoder=perceiver_encoder,\n",
    "    camera_names=CAMERAS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    voxel_size=VOXEL_SIZES[0],\n",
    "    voxel_feature_size=3,\n",
    "    num_rotation_classes=72,\n",
    "    rotation_resolution=ROTATION_RESOLUTION,\n",
    "    image_resolution=[IMAGE_SIZE, IMAGE_SIZE],\n",
    "    lambda_weight_l2=0.000001,\n",
    "    transform_augmentation=False,\n",
    "    rgb_augmentation=False,\n",
    "    optimizer_type='lamb',\n",
    ")\n",
    "peract_agent.build(training=False, device=device, language_goal=\"handing over banana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/home/ywatabe/Projects/PerAct/models/2024-11-29_04-23/best_model_train\" # Good\n",
    "# model_path = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-10_14-59/best_model_general\" # Good {non-uniform 1 kp}\n",
    "# model_path = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30/best_model_general\" # Bad {uniform 2 kp}\n",
    "# model_path = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30/best_model_general\" # Bad\n",
    "# model_path = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_13-34/best_model_general\"\n",
    "# model_path = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_17-25/best_model_general\" # {crop skip 10}\n",
    "# model_path = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_21-23/best_model_general\"\n",
    "model_path = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/best_model_general\" # {crop skip 10 new}\n",
    "\n",
    "peract_agent.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference: Run Inference on just observation data and Save as video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs\n",
    "from arm.utils import visualise_voxel\n",
    "from arm.utils import discrete_euler_to_quaternion, get_gripper_render_pose\n",
    "\n",
    "# What to visualize\n",
    "episode_idx_to_visualize = 646  # Index of the episode to visualize\n",
    "# Video output path\n",
    "video_output_path = f\"demo{episode_idx_to_visualize}_visualization.mp4\"\n",
    "\n",
    "# Get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                       index=episode_idx_to_visualize,\n",
    "                       cameras=CAMERAS,\n",
    "                       depth_scale=DEPTH_SCALE)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "\n",
    "# Open a video writer\n",
    "with imageio.get_writer(video_output_path, fps=10) as video_writer:\n",
    "    for ts in episode_length[::5]:\n",
    "        print(ts)\n",
    "        # Extract obs at timestep\n",
    "        obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "        # gripper_pose = demo[ts].gripper_pose\n",
    "        gripper_open = demo[ts].gripper_open\n",
    "        gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "        obs_dict[\"gripper_open\"] = gripper_open\n",
    "        obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "        (continuous_trans, continuous_quat, gripper_open, _, _), \\\n",
    "        (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent.forward(obs_dict, ts)\n",
    "        print(continuous_trans, continuous_quat, gripper_open)\n",
    "\n",
    "        # Things to visualize\n",
    "        vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "        pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "\n",
    "        voxel_size = 0.045\n",
    "        voxel_scale = voxel_size * 100\n",
    "        gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                                   SCENE_BOUNDS[:3],\n",
    "                                                   continuous_trans,\n",
    "                                                   continuous_quat)\n",
    "\n",
    "        rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                         None,\n",
    "                                         [pred_trans_coord],\n",
    "                                         None,\n",
    "                                         voxel_size=voxel_size,\n",
    "                                         rotation_amount=np.deg2rad(0),\n",
    "                                         render_gripper=True,\n",
    "                                         gripper_pose=gripper_pose_mat,\n",
    "                                         gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                           None,\n",
    "                                           [pred_trans_coord],\n",
    "                                           None,\n",
    "                                           voxel_size=voxel_size,\n",
    "                                           rotation_amount=np.deg2rad(45+180),\n",
    "                                           render_gripper=True,\n",
    "                                           gripper_pose=gripper_pose_mat,\n",
    "                                           gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        # Plot figures into a NumPy array\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        plt.imshow(rendered_img_0)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Front view\")\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        plt.imshow(rendered_img_270)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Side view\")\n",
    "\n",
    "        # Add timestamp as text with white font and black background\n",
    "        fig.text(0.02, 0.95, f\"Timestep: {ts}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                 bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "\n",
    "        # Convert the matplotlib figure to a NumPy array\n",
    "        fig.canvas.draw()\n",
    "        img_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        \n",
    "        video_writer.append_data(img_array)  # Add frame to video\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "print(f\"Video saved to {video_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Run Inference on just input data with ground truth to check what happends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_from_frame(episode_keypoints_gt_obs_dict, frame_idx):\n",
    "    # Get a sorted list of dictionary keys (frames with stored data)\n",
    "    episode_keypoints = sorted(episode_keypoints_gt_obs_dict.keys())\n",
    "\n",
    "    # Iterate through the given frame indices\n",
    "    # Find the smallest key that is greater than or equal to the current frame\n",
    "    for episode_kp in episode_keypoints:\n",
    "        if frame_idx <= episode_kp:\n",
    "            episode_kp_gt_obs_dict = episode_keypoints_gt_obs_dict[episode_kp]\n",
    "            return episode_kp_gt_obs_dict\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: NEED SOME WAY TO COLLECT THE GROUND TRUTH DATA FOR ALL FRAMES\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs_gt\n",
    "\n",
    "from arm.demo import _keypoint_discovery_available\n",
    "from arm.utils import point_to_voxel_index, visualise_voxel, get_gripper_render_pose\n",
    "\n",
    "\n",
    "# What to visualize\n",
    "episode_idx_to_visualize = 645  # Index of the episode to visualize\n",
    "# Video output path\n",
    "video_output_path = f\"demo{episode_idx_to_visualize}_gt_pred_visualization.mp4\"\n",
    "\n",
    "# Get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                       index=episode_idx_to_visualize,\n",
    "                       cameras=CAMERAS,\n",
    "                       depth_scale=DEPTH_SCALE)\n",
    "\n",
    "episode_keypoints_gt_obs_dict = dict()\n",
    "episode_keypoints = _keypoint_discovery_available(demo, approach_distance=0.3) #NOTE: Approach_distance Set\n",
    "episode_keypoints = [episode_keypoints[-1]]\n",
    "for episode_keypoint in episode_keypoints:\n",
    "    episode_keypoints_gt_obs_dict[episode_keypoint] = extract_obs_gt(obs = demo._observations[episode_keypoint],\n",
    "                                                                  cameras=CAMERAS)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "# episode_length = [40]\n",
    "\n",
    "# Open a video writer\n",
    "with imageio.get_writer(video_output_path, fps=10) as video_writer:\n",
    "    for ts in episode_length[::5]: # Skip some frames\n",
    "        print(ts)\n",
    "        # Extract obs at timestep\n",
    "        obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "        # gripper_pose = demo[ts].gripper_pose\n",
    "        gripper_open = demo[ts].gripper_open\n",
    "        gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "        obs_dict[\"gripper_open\"] = gripper_open\n",
    "        obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "        (continuous_trans, continuous_quat, gripper_open, trans_confidence, _), \\\n",
    "        (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent.forward(obs_dict, ts)\n",
    "\n",
    "        pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "        \n",
    "        # Get the ground truth\n",
    "        episode_keypoint_gt_obs_dict = get_gt_from_frame(episode_keypoints_gt_obs_dict, ts)\n",
    "        if not (episode_keypoint_gt_obs_dict is None):\n",
    "            gt_gripper_pose = episode_keypoint_gt_obs_dict[\"gripper_pose\"]\n",
    "            gt_trans_coord = point_to_voxel_index(gt_gripper_pose[:3], VOXEL_SIZES, SCENE_BOUNDS)[0]\n",
    "            error = np.linalg.norm(gt_gripper_pose[:3] - continuous_trans)\n",
    "            print(f\"GT (voxel): {gt_trans_coord} - Prediction (voxel): {pred_trans_coord} - Error: {error} - Prediction-score: {round(trans_confidence,4)}\")\n",
    "        else:\n",
    "            gt_trans_coord = None\n",
    "            error = False\n",
    "            print(\"GT coordinates not available for this frame\")\n",
    "        \n",
    "        # Things to visualize\n",
    "        vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "\n",
    "        voxel_size = 0.045\n",
    "        voxel_scale = voxel_size * 100\n",
    "        gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                                   SCENE_BOUNDS[:3],\n",
    "                                                   continuous_trans,\n",
    "                                                   continuous_quat)\n",
    "\n",
    "        rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                         None,\n",
    "                                         [pred_trans_coord],\n",
    "                                         gt_trans_coord,\n",
    "                                         alpha = 0.2,\n",
    "                                         voxel_size=voxel_size,\n",
    "                                         rotation_amount=np.deg2rad(0),\n",
    "                                         render_gripper=True,\n",
    "                                         gripper_pose=gripper_pose_mat,\n",
    "                                         gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                           None,\n",
    "                                           [pred_trans_coord],\n",
    "                                           gt_trans_coord,\n",
    "                                           alpha = 0.2,\n",
    "                                           voxel_size=voxel_size,\n",
    "                                           rotation_amount=np.deg2rad(45+180),\n",
    "                                           render_gripper=True,\n",
    "                                           gripper_pose=gripper_pose_mat,\n",
    "                                           gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        # Plot figures into a NumPy array\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        plt.imshow(rendered_img_0)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Front view\")\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        plt.imshow(rendered_img_270)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Side view\")\n",
    "\n",
    "        # Add timestamp as text with white font and black background\n",
    "        if error:\n",
    "            fig.text(0.02, 0.95, f\"Timestep: {ts}, Prediction-score: {round(trans_confidence,4)}, Error: {np.round(error, 3)}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                    bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "        else:\n",
    "            fig.text(0.02, 0.95, f\"Timestep: {ts}, Prediction-score: {round(trans_confidence,4)}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                    bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "\n",
    "        # Convert the matplotlib figure to a NumPy array\n",
    "        fig.canvas.draw()\n",
    "        img_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        \n",
    "        video_writer.append_data(img_array)  # Add frame to video\n",
    "        # plt.show()\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "        \n",
    "\n",
    "print(f\"Video saved to {video_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Run Inference with same batches as during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import clip\n",
    "\n",
    "from arm.replay_buffer import create_replay, fill_replay, uniform_fill_replay, fill_replay_copy_with_crop_from_approach, fill_replay_only_approach_test\n",
    "\n",
    "from yarr.replay_buffer.wrappers.pytorch_replay_buffer import PyTorchReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_OBJ_KEYPOINTS=False # Real - (changed later)\n",
    "TARGET_OBJ_USE_LAST_KP=False # Real - (changed later)\n",
    "TARGET_OBJ_IS_AVAIL = True # HandoverSim - (changed later)\n",
    "\n",
    "STOPPING_DELTA = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_replay_buffer(settings): # Taken from analyse_data.ipynb\n",
    "\n",
    "    # BATCH SETTINGS\n",
    "    FILL_REPLAY_SETTING = settings['fill_replay_setting']\n",
    "    CAMERAS = settings['cameras']\n",
    "    USE_APPROACH = settings['keypoint_approach']\n",
    "    DEMO_AUGMENTATION_EVERY_N = settings['demo_augm_n']\n",
    "\n",
    "    # Summary of run properties\n",
    "    print(\"\\nExperiment Setup\")\n",
    "    print(f\"Task: {TASK} - SETUP: {SETUP} - Cameras: {len(CAMERAS)}\")\n",
    "    print(\"Run Properties\")\n",
    "    print(f\"Fill replay setting: {FILL_REPLAY_SETTING} - DEMO_AUGM_N: {DEMO_AUGMENTATION_EVERY_N}\")\n",
    "\n",
    "    #___REPLAY-BUFFER___\n",
    "    train_replay_storage_dir = os.path.join(WORKSPACE_DIR,'replay_train')\n",
    "    if os.path.exists(train_replay_storage_dir):\n",
    "        print(f\"Emptying {train_replay_storage_dir}\")\n",
    "        shutil.rmtree(train_replay_storage_dir)\n",
    "    if not os.path.exists(train_replay_storage_dir):\n",
    "        print(f\"Could not find {train_replay_storage_dir}, creating directory.\")\n",
    "        os.mkdir(train_replay_storage_dir)\n",
    "\n",
    "    test_replay_storage_dir = os.path.join(WORKSPACE_DIR,'replay_test')\n",
    "    if os.path.exists(test_replay_storage_dir):\n",
    "        print(f\"Emptying {test_replay_storage_dir}\")\n",
    "        shutil.rmtree(test_replay_storage_dir)\n",
    "    if not os.path.exists(test_replay_storage_dir):\n",
    "        print(f\"Could not find {test_replay_storage_dir}, creating directory.\")\n",
    "        os.mkdir(test_replay_storage_dir)\n",
    "\n",
    "    train_replay_buffer = create_replay(batch_size=BATCH_SIZE,\n",
    "                                        timesteps=1,\n",
    "                                        save_dir=train_replay_storage_dir,\n",
    "                                        cameras=CAMERAS,\n",
    "                                        voxel_sizes=VOXEL_SIZES,\n",
    "                                        image_size=IMAGE_SIZE,\n",
    "                                        low_dim_size=LOW_DIM_SIZE)\n",
    "\n",
    "    test_replay_buffer = create_replay(batch_size=BATCH_SIZE,\n",
    "                                    timesteps=1,\n",
    "                                    save_dir=test_replay_storage_dir,\n",
    "                                    cameras=CAMERAS,\n",
    "                                    voxel_sizes=VOXEL_SIZES,\n",
    "                                    image_size=IMAGE_SIZE,\n",
    "                                    low_dim_size=LOW_DIM_SIZE)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    clip_model, preprocess = clip.load(\"RN50\", device=device) # CLIP-ResNet50\n",
    "\n",
    "    print(\"-- Train Buffer --\")\n",
    "    if FILL_REPLAY_SETTING.lower() == \"uniform\":\n",
    "        uniform_fill_replay(\n",
    "            data_path=train_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=train_replay_buffer,\n",
    "            d_indexes=TRAIN_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    elif FILL_REPLAY_SETTING.lower() == \"crop\":\n",
    "        fill_replay_copy_with_crop_from_approach(\n",
    "            data_path=train_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=train_replay_buffer,\n",
    "            d_indexes=TRAIN_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    elif FILL_REPLAY_SETTING.lower() == \"standard\":\n",
    "        fill_replay_only_approach_test(\n",
    "        # fill_replay(\n",
    "            data_path=train_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=train_replay_buffer,\n",
    "            d_indexes=TRAIN_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\"Unkown setting for fill replay buffer\")\n",
    "\n",
    "        \n",
    "    print(\"-- Test Buffer --\")\n",
    "    if FILL_REPLAY_SETTING.lower() == \"uniform\":\n",
    "        uniform_fill_replay(\n",
    "            data_path=test_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=test_replay_buffer,\n",
    "            d_indexes=TEST_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    elif FILL_REPLAY_SETTING.lower() == \"crop\":\n",
    "        fill_replay_copy_with_crop_from_approach(\n",
    "            data_path=test_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=test_replay_buffer,\n",
    "            d_indexes=TEST_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    elif FILL_REPLAY_SETTING.lower() == \"standard\":\n",
    "        fill_replay_only_approach_test(\n",
    "        # fill_replay(\n",
    "            data_path=test_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=test_replay_buffer,\n",
    "            d_indexes=TEST_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\"Unkown setting for fill replay buffer\")\n",
    "\n",
    "\n",
    "    # delete the CLIP model since we have already extracted language features\n",
    "    del clip_model\n",
    "\n",
    "    # wrap buffer with PyTorch dataset and make iterator\n",
    "    train_wrapped_replay = PyTorchReplayBuffer(train_replay_buffer)\n",
    "    train_dataset = train_wrapped_replay.dataset()\n",
    "    train_data_iter = iter(train_dataset)\n",
    "\n",
    "    test_wrapped_replay = PyTorchReplayBuffer(test_replay_buffer)\n",
    "    test_dataset = test_wrapped_replay.dataset()\n",
    "    test_data_iter = iter(test_dataset)\n",
    "\n",
    "    return train_data_iter, test_data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Setup\n",
      "Task: handing_over_banana - SETUP: s1 - Cameras: 3\n",
      "Run Properties\n",
      "Fill replay setting: crop - DEMO_AUGM_N: 10\n",
      "Emptying /home/ywatabe/Projects/PerAct/replay_train\n",
      "Could not find /home/ywatabe/Projects/PerAct/replay_train, creating directory.\n",
      "Emptying /home/ywatabe/Projects/PerAct/replay_test\n",
      "Could not find /home/ywatabe/Projects/PerAct/replay_test, creating directory.\n",
      "-- Train Buffer --\n",
      "Filling replay ...\n",
      "Filling demo 45\n",
      "Found 2 keypoints: [34, 58]. Using keypoint:58 and sampling till 34\n",
      "Using available\n",
      "Filling demo 46\n",
      "Found 2 keypoints: [35, 61]. Using keypoint:61 and sampling till 35\n",
      "Using available\n",
      "Filling demo 47\n",
      "Found 2 keypoints: [29, 60]. Using keypoint:60 and sampling till 29\n",
      "Using available\n",
      "Filling demo 48\n",
      "Found 2 keypoints: [28, 60]. Using keypoint:60 and sampling till 28\n",
      "Using available\n",
      "Filling demo 49\n",
      "Found 2 keypoints: [17, 54]. Using keypoint:54 and sampling till 17\n",
      "Using available\n",
      "Filling demo 145\n",
      "Found 2 keypoints: [26, 56]. Using keypoint:56 and sampling till 26\n",
      "Using available\n",
      "Filling demo 146\n",
      "Found 2 keypoints: [13, 52]. Using keypoint:52 and sampling till 13\n",
      "Using available\n",
      "Filling demo 147\n",
      "Found 2 keypoints: [25, 51]. Using keypoint:51 and sampling till 25\n",
      "Using available\n",
      "Filling demo 148\n",
      "Found 2 keypoints: [26, 52]. Using keypoint:52 and sampling till 26\n",
      "Using available\n",
      "Filling demo 149\n",
      "Found 2 keypoints: [26, 50]. Using keypoint:50 and sampling till 26\n",
      "Using available\n",
      "Filling demo 245\n",
      "Found 2 keypoints: [24, 52]. Using keypoint:52 and sampling till 24\n",
      "Using available\n",
      "Filling demo 246\n",
      "Found 2 keypoints: [20, 50]. Using keypoint:50 and sampling till 20\n",
      "Using available\n",
      "Filling demo 247\n",
      "Found 2 keypoints: [16, 51]. Using keypoint:51 and sampling till 16\n",
      "Using available\n",
      "Filling demo 248\n",
      "Found 2 keypoints: [17, 50]. Using keypoint:50 and sampling till 17\n",
      "Using available\n",
      "Filling demo 249\n",
      "Found 2 keypoints: [18, 50]. Using keypoint:50 and sampling till 18\n",
      "Using available\n",
      "Filling demo 345\n",
      "Found 2 keypoints: [24, 52]. Using keypoint:52 and sampling till 24\n",
      "Using available\n",
      "Filling demo 346\n",
      "Found 2 keypoints: [19, 49]. Using keypoint:49 and sampling till 19\n",
      "Using available\n",
      "Filling demo 348\n",
      "Found 2 keypoints: [29, 57]. Using keypoint:57 and sampling till 29\n",
      "Using available\n",
      "Filling demo 349\n",
      "Found 2 keypoints: [24, 51]. Using keypoint:51 and sampling till 24\n",
      "Using available\n",
      "Filling demo 445\n",
      "Found 2 keypoints: [30, 53]. Using keypoint:53 and sampling till 30\n",
      "Using available\n",
      "Filling demo 446\n",
      "Found 2 keypoints: [22, 55]. Using keypoint:55 and sampling till 22\n",
      "Using available\n",
      "Filling demo 447\n",
      "Found 2 keypoints: [27, 52]. Using keypoint:52 and sampling till 27\n",
      "Using available\n",
      "Filling demo 448\n",
      "Found 2 keypoints: [24, 54]. Using keypoint:54 and sampling till 24\n",
      "Using available\n",
      "Filling demo 545\n",
      "Found 2 keypoints: [28, 53]. Using keypoint:53 and sampling till 28\n",
      "Using available\n",
      "Filling demo 546\n",
      "Found 2 keypoints: [24, 49]. Using keypoint:49 and sampling till 24\n",
      "Using available\n",
      "Filling demo 548\n",
      "Found 2 keypoints: [24, 54]. Using keypoint:54 and sampling till 24\n",
      "Using available\n",
      "Filling demo 549\n",
      "Found 2 keypoints: [24, 54]. Using keypoint:54 and sampling till 24\n",
      "Using available\n",
      "Filling demo 945\n",
      "Found 2 keypoints: [30, 54]. Using keypoint:54 and sampling till 30\n",
      "Using available\n",
      "Filling demo 946\n",
      "Found 2 keypoints: [22, 51]. Using keypoint:51 and sampling till 22\n",
      "Using available\n",
      "Filling demo 947\n",
      "Found 2 keypoints: [24, 54]. Using keypoint:54 and sampling till 24\n",
      "Using available\n",
      "Filling demo 948\n",
      "Found 2 keypoints: [53, 82]. Using keypoint:82 and sampling till 53\n",
      "Using available\n",
      "Filling demo 949\n",
      "Found 2 keypoints: [23, 49]. Using keypoint:49 and sampling till 23\n",
      "Using available\n",
      "Replay filled with demos.\n",
      "-- Test Buffer --\n",
      "Filling replay ...\n",
      "Filling demo 645\n",
      "Found 2 keypoints: [28, 59]. Using keypoint:59 and sampling till 28\n",
      "Using available\n",
      "Filling demo 646\n",
      "Found 2 keypoints: [25, 55]. Using keypoint:55 and sampling till 25\n",
      "Using available\n",
      "Filling demo 647\n",
      "Found 2 keypoints: [30, 60]. Using keypoint:60 and sampling till 30\n",
      "Using available\n",
      "Filling demo 648\n",
      "Found 2 keypoints: [23, 53]. Using keypoint:53 and sampling till 23\n",
      "Using available\n",
      "Replay filled with demos.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/\" # {crop skip 10 new}\n",
    "\n",
    "path_settings = os.path.join(os.path.dirname(run_dir), \"training_settings.json\")\n",
    "with open(path_settings, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "train_data_iter, test_data_iter = load_replay_buffer(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights from /home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/run0/best_model_train/peract_agent.pt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "loaded weights from /home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/run1000/best_model_train/peract_agent.pt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m lang_goal \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang_goal\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(v) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor}\n\u001b[0;32m---> 24\u001b[0m update_dict \u001b[38;5;241m=\u001b[39m \u001b[43mperact_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackprop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m prediction_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(update_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_trans\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     26\u001b[0m pred_trans \u001b[38;5;241m=\u001b[39m update_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_action\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrans\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/PerAct/peract_colab/agent/peract_agent.py:258\u001b[0m, in \u001b[0;36mPerceiverActorAgent.update\u001b[0;34m(self, step, replay_sample, backprop)\u001b[0m\n\u001b[1;32m    254\u001b[0m         object_bbox_pcd \u001b[38;5;241m=\u001b[39m perturb_se3(object_bbox_pcd, trans_shift_4x4, rot_shift_4x4, action_gripper_4x4, bounds)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Q function TODO: I think forward of Qfunction\u001b[39;00m\n\u001b[1;32m    257\u001b[0m q_trans, rot_grip_q, collision_q, voxel_grid \\\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m              \u001b[49m\u001b[43mproprio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpcd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m              \u001b[49m\u001b[43mlang_goal_embs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbackprop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rgb_augmentation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m              \u001b[49m\u001b[43mgripper_bbox_pcd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_bbox_pcd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# NOTE: This is the prediction\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# one-hot expert actions\u001b[39;00m\n\u001b[1;32m    269\u001b[0m action_trans_one_hot, action_rot_x_one_hot, \\\n\u001b[1;32m    270\u001b[0m action_rot_y_one_hot, action_rot_z_one_hot, \\\n\u001b[1;32m    271\u001b[0m action_grip_one_hot, action_collision_one_hot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_one_hot_expert_actions(bs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m                                                                                  action_ignore_collisions,\n\u001b[1;32m    275\u001b[0m                                                                                  device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/PerAct/peract_colab/agent/q_function.py:126\u001b[0m, in \u001b[0;36mQFunction.forward\u001b[0;34m(self, obs, proprio, pcd, lang_goal_embs, bounds, backprop, rgb_augmentation, gripper_bbox_pcd, object_bbox_pcd)\u001b[0m\n\u001b[1;32m    123\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m bounds\u001b[38;5;241m.\u001b[39mrepeat(bs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# forward pass (TODO: I think forward() of PerceiverIO)\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m q_trans, rot_and_grip_q, collision_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_qnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvoxel_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mproprio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mlang_goal_embs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_trans, rot_and_grip_q, collision_q, voxel_grid\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/PerAct/peract_colab/agent/perceiver_io.py:360\u001b[0m, in \u001b[0;36mPerceiverIO.forward\u001b[0;34m(self, ins, proprio, lang_goal_embs, bounds, mask)\u001b[0m\n\u001b[1;32m    357\u001b[0m feats\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mss1(latents\u001b[38;5;241m.\u001b[39mcontiguous()), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_maxp(latents)\u001b[38;5;241m.\u001b[39mview(b, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# upsample layer\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m u0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m)\u001b[49m                         \u001b[38;5;66;03m# [B,64,100,100,100]\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# skip connection like in UNets\u001b[39;00m\n\u001b[1;32m    363\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal(torch\u001b[38;5;241m.\u001b[39mcat([d0, u0], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))     \u001b[38;5;66;03m# [B,64+64,100,100,100] -> [B,64,100,100,100]\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/PerAct/peract_colab/arm/network_utils.py:253\u001b[0m, in \u001b[0;36mConv3DUpsampleBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/PerAct/peract_colab/arm/network_utils.py:166\u001b[0m, in \u001b[0;36mConv3DBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 166\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    168\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/PerAct/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:592\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conv_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 592\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reversed_padding_repeated_twice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_triple\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    605\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_runs = natsorted([os.path.join(run_dir, run) for run in os.listdir(run_dir) if \"run\" in run])\n",
    "\n",
    "chosen_model = \"best_model_train\"\n",
    "\n",
    "model_run_distances = dict()\n",
    "model_run_scores = dict()\n",
    "\n",
    "for model_run_iter in model_runs:\n",
    "    \n",
    "    try:\n",
    "        peract_agent.load_weights(os.path.join(model_run_iter, chosen_model))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    distances_run = []\n",
    "    scores_run = []\n",
    "    \n",
    "    for i in range(20): # collest using 100 samples\n",
    "        print(i)\n",
    "        batch = next(train_data_iter) # NOTE: Choose which set to infer\n",
    "        lang_goal = batch['lang_goal'][0][0][0]\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "        \n",
    "        update_dict = peract_agent.update(None, batch, backprop=False)\n",
    "        prediction_score = round(update_dict[\"q_trans\"].max().item(), 4)\n",
    "        pred_trans = update_dict[\"pred_action\"][\"trans\"].detach().cpu().numpy()[0]\n",
    "        gt_trans = update_dict[\"expert_action\"][\"action_trans\"].detach().cpu().numpy()[0]\n",
    "        dist = np.round(np.linalg.norm(pred_trans-gt_trans), 4)\n",
    "        \n",
    "        distances_run.append(dist)\n",
    "        scores_run.append(prediction_score)\n",
    "    \n",
    "    zipped_lists = zip(distances_run, scores_run)\n",
    "\n",
    "    sorted_lists = sorted(zipped_lists, key=lambda x: x[0])\n",
    "\n",
    "    sorted_distances, sorted_scores = zip(*sorted_lists)\n",
    "    model_run_distances[os.path.dirname(model_run_iter)] = sorted_distances\n",
    "    model_run_scores[os.path.dirname(model_run_iter)] = sorted_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs\n",
    "\n",
    "batch = next(train_data_iter)\n",
    "\n",
    "# what to visualize\n",
    "episode_idx_to_visualize = 846#INDEXES[0] # out of 10 demos\n",
    "# ts = 70#25 # timestep out of total timesteps\n",
    "\n",
    "# get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                    index=episode_idx_to_visualize,\n",
    "                    cameras=CAMERAS,\n",
    "                    depth_scale=DEPTH_SCALE,)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "for ts in episode_length:\n",
    "\n",
    "    # extract obs at timestep\n",
    "    obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "    gripper_pose = demo[ts].gripper_pose\n",
    "    gripper_open = demo[ts].gripper_open\n",
    "    gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "    # obs_dict[\"gripper_pose\"] = gripper_pose\n",
    "    obs_dict[\"gripper_open\"] = gripper_open\n",
    "    obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "    # plot rgb and depth at timestep\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    rows, cols = 2, len(CAMERAS)\n",
    "\n",
    "    plot_idx = 1\n",
    "    for camera in CAMERAS:\n",
    "        # rgb\n",
    "        rgb_name = \"%s_%s\" % (camera, 'rgb')\n",
    "        rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
    "        fig.add_subplot(rows, cols, plot_idx)\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"%s_rgb | step %s\" % (camera, ts))\n",
    "\n",
    "        # depth\n",
    "        depth_name = \"%s_%s\" % (camera, 'depth')\n",
    "        depth = np.transpose(obs_dict[depth_name], (1, 2, 0))\n",
    "        fig.add_subplot(rows, cols, plot_idx+len(CAMERAS))\n",
    "        plt.imshow(depth)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"%s_depth | step %s\" % (camera, ts))\n",
    "\n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(obs_dict)\n",
    "\n",
    "    (continuous_trans, continuous_quat, gripper_open), (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent_inference.forward(obs_dict, ts)\n",
    "\n",
    "    from arm.utils import visualise_voxel\n",
    "    from arm.utils import discrete_euler_to_quaternion, get_gripper_render_pose\n",
    "\n",
    "    # things to visualize\n",
    "    vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "    pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "\n",
    "    # discrete to continuous\n",
    "    continuous_trans = continuous_trans[0].detach().cpu().numpy()\n",
    "    continuous_quat = discrete_euler_to_quaternion(rot_and_grip_indices[0][:3].detach().cpu().numpy(),\n",
    "                                                resolution=peract_agent_inference._rotation_resolution)\n",
    "    gripper_open = bool(rot_and_grip_indices[0][-1].detach().cpu().numpy())\n",
    "    ignore_collision = bool(test_update_dict['pred_action']['collision'][0][0].detach().cpu().numpy())\n",
    "\n",
    "    # # gripper visualization pose\n",
    "    voxel_size = 0.045\n",
    "    voxel_scale = voxel_size * 100\n",
    "    gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                            SCENE_BOUNDS[:3],\n",
    "                                            continuous_trans,\n",
    "                                            continuous_quat)\n",
    "\n",
    "    # #@markdown #### Show Q-Prediction and Best Action\n",
    "    show_expert_action = True  #@param {type:\"boolean\"}\n",
    "    show_q_values = False  #@param {type:\"boolean\"}\n",
    "    render_gripper = False  #@param {type:\"boolean\"}\n",
    "    rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "    rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(0),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale,\n",
    "                                perspective=False)\n",
    "\n",
    "    rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(45),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(rendered_img_0)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Front view\")\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(rendered_img_270)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Side view\")\n",
    "\n",
    "\n",
    "    # #@markdown #### Show Q-Prediction and Best Action\n",
    "    show_expert_action = True  #@param {type:\"boolean\"}\n",
    "    show_q_values = True  #@param {type:\"boolean\"}\n",
    "    render_gripper = True  #@param {type:\"boolean\"}\n",
    "    rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "    rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(0),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "    rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(45),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(rendered_img_0)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Front view\")\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(rendered_img_270)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Side view\")\n",
    "\n",
    "    print(f\"Lang goal: {lang_goal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from arm.utils import discrete_euler_to_quaternion, get_gripper_render_pose\n",
    "\n",
    "# batch = next(test_data_iter)\n",
    "\n",
    "# lang_goal = batch['lang_goal'][0][0][0]\n",
    "# print(lang_goal)\n",
    "\n",
    "# batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "# test_update_dict = peract_agent_inference.update(0, batch, backprop=False) # Here backprop == False: for evaluation, hence test_loss == total_loss\n",
    "\n",
    "# # Log test metrics\n",
    "# test_metrics = {\n",
    "#     \"total_loss\": test_update_dict['total_loss'],\n",
    "#     \"trans_loss\": test_update_dict['trans_loss'],\n",
    "#     \"rot_loss\": test_update_dict['rot_loss'],\n",
    "#     \"col_loss\": test_update_dict['col_loss']\n",
    "# }\n",
    "# for episode_kp, value in test_metrics.items():\n",
    "#     print(episode_kp, value)\n",
    "\n",
    "\n",
    "# from arm.utils import visualise_voxel\n",
    "\n",
    "# # # things to visualize\n",
    "# vis_voxel_grid = test_update_dict['voxel_grid'][0].detach().cpu().numpy()\n",
    "# vis_trans_q = test_update_dict['q_trans'][0].detach().cpu().numpy()\n",
    "# pred_trans_coord = test_update_dict['pred_action']['trans'][0].detach().cpu().numpy().tolist()\n",
    "# vis_gt_coord = test_update_dict['expert_action']['action_trans'][0].detach().cpu().numpy()\n",
    "\n",
    "# # discrete to continuous\n",
    "# continuous_trans = test_update_dict['pred_action']['continuous_trans'][0].detach().cpu().numpy()\n",
    "# continuous_quat = discrete_euler_to_quaternion(test_update_dict['pred_action']['rot_and_grip'][0][:3].detach().cpu().numpy(),\n",
    "#                                             resolution=peract_agent_inference._rotation_resolution)\n",
    "# gripper_open = bool(test_update_dict['pred_action']['rot_and_grip'][0][-1].detach().cpu().numpy())\n",
    "# ignore_collision = bool(test_update_dict['pred_action']['collision'][0][0].detach().cpu().numpy())\n",
    "\n",
    "# # gripper visualization pose\n",
    "# voxel_size = 0.045\n",
    "# voxel_scale = voxel_size * 100\n",
    "# # print(continuous_trans, continuous_quat)\n",
    "# gripper_pose = batch['gripper_state'][:, -1][0].detach().cpu().numpy()\n",
    "# continuous_trans = gripper_pose[:3]\n",
    "# continuous_quat = gripper_pose[3:7]\n",
    "# gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "#                                         SCENE_BOUNDS[:3],\n",
    "#                                         continuous_trans,\n",
    "#                                         continuous_quat)\n",
    "\n",
    "\n",
    "# # #@markdown #### Show Q-Prediction and Best Action\n",
    "# show_expert_action = True  #@param {type:\"boolean\"}\n",
    "# show_q_values = True  #@param {type:\"boolean\"}\n",
    "# render_gripper = True  #@param {type:\"boolean\"}\n",
    "# rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "# rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "#                             vis_trans_q if show_q_values else None,\n",
    "#                             [pred_trans_coord],\n",
    "#                             vis_gt_coord if show_expert_action else None,\n",
    "#                             voxel_size=voxel_size,\n",
    "#                             rotation_amount=np.deg2rad(0),\n",
    "#                             render_gripper=render_gripper,\n",
    "#                             gripper_pose=gripper_pose_mat,\n",
    "#                             gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "# rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "#                             vis_trans_q if show_q_values else None,\n",
    "#                             [pred_trans_coord],\n",
    "#                             vis_gt_coord if show_expert_action else None,\n",
    "#                             voxel_size=voxel_size,\n",
    "#                             rotation_amount=np.deg2rad(45),\n",
    "#                             render_gripper=render_gripper,\n",
    "#                             gripper_pose=gripper_pose_mat,\n",
    "#                             gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(20, 15))\n",
    "# fig.add_subplot(1, 2, 1)\n",
    "# plt.imshow(rendered_img_0)\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Front view\")\n",
    "# fig.add_subplot(1, 2, 2)\n",
    "# plt.imshow(rendered_img_270)\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Side view\")\n",
    "\n",
    "# # print(f\"Lang goal: {lang_goal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
