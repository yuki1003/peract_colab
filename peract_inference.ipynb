{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Trained PerAct Agent on handoversim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_ # bad trick to fix numpy version issue :(\n",
    "import os\n",
    "import sys\n",
    "sys.path = [p for p in sys.path if '/peract/' not in p]\n",
    "\n",
    "# Set `PYOPENGL_PLATFORM=egl` for pyrender visualizations\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\" # Depends on your computer and available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from notebook_helpers.constants import * # Load global constant variables from constants.py\n",
    "from notebook_helpers.build_training import build_agent\n",
    "\n",
    "# Choose the run\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-10_14-59\" # Good {non-uniform 1 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad {uniform 2 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_13-34\"\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_17-25\" # {crop skip 10}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_21-23\"\n",
    "run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/\" # {crop skip 10 new}\n",
    "\n",
    "# Obtain settings\n",
    "path_settings = os.path.join(os.path.dirname(run_dir), \"training_settings.json\")\n",
    "with open(path_settings, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "CAMERAS = settings['cameras']\n",
    "\n",
    "if BATCH_SIZE != 1:\n",
    "    raise ValueError(\"For Inference, 'batch_size' must be set to 1 in constants.py.\")\n",
    "\n",
    "peract_agent = build_agent(settings, training=False)\n",
    "peract_agent.set_language_goal(\"handing over banana\")\n",
    "\n",
    "# Choose model\n",
    "iteration = \"run8000\"\n",
    "best_type = \"best_model_general\"\n",
    "model_path = os.path.join(run_dir, iteration, best_type)\n",
    "\n",
    "peract_agent.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference: Run Inference on just observation data and Save as video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs\n",
    "from arm.utils import visualise_voxel\n",
    "from arm.utils import get_gripper_render_pose\n",
    "\n",
    "\n",
    "# What to visualize\n",
    "episode_idx_to_visualize = 646  # Index of the episode to visualize\n",
    "# Video output path\n",
    "video_output_path = f\"demo{episode_idx_to_visualize}_visualization.mp4\"\n",
    "\n",
    "# Get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                       index=episode_idx_to_visualize,\n",
    "                       cameras=CAMERAS,\n",
    "                       depth_scale=DEPTH_SCALE)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "\n",
    "# Open a video writer\n",
    "with imageio.get_writer(video_output_path, fps=10) as video_writer:\n",
    "    for ts in episode_length[::5]:\n",
    "        print(ts)\n",
    "        # Extract obs at timestep\n",
    "        obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "        # gripper_pose = demo[ts].gripper_pose\n",
    "        gripper_open = demo[ts].gripper_open\n",
    "        gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "        obs_dict[\"gripper_open\"] = gripper_open\n",
    "        obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "        (continuous_trans, continuous_quat, gripper_open, _, _), \\\n",
    "        (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent.forward(obs_dict, ts)\n",
    "        print(continuous_trans, continuous_quat, gripper_open)\n",
    "\n",
    "        # Things to visualize\n",
    "        vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "        pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "\n",
    "        voxel_size = 0.045\n",
    "        voxel_scale = voxel_size * 100\n",
    "        gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                                   SCENE_BOUNDS[:3],\n",
    "                                                   continuous_trans,\n",
    "                                                   continuous_quat)\n",
    "\n",
    "        rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                         None,\n",
    "                                         [pred_trans_coord],\n",
    "                                         None,\n",
    "                                         voxel_size=voxel_size,\n",
    "                                         rotation_amount=np.deg2rad(0),\n",
    "                                         render_gripper=True,\n",
    "                                         gripper_pose=gripper_pose_mat,\n",
    "                                         gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                           None,\n",
    "                                           [pred_trans_coord],\n",
    "                                           None,\n",
    "                                           voxel_size=voxel_size,\n",
    "                                           rotation_amount=np.deg2rad(45+180),\n",
    "                                           render_gripper=True,\n",
    "                                           gripper_pose=gripper_pose_mat,\n",
    "                                           gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        # Plot figures into a NumPy array\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        plt.imshow(rendered_img_0)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Front view\")\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        plt.imshow(rendered_img_270)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Side view\")\n",
    "\n",
    "        # Add timestamp as text with white font and black background\n",
    "        fig.text(0.02, 0.95, f\"Timestep: {ts}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                 bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "\n",
    "        # Convert the matplotlib figure to a NumPy array\n",
    "        fig.canvas.draw()\n",
    "        img_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        \n",
    "        video_writer.append_data(img_array)  # Add frame to video\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "print(f\"Video saved to {video_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Run Inference on just input data with ground truth to check what happends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_from_frame(episode_keypoints_gt_obs_dict, frame_idx):\n",
    "    # Get a sorted list of dictionary keys (frames with stored data)\n",
    "    episode_keypoints = sorted(episode_keypoints_gt_obs_dict.keys())\n",
    "\n",
    "    # Iterate through the given frame indices\n",
    "    # Find the smallest key that is greater than or equal to the current frame\n",
    "    for episode_kp in episode_keypoints:\n",
    "        if frame_idx <= episode_kp:\n",
    "            episode_kp_gt_obs_dict = episode_keypoints_gt_obs_dict[episode_kp]\n",
    "            return episode_kp_gt_obs_dict\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: NEED SOME WAY TO COLLECT THE GROUND TRUTH DATA FOR ALL FRAMES\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs_gt\n",
    "\n",
    "from arm.demo import _keypoint_discovery_available\n",
    "from arm.utils import point_to_voxel_index, visualise_voxel, get_gripper_render_pose\n",
    "\n",
    "\n",
    "# What to visualize\n",
    "episode_idx_to_visualize = 645  # Index of the episode to visualize\n",
    "# Video output path\n",
    "video_output_path = f\"demo{episode_idx_to_visualize}_gt_pred_visualization.mp4\"\n",
    "\n",
    "# Get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                       index=episode_idx_to_visualize,\n",
    "                       cameras=CAMERAS,\n",
    "                       depth_scale=DEPTH_SCALE)\n",
    "\n",
    "episode_keypoints_gt_obs_dict = dict()\n",
    "episode_keypoints = _keypoint_discovery_available(demo, approach_distance=0.3) #NOTE: Approach_distance Set\n",
    "episode_keypoints = [episode_keypoints[-1]]\n",
    "for episode_keypoint in episode_keypoints:\n",
    "    episode_keypoints_gt_obs_dict[episode_keypoint] = extract_obs_gt(obs = demo._observations[episode_keypoint],\n",
    "                                                                  cameras=CAMERAS)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "# episode_length = [40]\n",
    "\n",
    "# Open a video writer\n",
    "with imageio.get_writer(video_output_path, fps=10) as video_writer:\n",
    "    for ts in episode_length[::5]: # Skip some frames\n",
    "        print(ts)\n",
    "        # Extract obs at timestep\n",
    "        obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "        # gripper_pose = demo[ts].gripper_pose\n",
    "        gripper_open = demo[ts].gripper_open\n",
    "        gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "        obs_dict[\"gripper_open\"] = gripper_open\n",
    "        obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "        (continuous_trans, continuous_quat, gripper_open, trans_confidence, _), \\\n",
    "        (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent.forward(obs_dict, ts)\n",
    "\n",
    "        pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "        \n",
    "        # Get the ground truth\n",
    "        episode_keypoint_gt_obs_dict = get_gt_from_frame(episode_keypoints_gt_obs_dict, ts)\n",
    "        if not (episode_keypoint_gt_obs_dict is None):\n",
    "            gt_gripper_pose = episode_keypoint_gt_obs_dict[\"gripper_pose\"]\n",
    "            gt_trans_coord = point_to_voxel_index(gt_gripper_pose[:3], VOXEL_SIZES, SCENE_BOUNDS)[0]\n",
    "            error = np.linalg.norm(gt_gripper_pose[:3] - continuous_trans)\n",
    "            print(f\"GT (voxel): {gt_trans_coord} - Prediction (voxel): {pred_trans_coord} - Error: {error} - Prediction-score: {round(trans_confidence,4)}\")\n",
    "        else:\n",
    "            gt_trans_coord = None\n",
    "            error = False\n",
    "            print(\"GT coordinates not available for this frame\")\n",
    "        \n",
    "        # Things to visualize\n",
    "        vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "\n",
    "        voxel_size = 0.045\n",
    "        voxel_scale = voxel_size * 100\n",
    "        gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                                   SCENE_BOUNDS[:3],\n",
    "                                                   continuous_trans,\n",
    "                                                   continuous_quat)\n",
    "\n",
    "        rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                         None,\n",
    "                                         [pred_trans_coord],\n",
    "                                         gt_trans_coord,\n",
    "                                         alpha = 0.2,\n",
    "                                         voxel_size=voxel_size,\n",
    "                                         rotation_amount=np.deg2rad(0),\n",
    "                                         render_gripper=True,\n",
    "                                         gripper_pose=gripper_pose_mat,\n",
    "                                         gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                           None,\n",
    "                                           [pred_trans_coord],\n",
    "                                           gt_trans_coord,\n",
    "                                           alpha = 0.2,\n",
    "                                           voxel_size=voxel_size,\n",
    "                                           rotation_amount=np.deg2rad(45+180),\n",
    "                                           render_gripper=True,\n",
    "                                           gripper_pose=gripper_pose_mat,\n",
    "                                           gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        # Plot figures into a NumPy array\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        plt.imshow(rendered_img_0)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Front view\")\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        plt.imshow(rendered_img_270)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Side view\")\n",
    "\n",
    "        # Add timestamp as text with white font and black background\n",
    "        if error:\n",
    "            fig.text(0.02, 0.95, f\"Timestep: {ts}, Prediction-score: {round(trans_confidence,4)}, Error: {np.round(error, 3)}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                    bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "        else:\n",
    "            fig.text(0.02, 0.95, f\"Timestep: {ts}, Prediction-score: {round(trans_confidence,4)}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                    bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "\n",
    "        # Convert the matplotlib figure to a NumPy array\n",
    "        fig.canvas.draw()\n",
    "        img_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        \n",
    "        video_writer.append_data(img_array)  # Add frame to video\n",
    "        # plt.show()\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "        \n",
    "\n",
    "print(f\"Video saved to {video_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Run Inference with same batches as during training (You can change `batch_size` in <i>constants.py</i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from notebook_helpers.build_replay import load_replay_buffer\n",
    "\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/\" # {crop skip 10 new}\n",
    "\n",
    "# path_settings = os.path.join(os.path.dirname(run_dir), \"training_settings.json\")\n",
    "# with open(path_settings, 'r') as f:\n",
    "#     settings = json.load(f)\n",
    "\n",
    "# train_data_iter, test_data_iter = load_replay_buffer(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from notebook_helpers.constants import * # Load global constant variables from constants.py\n",
    "from notebook_helpers.build_training import build_agent\n",
    "from notebook_helpers.build_replay import load_replay_buffer\n",
    "\n",
    "# Choose the run\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-10_14-59\" # Good {non-uniform 1 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad {uniform 2 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_13-34\"\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_17-25\" # {crop skip 10}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_21-23\"\n",
    "run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/\" # {crop skip 10 new}\n",
    "\n",
    "# Obtain settings\n",
    "path_settings = os.path.join(os.path.dirname(run_dir), \"training_settings.json\")\n",
    "with open(path_settings, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "train_data_iter, test_data_iter = load_replay_buffer(settings)\n",
    "\n",
    "peract_agent = build_agent(settings, training=True)\n",
    "peract_agent.set_language_goal(\"handing over banana\")\n",
    "\n",
    "# Choose model\n",
    "iteration = \"run8000\"\n",
    "best_type = \"best_model_general\"\n",
    "model_path = os.path.join(run_dir, iteration, best_type)\n",
    "\n",
    "peract_agent.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs\n",
    "\n",
    "CAMERAS = settings[\"cameras\"]\n",
    "\n",
    "batch = next(train_data_iter)\n",
    "\n",
    "# what to visualize\n",
    "episode_idx_to_visualize = 646#INDEXES[0] # out of 10 demos\n",
    "# ts = 70#25 # timestep out of total timesteps\n",
    "\n",
    "# get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                    index=episode_idx_to_visualize,\n",
    "                    cameras=CAMERAS,\n",
    "                    depth_scale=DEPTH_SCALE,)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "for ts in episode_length:\n",
    "\n",
    "    # extract obs at timestep\n",
    "    obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "    gripper_pose = demo[ts].gripper_pose\n",
    "    gripper_open = demo[ts].gripper_open\n",
    "    gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "    # obs_dict[\"gripper_pose\"] = gripper_pose\n",
    "    obs_dict[\"gripper_open\"] = gripper_open\n",
    "    obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "    # plot rgb and depth at timestep\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    rows, cols = 2, len(CAMERAS)\n",
    "\n",
    "    plot_idx = 1\n",
    "    for camera in CAMERAS:\n",
    "        # rgb\n",
    "        rgb_name = \"%s_%s\" % (camera, 'rgb')\n",
    "        rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
    "        fig.add_subplot(rows, cols, plot_idx)\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"%s_rgb | step %s\" % (camera, ts))\n",
    "\n",
    "        # depth\n",
    "        depth_name = \"%s_%s\" % (camera, 'depth')\n",
    "        depth = np.transpose(obs_dict[depth_name], (1, 2, 0))\n",
    "        fig.add_subplot(rows, cols, plot_idx+len(CAMERAS))\n",
    "        plt.imshow(depth)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"%s_depth | step %s\" % (camera, ts))\n",
    "\n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(obs_dict)\n",
    "\n",
    "    (continuous_trans, continuous_quat, gripper_open), (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent.forward(obs_dict, ts)\n",
    "\n",
    "    from arm.utils import visualise_voxel\n",
    "    from arm.utils import discrete_euler_to_quaternion, get_gripper_render_pose\n",
    "\n",
    "    # things to visualize\n",
    "    vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "    pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "\n",
    "    # discrete to continuous\n",
    "    continuous_trans = continuous_trans[0].detach().cpu().numpy()\n",
    "    continuous_quat = discrete_euler_to_quaternion(rot_and_grip_indices[0][:3].detach().cpu().numpy(),\n",
    "                                                resolution=peract_agent._rotation_resolution)\n",
    "    gripper_open = bool(rot_and_grip_indices[0][-1].detach().cpu().numpy())\n",
    "    ignore_collision = bool(test_update_dict['pred_action']['collision'][0][0].detach().cpu().numpy())\n",
    "\n",
    "    # # gripper visualization pose\n",
    "    voxel_size = 0.045\n",
    "    voxel_scale = voxel_size * 100\n",
    "    gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                            SCENE_BOUNDS[:3],\n",
    "                                            continuous_trans,\n",
    "                                            continuous_quat)\n",
    "\n",
    "    # #@markdown #### Show Q-Prediction and Best Action\n",
    "    show_expert_action = True  #@param {type:\"boolean\"}\n",
    "    show_q_values = False  #@param {type:\"boolean\"}\n",
    "    render_gripper = False  #@param {type:\"boolean\"}\n",
    "    rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "    rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(0),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale,\n",
    "                                perspective=False)\n",
    "\n",
    "    rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(45),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(rendered_img_0)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Front view\")\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(rendered_img_270)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Side view\")\n",
    "\n",
    "\n",
    "    # #@markdown #### Show Q-Prediction and Best Action\n",
    "    show_expert_action = True  #@param {type:\"boolean\"}\n",
    "    show_q_values = True  #@param {type:\"boolean\"}\n",
    "    render_gripper = True  #@param {type:\"boolean\"}\n",
    "    rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "    rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(0),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "    rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(45),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(rendered_img_0)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Front view\")\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(rendered_img_270)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Side view\")\n",
    "\n",
    "    print(f\"Lang goal: {lang_goal}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Run Inference with same batches as during training (You can change `batch_size` in <i>constants.py</i>) - Check confidence over different iteration timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from notebook_helpers.constants import * # Load global constant variables from constants.py\n",
    "from notebook_helpers.build_training import build_agent\n",
    "from notebook_helpers.build_replay import load_replay_buffer\n",
    "\n",
    "# Choose the run directory containing models and settings\n",
    "# run_dir = \"/home/bepgroup/Projects/PerAct_ws/outputs/models/handing_over_banana/2024-12-17_16-44\" # {crop no RGB}\n",
    "# run_dir = \"/home/bepgroup/Projects/PerAct_ws/outputs/models/handing_over_banana/2024-12-18_01-48\" # {crop full RGB}\n",
    "# run_dir = \"/home/bepgroup/Projects/PerAct_ws/outputs/models/handing_over_banana/2024-12-17_21-14\" # {crop partial RGB}\n",
    "run_dir = \"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/open_drawer/2025-01-07_12-12\" # {standard no RGB}\n",
    "\n",
    "# Obtain settings\n",
    "path_settings = os.path.join(run_dir, \"training_settings.json\")\n",
    "with open(path_settings, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "train_data_iter, test_data_iter = load_replay_buffer(settings)\n",
    "\n",
    "peract_agent = build_agent(settings, training=True) # Set training to True for running with replaybuffer\n",
    "peract_agent.set_language_goal(\"handing over banana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_helpers.build_training import NumpyEncoder\n",
    "\n",
    "\n",
    "model_runs = natsorted([run for run in os.listdir(run_dir) if \"run\" in run])\n",
    "\n",
    "# Choose the loss metric at which model is saved\n",
    "# chosen_model = \"best_model_train\"\n",
    "# chosen_model = \"best_model_test\"\n",
    "# chosen_model = \"best_model_general\"\n",
    "chosen_model = \"last_model\"\n",
    "\n",
    "# Save results\n",
    "model_run_scored_sampled = dict()\n",
    "model_run_distances_sampled = dict()\n",
    "# model_run_rotations = dict() # Ignore rotation, location matters most\n",
    "\n",
    "# Loop over iterations\n",
    "for model_run_iter in model_runs:\n",
    "    \n",
    "    # Load model if available\n",
    "    try:\n",
    "        peract_agent.load_weights(os.path.join(run_dir, model_run_iter, chosen_model))\n",
    "    except:\n",
    "        print(f\"Model {model_run_iter} not found, skipping.\")\n",
    "        continue\n",
    "\n",
    "    distances_run = []\n",
    "    scores_run = []\n",
    "    \n",
    "    for i in range(20): # collect using ... samples\n",
    "\n",
    "        batch = next(train_data_iter) # collect batch\n",
    "\n",
    "        lang_goal = batch['lang_goal']\n",
    "        print(f\"batch: {i} - analyzing: {lang_goal}\")\n",
    "\n",
    "        # Set batch tensor on GPU and predict\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "        update_dict = peract_agent.update(None, batch, backprop=False)\n",
    "        \n",
    "        # Results\n",
    "        prediction_scores = torch.amax(update_dict[\"q_trans\"], dim=(1,2,3,4)).detach().cpu().numpy()\n",
    "        prediction_scores = np.around(prediction_scores, 4)\n",
    "\n",
    "        pred_trans = update_dict[\"pred_action\"][\"trans\"].detach().cpu().numpy()\n",
    "        gt_trans = update_dict[\"expert_action\"][\"action_trans\"].detach().cpu().numpy()\n",
    "        dist = np.round(np.linalg.norm(pred_trans-gt_trans, axis=1), 4)\n",
    "        \n",
    "        # Save Results\n",
    "        distances_run.extend(dist.tolist())\n",
    "        scores_run.extend(prediction_scores.tolist())\n",
    "    \n",
    "    zipped_lists = zip(distances_run, scores_run)\n",
    "    sorted_lists = sorted(zipped_lists, key=lambda x: x[0]) # Order ascending distances\n",
    "    sorted_distances, sorted_scores = zip(*sorted_lists) # Unzip\n",
    "\n",
    "    # Save results to iteration\n",
    "    model_run_distances_sampled[model_run_iter] = sorted_distances\n",
    "    model_run_scored_sampled[model_run_iter] = sorted_scores\n",
    "\n",
    "with open(os.path.join(run_dir, f\"results_distances_{chosen_model}.json\"), 'w') as f:\n",
    "    json.dump(model_run_distances_sampled, f, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "with open(os.path.join(run_dir, f\"results_scores_{chosen_model}.json\"), 'w') as f:\n",
    "    json.dump(model_run_scored_sampled, f, indent=4, cls=NumpyEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "n_samples = 4\n",
    "\n",
    "run_iteration_keys = list(model_run_distances_sampled.keys())\n",
    "\n",
    "run_iterations = np.linspace(0, len(run_iteration_keys)-1, n_samples, dtype=int)\n",
    "sampled_keys = [run_iteration_keys[run_iteration] for run_iteration in run_iterations]\n",
    "\n",
    "model_run_distances_sampled = {key: model_run_distances_sampled[key] for key in sampled_keys}\n",
    "model_run_scored_sampled = {key: model_run_scored_sampled[key] for key in sampled_keys}\n",
    "\n",
    "df_dist = pd.concat([\n",
    "    pd.DataFrame({'Iteration': key, 'Error': values}) for i, (key, values) in enumerate(model_run_distances_sampled.items())],\n",
    "    ignore_index=True\n",
    ")\n",
    "# df_dist = df_dist.iloc[::3]\n",
    "df_conf = pd.concat([\n",
    "    pd.DataFrame({'Iteration': key, 'Confidence': values}) for i, (key, values) in enumerate(model_run_scored_sampled.items())],\n",
    "    ignore_index=True\n",
    ")\n",
    "# df_conf = df_conf.iloc[::3]\n",
    "\n",
    "sns.histplot(data=df_dist, x='Error', hue='Iteration', fill=True, alpha=0.3)\n",
    "plt.title('Distribution of Error Across Iterations')\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(data=df_conf, x='Confidence', hue='Iteration', fill=True, alpha=0.3)\n",
    "plt.title('Distribution of Confidences Across Iterations')\n",
    "plt.show()\n",
    "\n",
    "df_merged = pd.merge(df_dist, df_conf, on='Iteration', how=\"inner\")\n",
    "sns.scatterplot(data=df_merged, x='Error', y='Confidence', hue='Iteration', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
