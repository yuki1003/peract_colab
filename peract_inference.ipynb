{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Trained PerAct Agent on handoversim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_ # bad trick to fix numpy version issue :(\n",
    "import os\n",
    "import sys\n",
    "sys.path = [p for p in sys.path if '/peract/' not in p]\n",
    "\n",
    "# Set `PYOPENGL_PLATFORM=egl` for pyrender visualizations\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\" # Depends on your computer and available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from notebook_helpers.constants import * # Load global constant variables from constants.py\n",
    "from notebook_helpers.build_training import build_agent\n",
    "\n",
    "# Choose the run\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-10_14-59\" # Good {non-uniform 1 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad {uniform 2 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_13-34\"\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_17-25\" # {crop skip 10}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_21-23\"\n",
    "run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/\" # {crop skip 10 new}\n",
    "\n",
    "# Obtain settings\n",
    "path_settings = os.path.join(os.path.dirname(run_dir), \"training_settings.json\")\n",
    "with open(path_settings, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "CAMERAS = settings['cameras']\n",
    "\n",
    "if BATCH_SIZE != 1:\n",
    "    raise ValueError(\"For Inference, 'batch_size' must be set to 1 in constants.py.\")\n",
    "\n",
    "peract_agent = build_agent(settings, training=False)\n",
    "peract_agent.set_language_goal(\"handing over banana\")\n",
    "\n",
    "# Choose model\n",
    "iteration = \"run8000\"\n",
    "best_type = \"best_model_general\"\n",
    "model_path = os.path.join(run_dir, iteration, best_type)\n",
    "\n",
    "peract_agent.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference: Run Inference on just observation data and Save as video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs\n",
    "from arm.utils import visualise_voxel\n",
    "from arm.utils import get_gripper_render_pose\n",
    "\n",
    "\n",
    "# What to visualize\n",
    "episode_idx_to_visualize = 646  # Index of the episode to visualize\n",
    "# Video output path\n",
    "video_output_path = f\"demo{episode_idx_to_visualize}_visualization.mp4\"\n",
    "\n",
    "# Get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                       index=episode_idx_to_visualize,\n",
    "                       cameras=CAMERAS,\n",
    "                       depth_scale=DEPTH_SCALE)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "\n",
    "# Open a video writer\n",
    "with imageio.get_writer(video_output_path, fps=10) as video_writer:\n",
    "    for ts in episode_length[::5]:\n",
    "        print(ts)\n",
    "        # Extract obs at timestep\n",
    "        obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "        # gripper_pose = demo[ts].gripper_pose\n",
    "        gripper_open = demo[ts].gripper_open\n",
    "        gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "        obs_dict[\"gripper_open\"] = gripper_open\n",
    "        obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "        (continuous_trans, continuous_quat, gripper_open, _, _), \\\n",
    "        (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent.forward(obs_dict, ts)\n",
    "        print(continuous_trans, continuous_quat, gripper_open)\n",
    "\n",
    "        # Things to visualize\n",
    "        vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "        pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "\n",
    "        voxel_size = 0.045\n",
    "        voxel_scale = voxel_size * 100\n",
    "        gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                                   SCENE_BOUNDS[:3],\n",
    "                                                   continuous_trans,\n",
    "                                                   continuous_quat)\n",
    "\n",
    "        rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                         None,\n",
    "                                         [pred_trans_coord],\n",
    "                                         None,\n",
    "                                         voxel_size=voxel_size,\n",
    "                                         rotation_amount=np.deg2rad(0),\n",
    "                                         render_gripper=True,\n",
    "                                         gripper_pose=gripper_pose_mat,\n",
    "                                         gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                           None,\n",
    "                                           [pred_trans_coord],\n",
    "                                           None,\n",
    "                                           voxel_size=voxel_size,\n",
    "                                           rotation_amount=np.deg2rad(45+180),\n",
    "                                           render_gripper=True,\n",
    "                                           gripper_pose=gripper_pose_mat,\n",
    "                                           gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        # Plot figures into a NumPy array\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        plt.imshow(rendered_img_0)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Front view\")\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        plt.imshow(rendered_img_270)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Side view\")\n",
    "\n",
    "        # Add timestamp as text with white font and black background\n",
    "        fig.text(0.02, 0.95, f\"Timestep: {ts}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                 bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "\n",
    "        # Convert the matplotlib figure to a NumPy array\n",
    "        fig.canvas.draw()\n",
    "        img_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        \n",
    "        video_writer.append_data(img_array)  # Add frame to video\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "print(f\"Video saved to {video_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Run Inference on just input data with ground truth to check what happends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_from_frame(episode_keypoints_gt_obs_dict, frame_idx):\n",
    "    # Get a sorted list of dictionary keys (frames with stored data)\n",
    "    episode_keypoints = sorted(episode_keypoints_gt_obs_dict.keys())\n",
    "\n",
    "    # Iterate through the given frame indices\n",
    "    # Find the smallest key that is greater than or equal to the current frame\n",
    "    for episode_kp in episode_keypoints:\n",
    "        if frame_idx <= episode_kp:\n",
    "            episode_kp_gt_obs_dict = episode_keypoints_gt_obs_dict[episode_kp]\n",
    "            return episode_kp_gt_obs_dict\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: NEED SOME WAY TO COLLECT THE GROUND TRUTH DATA FOR ALL FRAMES\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs_gt\n",
    "\n",
    "from arm.demo import _keypoint_discovery_available\n",
    "from arm.utils import point_to_voxel_index, visualise_voxel, get_gripper_render_pose\n",
    "\n",
    "\n",
    "# What to visualize\n",
    "episode_idx_to_visualize = 645  # Index of the episode to visualize\n",
    "# Video output path\n",
    "video_output_path = f\"demo{episode_idx_to_visualize}_gt_pred_visualization.mp4\"\n",
    "\n",
    "# Get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                       index=episode_idx_to_visualize,\n",
    "                       cameras=CAMERAS,\n",
    "                       depth_scale=DEPTH_SCALE)\n",
    "\n",
    "episode_keypoints_gt_obs_dict = dict()\n",
    "episode_keypoints = _keypoint_discovery_available(demo, approach_distance=0.3) #NOTE: Approach_distance Set\n",
    "episode_keypoints = [episode_keypoints[-1]]\n",
    "for episode_keypoint in episode_keypoints:\n",
    "    episode_keypoints_gt_obs_dict[episode_keypoint] = extract_obs_gt(obs = demo._observations[episode_keypoint],\n",
    "                                                                  cameras=CAMERAS)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "# episode_length = [40]\n",
    "\n",
    "# Open a video writer\n",
    "with imageio.get_writer(video_output_path, fps=10) as video_writer:\n",
    "    for ts in episode_length[::5]: # Skip some frames\n",
    "        print(ts)\n",
    "        # Extract obs at timestep\n",
    "        obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "        # gripper_pose = demo[ts].gripper_pose\n",
    "        gripper_open = demo[ts].gripper_open\n",
    "        gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "        obs_dict[\"gripper_open\"] = gripper_open\n",
    "        obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "        (continuous_trans, continuous_quat, gripper_open, trans_confidence, _), \\\n",
    "        (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent.forward(obs_dict, ts)\n",
    "\n",
    "        pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "        \n",
    "        # Get the ground truth\n",
    "        episode_keypoint_gt_obs_dict = get_gt_from_frame(episode_keypoints_gt_obs_dict, ts)\n",
    "        if not (episode_keypoint_gt_obs_dict is None):\n",
    "            gt_gripper_pose = episode_keypoint_gt_obs_dict[\"gripper_pose\"]\n",
    "            gt_trans_coord = point_to_voxel_index(gt_gripper_pose[:3], VOXEL_SIZES, SCENE_BOUNDS)[0]\n",
    "            error = np.linalg.norm(gt_gripper_pose[:3] - continuous_trans)\n",
    "            print(f\"GT (voxel): {gt_trans_coord} - Prediction (voxel): {pred_trans_coord} - Error: {error} - Prediction-score: {round(trans_confidence,4)}\")\n",
    "        else:\n",
    "            gt_trans_coord = None\n",
    "            error = False\n",
    "            print(\"GT coordinates not available for this frame\")\n",
    "        \n",
    "        # Things to visualize\n",
    "        vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "\n",
    "        voxel_size = 0.045\n",
    "        voxel_scale = voxel_size * 100\n",
    "        gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                                   SCENE_BOUNDS[:3],\n",
    "                                                   continuous_trans,\n",
    "                                                   continuous_quat)\n",
    "\n",
    "        rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                         None,\n",
    "                                         [pred_trans_coord],\n",
    "                                         gt_trans_coord,\n",
    "                                         alpha = 0.2,\n",
    "                                         voxel_size=voxel_size,\n",
    "                                         rotation_amount=np.deg2rad(0),\n",
    "                                         render_gripper=True,\n",
    "                                         gripper_pose=gripper_pose_mat,\n",
    "                                         gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                           None,\n",
    "                                           [pred_trans_coord],\n",
    "                                           gt_trans_coord,\n",
    "                                           alpha = 0.2,\n",
    "                                           voxel_size=voxel_size,\n",
    "                                           rotation_amount=np.deg2rad(45+180),\n",
    "                                           render_gripper=True,\n",
    "                                           gripper_pose=gripper_pose_mat,\n",
    "                                           gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        # Plot figures into a NumPy array\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        plt.imshow(rendered_img_0)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Front view\")\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        plt.imshow(rendered_img_270)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Side view\")\n",
    "\n",
    "        # Add timestamp as text with white font and black background\n",
    "        if error:\n",
    "            fig.text(0.02, 0.95, f\"Timestep: {ts}, Prediction-score: {round(trans_confidence,4)}, Error: {np.round(error, 3)}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                    bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "        else:\n",
    "            fig.text(0.02, 0.95, f\"Timestep: {ts}, Prediction-score: {round(trans_confidence,4)}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                    bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "\n",
    "        # Convert the matplotlib figure to a NumPy array\n",
    "        fig.canvas.draw()\n",
    "        img_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        \n",
    "        video_writer.append_data(img_array)  # Add frame to video\n",
    "        # plt.show()\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "        \n",
    "\n",
    "print(f\"Video saved to {video_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Run Inference with same batches as during training (You can change `batch_size` in <i>constants.py</i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from notebook_helpers.build_replay import load_replay_buffer\n",
    "\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/\" # {crop skip 10 new}\n",
    "\n",
    "# path_settings = os.path.join(os.path.dirname(run_dir), \"training_settings.json\")\n",
    "# with open(path_settings, 'r') as f:\n",
    "#     settings = json.load(f)\n",
    "\n",
    "# train_data_iter, test_data_iter = load_replay_buffer(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from notebook_helpers.constants import * # Load global constant variables from constants.py\n",
    "from notebook_helpers.build_training import build_agent\n",
    "from notebook_helpers.build_replay import load_replay_buffer\n",
    "\n",
    "# Choose the run\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-10_14-59\" # Good {non-uniform 1 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad {uniform 2 kp}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_12-30\" # Bad\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-11_13-34\"\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_17-25\" # {crop skip 10}\n",
    "# run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-12_21-23\"\n",
    "run_dir = \"/home/ywatabe/Projects/PerAct/outputs/models/handing_over_banana/2024-12-16_11-25/\" # {crop skip 10 new}\n",
    "\n",
    "# Obtain settings\n",
    "path_settings = os.path.join(os.path.dirname(run_dir), \"training_settings.json\")\n",
    "with open(path_settings, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "train_data_iter, test_data_iter = load_replay_buffer(settings)\n",
    "\n",
    "peract_agent = build_agent(settings, training=True)\n",
    "peract_agent.set_language_goal(\"handing over banana\")\n",
    "\n",
    "# Choose model\n",
    "iteration = \"run8000\"\n",
    "best_type = \"best_model_general\"\n",
    "model_path = os.path.join(run_dir, iteration, best_type)\n",
    "\n",
    "peract_agent.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rlbench.utils import get_stored_demo\n",
    "from rlbench.backend.utils import extract_obs\n",
    "\n",
    "CAMERAS = settings[\"cameras\"]\n",
    "\n",
    "batch = next(train_data_iter)\n",
    "\n",
    "# what to visualize\n",
    "episode_idx_to_visualize = 646#INDEXES[0] # out of 10 demos\n",
    "# ts = 70#25 # timestep out of total timesteps\n",
    "\n",
    "# get demo\n",
    "demo = get_stored_demo(data_path=test_data_path,\n",
    "                    index=episode_idx_to_visualize,\n",
    "                    cameras=CAMERAS,\n",
    "                    depth_scale=DEPTH_SCALE,)\n",
    "\n",
    "episode_length = list(range(len(demo._observations)))\n",
    "for ts in episode_length:\n",
    "\n",
    "    # extract obs at timestep\n",
    "    obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
    "    gripper_pose = demo[ts].gripper_pose\n",
    "    gripper_open = demo[ts].gripper_open\n",
    "    gripper_joint_positions = demo[ts].gripper_joint_positions\n",
    "\n",
    "    # obs_dict[\"gripper_pose\"] = gripper_pose\n",
    "    obs_dict[\"gripper_open\"] = gripper_open\n",
    "    obs_dict[\"gripper_joint_positions\"] = gripper_joint_positions\n",
    "\n",
    "    # plot rgb and depth at timestep\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    rows, cols = 2, len(CAMERAS)\n",
    "\n",
    "    plot_idx = 1\n",
    "    for camera in CAMERAS:\n",
    "        # rgb\n",
    "        rgb_name = \"%s_%s\" % (camera, 'rgb')\n",
    "        rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
    "        fig.add_subplot(rows, cols, plot_idx)\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"%s_rgb | step %s\" % (camera, ts))\n",
    "\n",
    "        # depth\n",
    "        depth_name = \"%s_%s\" % (camera, 'depth')\n",
    "        depth = np.transpose(obs_dict[depth_name], (1, 2, 0))\n",
    "        fig.add_subplot(rows, cols, plot_idx+len(CAMERAS))\n",
    "        plt.imshow(depth)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"%s_depth | step %s\" % (camera, ts))\n",
    "\n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(obs_dict)\n",
    "\n",
    "    (continuous_trans, continuous_quat, gripper_open), (voxel_grid, coord_indices, rot_and_grip_indices, gripper_open) = peract_agent.forward(obs_dict, ts)\n",
    "\n",
    "    from arm.utils import visualise_voxel\n",
    "    from arm.utils import discrete_euler_to_quaternion, get_gripper_render_pose\n",
    "\n",
    "    # things to visualize\n",
    "    vis_voxel_grid = voxel_grid[0].detach().cpu().numpy()\n",
    "    pred_trans_coord = coord_indices[0].detach().cpu().numpy().tolist()\n",
    "\n",
    "    # discrete to continuous\n",
    "    continuous_trans = continuous_trans[0].detach().cpu().numpy()\n",
    "    continuous_quat = discrete_euler_to_quaternion(rot_and_grip_indices[0][:3].detach().cpu().numpy(),\n",
    "                                                resolution=peract_agent._rotation_resolution)\n",
    "    gripper_open = bool(rot_and_grip_indices[0][-1].detach().cpu().numpy())\n",
    "    ignore_collision = bool(test_update_dict['pred_action']['collision'][0][0].detach().cpu().numpy())\n",
    "\n",
    "    # # gripper visualization pose\n",
    "    voxel_size = 0.045\n",
    "    voxel_scale = voxel_size * 100\n",
    "    gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                            SCENE_BOUNDS[:3],\n",
    "                                            continuous_trans,\n",
    "                                            continuous_quat)\n",
    "\n",
    "    # #@markdown #### Show Q-Prediction and Best Action\n",
    "    show_expert_action = True  #@param {type:\"boolean\"}\n",
    "    show_q_values = False  #@param {type:\"boolean\"}\n",
    "    render_gripper = False  #@param {type:\"boolean\"}\n",
    "    rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "    rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(0),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale,\n",
    "                                perspective=False)\n",
    "\n",
    "    rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(45),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(rendered_img_0)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Front view\")\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(rendered_img_270)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Side view\")\n",
    "\n",
    "\n",
    "    # #@markdown #### Show Q-Prediction and Best Action\n",
    "    show_expert_action = True  #@param {type:\"boolean\"}\n",
    "    show_q_values = True  #@param {type:\"boolean\"}\n",
    "    render_gripper = True  #@param {type:\"boolean\"}\n",
    "    rotation_amount = -90 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
    "\n",
    "    rendered_img_0 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(0),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "    rendered_img_270 = visualise_voxel(vis_voxel_grid,\n",
    "                                None,\n",
    "                                [pred_trans_coord],\n",
    "                                None,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(45),\n",
    "                                render_gripper=render_gripper,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(rendered_img_0)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Front view\")\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(rendered_img_270)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Side view\")\n",
    "\n",
    "    print(f\"Lang goal: {lang_goal}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Run Inference with same batches as during training (You can change `batch_size` in <i>constants.py</i>) - Check confidence over different iteration timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from notebook_helpers.constants import BATCH_SIZE # Load global constant variables from constants.py\n",
    "from notebook_helpers.build_training import build_agent\n",
    "from notebook_helpers.build_replay import load_replay_buffer\n",
    "\n",
    "\n",
    "if not BATCH_SIZE in [4, 6]:\n",
    "    raise ValueError('Set BATCH_SIZE = 6 in notebook_helpers.constants.py!')\n",
    "\n",
    "# TASK = \"handing_over_banana\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_16-17\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_18-40\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_21-05\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_23-30\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_01-56\"\n",
    "\n",
    "TASK = \"handing_over_mug\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_17-28\" #camera_setting = {6, 8, 10}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_19-52\" #camera_setting = {6, 7, 8, 9, 10}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_22-18\" #camera_setting = {0, 1, 2, 3, 4, 5}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_00-43\" #camera_setting = {1, 3, 5, 6, 8, 10}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_03-09\" #camera_setting = {4, 5, 6, 7, 8, 9}\n",
    "run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-22_17-07\" #task=\"handing_over_mug_and_grasp_handle\"\n",
    "run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-22_23-18\" #task=\"handing_over_mug_and_grasp_rim\"\n",
    "run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-23_06-46\" #task={\"handing_over_mug_and_grasp_handle\", \"handing_over_mug_and_grasp_rim\"}\n",
    "\n",
    "# TASK = \"handing_over_bowl\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_12-22\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17-13-35\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17-14-49\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_16-11\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_17-30\"\n",
    "\n",
    "# TASK = \"handing_over_pitcher_base\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_11-02\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_12-06\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_13-10\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_14-15\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_15-19\"\n",
    "\n",
    "# Obtain settings\n",
    "path_settings = os.path.join(run_dir, \"training_settings.json\")\n",
    "with open(path_settings, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "EPISODE_FOLDER = 'episode%d'\n",
    "SETUP = 's1'\n",
    "\n",
    "WORKSPACE_DIR = os.getcwd()\n",
    "DATA_FOLDER  = os.path.join(WORKSPACE_DIR, 'task_data', 'handoversim_v4')\n",
    "DATA_FOLDER = DATA_FOLDER.replace('/peract_colab', '')\n",
    "TASK = 'handing_over_mug'\n",
    "EPISODES_FOLDER = os.path.join(TASK, \"all_variations\", \"episodes\")\n",
    "\n",
    "train_data_path = os.path.join(DATA_FOLDER, f\"train_{SETUP}\", EPISODES_FOLDER)\n",
    "test_data_path = os.path.join(DATA_FOLDER, f\"val_{SETUP}\", EPISODES_FOLDER)\n",
    "TRAIN_INDEXES = [int(episode_nr.replace(\"episode\", \"\")) for episode_nr in natsorted(os.listdir(train_data_path))]\n",
    "TEST_INDEXES = [int(episode_nr.replace(\"episode\", \"\")) for episode_nr in natsorted(os.listdir(test_data_path))]\n",
    "#TASK = MUG. Separate by handle vs. rim\n",
    "# TRAIN_INDEXES = [66, 266, 268, 269, 368, 465, 466]\n",
    "# TRAIN_INDEXES = [167, 169, 265, 365, 366, 367, 369, 468, 566, 567, 568, 569, 966, 967, 968]\n",
    "test_data_path = os.path.join(DATA_FOLDER, f\"train_{SETUP}\", EPISODES_FOLDER)\n",
    "TRAIN_INDEXES = [66, 266, 268, 269, 368] # handle\n",
    "TEST_INDEXES = [465, 466] # handle\n",
    "\n",
    "TRAIN_INDEXES = TRAIN_INDEXES + [167, 169, 265, 365, 366, 367, 369, 468, 566, 567, 568, 569] # rim\n",
    "TEST_INDEXES = TEST_INDEXES + [966, 967, 968] # rim\n",
    "\n",
    "train_data_iter, test_data_iter = load_replay_buffer(settings,\n",
    "                                                     WORKSPACE_DIR, SETUP, EPISODE_FOLDER,\n",
    "                                                     TASK,\n",
    "                                                     train_data_path, test_data_path, TRAIN_INDEXES, TEST_INDEXES)\n",
    "\n",
    "peract_agent = build_agent(settings, training=True) # Set training to True for running with replaybuffer\n",
    "peract_agent.set_language_goal(TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_helpers.build_training import NumpyEncoder\n",
    "from notebook_helpers.constants import *\n",
    "\n",
    "\n",
    "model_runs = natsorted([run for run in os.listdir(run_dir) if \"run\" in run])\n",
    "\n",
    "# Choose the loss metric at which model is saved\n",
    "# chosen_model = \"best_model_train\"\n",
    "# chosen_model = \"best_model_test\"\n",
    "# chosen_model = \"best_model_general\"\n",
    "chosen_model = \"last_model\"\n",
    "\n",
    "# Choose validation set\n",
    "testing_set = \"train\"\n",
    "testing_set = \"train_handle\"\n",
    "testing_set = \"val_handle_or_rim\"\n",
    "\n",
    "# Save results\n",
    "model_run_scored = dict()\n",
    "model_run_distances = dict()\n",
    "# model_run_rotations = dict() # Ignore rotation, location matters most\n",
    "\n",
    "# Loop over iterations\n",
    "for model_run_iter in model_runs:\n",
    "    \n",
    "    # Load model if available\n",
    "    try:\n",
    "        peract_agent.load_weights(os.path.join(run_dir, model_run_iter, chosen_model))\n",
    "    except:\n",
    "        print(f\"Model {model_run_iter} not found, skipping.\")\n",
    "        continue\n",
    "\n",
    "    distances_run = []\n",
    "    scores_run = []\n",
    "    \n",
    "    for i in range(30): # collect using ... samples\n",
    "        \n",
    "        if \"train\" in testing_set:\n",
    "            batch = next(train_data_iter) # collect batch\n",
    "        if \"val\" in testing_set:\n",
    "            batch = next(test_data_iter) # collect batch\n",
    "\n",
    "        lang_goal = batch['lang_goal']\n",
    "        print(f\"batch: {i} - analyzing: {lang_goal}\")\n",
    "\n",
    "        # Set batch tensor on GPU and predict\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "        update_dict = peract_agent.update(None, batch, backprop=False)\n",
    "        \n",
    "        # Results\n",
    "        prediction_scores = torch.amax(update_dict[\"q_trans\"], dim=(1,2,3,4)).detach().cpu().numpy()\n",
    "        prediction_scores = np.around(prediction_scores, 4)\n",
    "\n",
    "        pred_trans = update_dict[\"pred_action\"][\"trans\"]\n",
    "        gt_trans = update_dict[\"expert_action\"][\"action_trans\"]\n",
    "        dist = np.round(np.linalg.norm(pred_trans-gt_trans, axis=1), 4)\n",
    "        \n",
    "        # Save Results\n",
    "        distances_run.extend(dist.tolist())\n",
    "        scores_run.extend(prediction_scores.tolist())\n",
    "    \n",
    "    zipped_lists = zip(distances_run, scores_run)\n",
    "    sorted_lists = sorted(zipped_lists, key=lambda x: x[0]) # Order ascending distances\n",
    "    sorted_distances, sorted_scores = zip(*sorted_lists) # Unzip\n",
    "\n",
    "    # Save results to iteration\n",
    "    model_run_distances[model_run_iter] = sorted_distances\n",
    "    model_run_scored[model_run_iter] = sorted_scores\n",
    "\n",
    "with open(os.path.join(run_dir, f\"results_distances_{chosen_model}_on_{testing_set}.json\"), 'w') as f:\n",
    "    json.dump(model_run_distances, f, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "with open(os.path.join(run_dir, f\"results_scores_{chosen_model}_on_{testing_set}.json\"), 'w') as f:\n",
    "    json.dump(model_run_scored, f, indent=4, cls=NumpyEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "n_samples = 3\n",
    "\n",
    "# Choose the loss metric at which model is saved\n",
    "# chosen_model = \"best_model_train\"\n",
    "# chosen_model = \"best_model_test\"\n",
    "# chosen_model = \"best_model_general\"\n",
    "chosen_model = \"last_model\"\n",
    "\n",
    "# Choose validation set\n",
    "# testing_set = \"train_handle\"\n",
    "# testing_set = \"train_rim\"\n",
    "testing_set = \"val_handle\"\n",
    "\n",
    "# TASK = \"handing_over_banana\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_16-17\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_18-40\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_21-05\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_23-30\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_01-56\"\n",
    "\n",
    "TASK = \"handing_over_mug\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_17-28\" #camera_setting = {6, 8, 10}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_19-52\" #camera_setting = {6, 7, 8, 9, 10}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_22-18\" #camera_setting = {0, 1, 2, 3, 4, 5}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_00-43\" #camera_setting = {1, 3, 5, 6, 8, 10}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_03-09\" #camera_setting = {4, 5, 6, 7, 8, 9}\n",
    "run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-22_17-07\" #task=\"handing_over_mug_and_grasp_handle\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-22_23-18\" #task=\"handing_over_mug_and_grasp_rim\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-23_06-46\" #task={\"handing_over_mug_and_grasp_handle\", \"handing_over_mug_and_grasp_rim\"}\n",
    "\n",
    "# TASK = \"handing_over_bowl\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_12-22\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17-13-35\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17-14-49\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_16-11\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_17-30\"\n",
    "\n",
    "# TASK = \"handing_over_pitcher_base\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_11-02\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_12-06\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_13-10\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_14-15\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_15-19\"\n",
    "\n",
    "with open(os.path.join(run_dir, f\"results_distances_{chosen_model}_on_{testing_set}.json\")) as f:\n",
    "    model_run_distances = json.load(f)\n",
    "\n",
    "with open(os.path.join(run_dir, f\"results_scores_{chosen_model}_on_{testing_set}.json\")) as f:\n",
    "    model_run_scored = json.load(f)\n",
    "\n",
    "run_iteration_keys = list(model_run_distances.keys())\n",
    "\n",
    "run_iterations = np.linspace(0, len(run_iteration_keys)-1, n_samples, dtype=int)\n",
    "sampled_keys = [[run_iteration_keys[run_iteration] for run_iteration in run_iterations][-1]]\n",
    "# sampled_keys = [sampled_keys[-1]] if n_samples == 1 else sampled_keys\n",
    "\n",
    "model_run_distances_sampled = {key: model_run_distances[key] for key in sampled_keys}\n",
    "model_run_scored_sampled = {key: model_run_scored[key] for key in sampled_keys}\n",
    "\n",
    "df_dist = pd.concat([\n",
    "    pd.DataFrame({'Iteration': key, 'Error': values}) for i, (key, values) in enumerate(model_run_distances_sampled.items())],\n",
    "    ignore_index=True\n",
    ")\n",
    "# df_dist = df_dist.iloc[::3]\n",
    "df_conf = pd.concat([\n",
    "    pd.DataFrame({'Iteration': key, 'Confidence': values}) for i, (key, values) in enumerate(model_run_scored_sampled.items())],\n",
    "    ignore_index=True\n",
    ")\n",
    "# df_conf = df_conf.iloc[::3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(data=df_dist, x='Error', hue='Iteration', fill=True, alpha=0.3)\n",
    "# sns.displot(data=df_dist, x='Error', kde=True)\n",
    "# sns.boxenplot(data=df_dist, x=\"Error\", y=\"Iteration\", ax = ax)\n",
    "plt.title('Distribution of Error Across Iterations')\n",
    "plt.xlabel('Translation error [voxels]')\n",
    "# ax.set_xlim(1, 20)\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(data=df_conf, x='Confidence', hue='Iteration', fill=True, alpha=0.3)\n",
    "plt.title('Distribution of Confidences Across Iterations')\n",
    "plt.show()\n",
    "\n",
    "df_merged = pd.merge(df_dist, df_conf, on='Iteration', how=\"inner\")\n",
    "sns.scatterplot(data=df_merged, x='Error', y='Confidence', hue='Iteration', alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Run Inference with same batches as during training (You can change `batch_size` in <i>constants.py</i>) - Check the losses over time per episode, using `last_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from notebook_helpers.constants import BATCH_SIZE # Load global constant variables from constants.py\n",
    "from notebook_helpers.build_training import build_agent\n",
    "from notebook_helpers.build_replay import load_replay_buffer\n",
    "\n",
    "\n",
    "if BATCH_SIZE != 1:\n",
    "    raise ValueError('Set BATCH_SIZE = 1 in notebook_helpers.constants.py!')\n",
    "\n",
    "# TASK = \"handing_over_mug\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_17-28\" #camera_setting = {6, 8, 10}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_19-52\" #camera_setting = {6, 7, 8, 9, 10}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_22-18\" #camera_setting = {0, 1, 2, 3, 4, 5}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_00-43\" #camera_setting = {1, 3, 5, 6, 8, 10}\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-17_03-09\" #camera_setting = {4, 5, 6, 7, 8, 9}\n",
    "# # run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-22_17-07\" #task=\"handing_over_mug_and_grasp_handle\"\n",
    "# # run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-22_23-18\" #task=\"handing_over_mug_and_grasp_rim\"\n",
    "# # run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-23_06-46\" #task={\"handing_over_mug_and_grasp_handle\", \"handing_over_mug_and_grasp_rim\"}\n",
    "# # run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-23_15-11\" #task=\"handing_over_mug_and_grasp_handle\" val = {465, 466}\n",
    "\n",
    "# TASK = \"handing_over_pitcher_base\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_11-02\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_12-06\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_13-10\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_14-15\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_15-19\"\n",
    "\n",
    "# TASK = \"handing_over_banana\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_11-22\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-12_12-06\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_13-10\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_14-15\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_15-19\"\n",
    "\n",
    "# TASK = \"handing_over_banana\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-16_11-22\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-12_12-06\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_13-10\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_14-15\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-21_15-19\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-31_15-33\"\n",
    "# run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs/models/{TASK}/2025-01-31_17-54\"\n",
    "\n",
    "TASK = \"handing_over_banana\"\n",
    "run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs_rgb_augmentation/models/{TASK}/2025-02-04_08-05\"\n",
    "run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs_rgb_augmentation/models/{TASK}/2025-02-04_09-11\"\n",
    "run_dir = f\"/home/bepgroup/Projects/PerAct_ws/peract_colab/outputs_rgb_augmentation/models/{TASK}/2025-02-04_10-17\"\n",
    "\n",
    "# Obtain settings\n",
    "path_settings = os.path.join(run_dir, \"training_settings.json\")\n",
    "with open(path_settings, 'r') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "EPISODE_FOLDER = 'episode%d'\n",
    "SETUP = 's1'\n",
    "\n",
    "WORKSPACE_DIR = os.getcwd()\n",
    "DATA_FOLDER  = os.path.join(WORKSPACE_DIR, 'task_data', 'handoversim_v4')\n",
    "DATA_FOLDER = DATA_FOLDER.replace('/peract_colab', '')\n",
    "EPISODES_FOLDER = os.path.join(TASK, \"all_variations\", \"episodes\")\n",
    "\n",
    "train_data_path = os.path.join(DATA_FOLDER, f\"train_{SETUP}\", EPISODES_FOLDER)\n",
    "test_data_path = os.path.join(DATA_FOLDER, f\"val_{SETUP}\", EPISODES_FOLDER)\n",
    "TRAIN_INDEXES = [int(episode_nr.replace(\"episode\", \"\")) for episode_nr in natsorted(os.listdir(train_data_path))]\n",
    "TEST_INDEXES = [int(episode_nr.replace(\"episode\", \"\")) for episode_nr in natsorted(os.listdir(test_data_path))]\n",
    "#TASK = MUG. Separate by handle vs. rim\n",
    "# TRAIN_INDEXES = [66, 266, 268, 269, 368, 465, 466]\n",
    "# TRAIN_INDEXES = [167, 169, 265, 365, 366, 367, 369, 468, 566, 567, 568, 569, 966, 967, 968]\n",
    "# test_data_path = os.path.join(DATA_FOLDER, f\"train_{SETUP}\", EPISODES_FOLDER)\n",
    "# TRAIN_INDEXES = [66, 266, 268, 269, 368] # handle\n",
    "# TEST_INDEXES = [465, 466] # handle\n",
    "# TRAIN_INDEXES = [167, 169, 265, 365, 366, 367, 369, 468, 566, 567, 568, 569] # rim\n",
    "# TEST_INDEXES = [966, 967, 968] # rim\n",
    "\n",
    "_, test_data_iter = load_replay_buffer(settings,\n",
    "                                       WORKSPACE_DIR, SETUP, EPISODE_FOLDER,\n",
    "                                       TASK,\n",
    "                                       train_data_path, test_data_path, TRAIN_INDEXES, TEST_INDEXES)\n",
    "\n",
    "peract_agent = build_agent(settings, training=True) # Set training to True for running with replaybuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COLLECT ALL FRAMES USED FOR ANALYSIS\n",
    "\n",
    "import torch\n",
    "\n",
    "dict_test_episodes_frames = dict()\n",
    "\n",
    "## First find analysis\n",
    "for analyzed_episode in TEST_INDEXES:\n",
    "\n",
    "    lang_goals_set = set()\n",
    "\n",
    "    for i in range(1000):\n",
    "        # sample from dataset\n",
    "        batch = next(test_data_iter)\n",
    "        lang_goal = batch['lang_goal'][0][0][0]\n",
    "        task, episode, frame, kp = lang_goal.split('-')\n",
    "        if analyzed_episode == int(episode.replace('episode_', '')):\n",
    "            lang_goals_set.add(lang_goal)\n",
    "\n",
    "    replay_buffer_list = natsorted(lang_goals_set)\n",
    "\n",
    "    dict_test_episodes_frames[analyzed_episode] = replay_buffer_list\n",
    "\n",
    "print(dict_test_episodes_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import json\n",
    "\n",
    "from scipy.spatial.transform import Rotation as Rot\n",
    "\n",
    "from handover.ycb import YCB\n",
    "from notebook_helpers.constants import SCENE_BOUNDS, ROTATION_RESOLUTION\n",
    "\n",
    "\n",
    "def compose_qq(q1, q2):\n",
    "    qww = q1[..., 6] * q2[..., 6]\n",
    "    qxx = q1[..., 3] * q2[..., 3]\n",
    "    qyy = q1[..., 4] * q2[..., 4]\n",
    "    qzz = q1[..., 5] * q2[..., 5]\n",
    "\n",
    "    q1w2x = q1[..., 6] * q2[..., 3]\n",
    "    q2w1x = q2[..., 6] * q1[..., 3]\n",
    "    q1y2z = q1[..., 4] * q2[..., 5]\n",
    "    q2y1z = q2[..., 4] * q1[..., 5]\n",
    "\n",
    "    q1w2y = q1[..., 6] * q2[..., 4]\n",
    "    q2w1y = q2[..., 6] * q1[..., 4]\n",
    "    q1z2x = q1[..., 5] * q2[..., 3]\n",
    "    q2z1x = q2[..., 5] * q1[..., 3]\n",
    "\n",
    "    q1w2z = q1[..., 6] * q2[..., 5]\n",
    "    q2w1z = q2[..., 6] * q1[..., 5]\n",
    "    q1x2y = q1[..., 3] * q2[..., 4]\n",
    "    q2x1y = q2[..., 3] * q1[..., 4]\n",
    "\n",
    "    q3 = np.zeros(np.broadcast_shapes(q1.shape, q2.shape))\n",
    "    q3[..., 0:3] = compose_qp(q1, q2[..., 0:3])\n",
    "    q3[..., 3] = q1w2x + q2w1x + q1y2z - q2y1z\n",
    "    q3[..., 4] = q1w2y + q2w1y + q1z2x - q2z1x\n",
    "    q3[..., 5] = q1w2z + q2w1z + q1x2y - q2x1y\n",
    "    q3[..., 6] = qww - qxx - qyy - qzz\n",
    "\n",
    "    return q3\n",
    "\n",
    "\n",
    "def compose_qp(q, pt):\n",
    "    px = pt[..., 0]\n",
    "    py = pt[..., 1]\n",
    "    pz = pt[..., 2]\n",
    "\n",
    "    x = q[..., 0]\n",
    "    y = q[..., 1]\n",
    "    z = q[..., 2]\n",
    "    qx = q[..., 3]\n",
    "    qy = q[..., 4]\n",
    "    qz = q[..., 5]\n",
    "    qw = q[..., 6]\n",
    "\n",
    "    qxx = qx**2\n",
    "    qyy = qy**2\n",
    "    qzz = qz**2\n",
    "    qwx = qw * qx\n",
    "    qwy = qw * qy\n",
    "    qwz = qw * qz\n",
    "    qxy = qx * qy\n",
    "    qxz = qx * qz\n",
    "    qyz = qy * qz\n",
    "\n",
    "    pt2 = np.zeros((*np.broadcast_shapes(q.shape[:-1], pt.shape[:-1]), 3))\n",
    "    pt2[..., 0] = x + px + 2 * ((-1 * (qyy + qzz) * px) + ((qxy - qwz) * py) + ((qwy + qxz) * pz))\n",
    "    pt2[..., 1] = y + py + 2 * (((qwz + qxy) * px) + (-1 * (qxx + qzz) * py) + ((qyz - qwx) * pz))\n",
    "    pt2[..., 2] = z + pz + 2 * (((qxz - qwy) * px) + ((qwx + qyz) * py) + (-1 * (qxx + qyy) * pz))\n",
    "\n",
    "    return pt2\n",
    "\n",
    "def normalize_quaternion(quat):\n",
    "    return np.array(quat) / np.linalg.norm(quat, axis=-1, keepdims=True)\n",
    "\n",
    "def quaternion_to_discrete_euler(quaternion, resolution):\n",
    "    euler = Rot.from_quat(quaternion).as_euler('xyz', degrees=True) + 180 # extrinsic rotations\n",
    "    assert np.min(euler) >= 0 and np.max(euler) <= 360\n",
    "    disc = np.around((euler / resolution)).astype(int)\n",
    "    disc[disc == int(360 / resolution)] = 0\n",
    "    return disc\n",
    "\n",
    "def discrete_euler_to_quaternion(discrete_euler, resolution):\n",
    "    euler = (discrete_euler * resolution) - 180\n",
    "    return Rot.from_euler('xyz', euler, degrees=True).as_quat()\n",
    "\n",
    "def get_all_available_grasp_episode(episode):\n",
    "    dex_ycb_cache = \"/home/bepgroup/Projects/PerAct_ws/handover-sim/handover/data/dex-ycb-cache\"\n",
    "    # dex_ycb_cache = \"/home/ywatabe/Projects/PerAct/handover-sim/handover/data/dex-ycb-cache\"\n",
    "    pose_file_str = os.path.join(dex_ycb_cache, \"pose_{:03d}.npz\")\n",
    "    episode_index = episode\n",
    "    pose_file = pose_file_str.format(episode_index)\n",
    "    # print(\"Loading poses from cache: {}\".format(pose_file))\n",
    "    pose_data = np.load(pose_file)\n",
    "\n",
    "    meta_file_str = os.path.join(dex_ycb_cache, \"meta_{:03d}.json\")\n",
    "    meta_file = meta_file_str.format(episode_index)\n",
    "    # print(\"Loading meta from cache: {}\".format(meta_file))\n",
    "    with open(meta_file, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    # Get all poses in scene\n",
    "    pose = pose_data[\"pose_y\"]\n",
    "    ycb_ids = meta[\"ycb_ids\"]\n",
    "    ycb_grasp_ind = meta[\"ycb_grasp_ind\"]\n",
    "    pose[:, :, 2] += 0.92 # Increase z-value by table height\n",
    "\n",
    "    # Get handover-object pose at handover\n",
    "    ycb_grasp_id = ycb_ids[ycb_grasp_ind]\n",
    "    object_pose = pose[-1, ycb_grasp_ind] # Choose last frame\n",
    "    object_posit = object_pose[:3]\n",
    "    object_quat = Rot.from_euler('XYZ', object_pose[3:], degrees=False).as_quat()\n",
    "    object_pose = np.concatenate([object_posit, object_quat])\n",
    "\n",
    "    # Get handover-object class name and grasps\n",
    "    grasp_dir = \"/home/bepgroup/Projects/PerAct_ws/handover-sim/handover/data/assets/grasps\"\n",
    "    # grasp_dir = \"/home/ywatabe/Projects/PerAct/handover-sim/handover/data/assets/grasps\"\n",
    "    ycb_classes = YCB.CLASSES\n",
    "    class_name = ycb_classes[ycb_grasp_id]\n",
    "    grasp_file = os.path.join(grasp_dir, \"{}.npy\".format(class_name))\n",
    "    # print(\"Loading grasps from:\", grasp_file)\n",
    "    data = np.load(grasp_file, allow_pickle=True, encoding=\"bytes\")\n",
    "    grasps = data.item()[b\"transforms\"]\n",
    "    grasps_pq = np.zeros((len(grasps), 7))\n",
    "    grasps_pq[:, 0:3] = grasps[:, :3, 3]\n",
    "    grasps_pq[:, 3:7] = Rot.from_matrix(grasps[:, :3, :3]).as_quat()\n",
    "\n",
    "    # Get object grasps and translate to handover-object pose\n",
    "    object_grasps = compose_qq(object_pose, grasps_pq)\n",
    "    # NOTE: I think we need to translate to ee-frame for getting the correct grasps - YES we DO\n",
    "    object_grasps = compose_qq(object_grasps, np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.7071068, -0.7071068]))\n",
    "\n",
    "    # Convert handoer-object graspposes to voxel based\n",
    "    scene_bounds = np.array(SCENE_BOUNDS)\n",
    "    object_grasps_trans = (object_grasps[:,:3] - scene_bounds[:3]) / (scene_bounds[3:] - scene_bounds[:3]) * 100\n",
    "    object_grasps_rot = quaternion_to_discrete_euler(object_grasps[:, 3:7], ROTATION_RESOLUTION)\n",
    "    object_grasps_voxel = np.concatenate((object_grasps_trans.astype(int), object_grasps_rot), axis=1)\n",
    "    return object_grasps_voxel, object_grasps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the grasps for the episode using matplotlib\n",
    "\n",
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# from pyquaternion import Quaternion  # For converting quaternions to rotation vectors\n",
    "\n",
    "# # Example 2D ndarray of x, y, z, and quaternion (x, y, z, w) coordinates\n",
    "# data = object_grasps\n",
    "\n",
    "# # Extract positions and quaternions\n",
    "# positions = data[:, :3]\n",
    "# quaternions = data[:, 3:]\n",
    "\n",
    "# # Create lists to hold the arrow start and end points\n",
    "# x_start, y_start, z_start = [], [], []\n",
    "# x_end_x, y_end_x, z_end_x = [], [], []\n",
    "# x_end_y, y_end_y, z_end_y = [], [], []\n",
    "# x_end_z, y_end_z, z_end_z = [], [], []\n",
    "\n",
    "# # Calculate orientation vectors derived from quaternions\n",
    "# for pos, quat in zip(positions, quaternions):\n",
    "#     q = Quaternion(quat[3], quat[0], quat[1], quat[2])  # Convert to Quaternion object\n",
    "#     x_dir = q.rotate([1, 0, 0])  # X-axis direction\n",
    "#     y_dir = q.rotate([0, 1, 0])  # Y-axis direction\n",
    "#     z_dir = q.rotate([0, 0, 1])  # Z-axis direction\n",
    "    \n",
    "#     # Start points\n",
    "#     x_start.append(pos[0])\n",
    "#     y_start.append(pos[1])\n",
    "#     z_start.append(pos[2])\n",
    "    \n",
    "#     # End points for x, y, z directions\n",
    "#     x_end_x.append(pos[0] + x_dir[0])\n",
    "#     y_end_x.append(pos[1] + x_dir[1])\n",
    "#     z_end_x.append(pos[2] + x_dir[2])\n",
    "    \n",
    "#     x_end_y.append(pos[0] + y_dir[0])\n",
    "#     y_end_y.append(pos[1] + y_dir[1])\n",
    "#     z_end_y.append(pos[2] + y_dir[2])\n",
    "    \n",
    "#     x_end_z.append(pos[0] + z_dir[0])\n",
    "#     y_end_z.append(pos[1] + z_dir[1])\n",
    "#     z_end_z.append(pos[2] + z_dir[2])\n",
    "\n",
    "# # Create a 3D scatter plot for positions\n",
    "# scatter = go.Scatter3d(\n",
    "#     x=positions[:, 0],\n",
    "#     y=positions[:, 1],\n",
    "#     z=positions[:, 2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(size=5, color='blue'),\n",
    "#     name='Positions'\n",
    "# )\n",
    "\n",
    "# # Create 3D quiver-like plots for X, Y, and Z orientations\n",
    "# quiver_x = go.Cone(\n",
    "#     x=x_start,\n",
    "#     y=y_start,\n",
    "#     z=z_start,\n",
    "#     u=np.array(x_end_x) - np.array(x_start),\n",
    "#     v=np.array(y_end_x) - np.array(y_start),\n",
    "#     w=np.array(z_end_x) - np.array(z_start),\n",
    "#     sizemode=\"scaled\",\n",
    "#     sizeref=0.5,\n",
    "#     anchor=\"tail\",\n",
    "#     colorscale=[[0, 'red'], [1, 'red']],\n",
    "#     name='X-axis'\n",
    "# )\n",
    "\n",
    "# quiver_y = go.Cone(\n",
    "#     x=x_start,\n",
    "#     y=y_start,\n",
    "#     z=z_start,\n",
    "#     u=np.array(x_end_y) - np.array(x_start),\n",
    "#     v=np.array(y_end_y) - np.array(y_start),\n",
    "#     w=np.array(z_end_y) - np.array(z_start),\n",
    "#     sizemode=\"scaled\",\n",
    "#     sizeref=0.5,\n",
    "#     anchor=\"tail\",\n",
    "#     colorscale=[[0, 'green'], [1, 'green']],\n",
    "#     name='Y-axis'\n",
    "# )\n",
    "\n",
    "# quiver_z = go.Cone(\n",
    "#     x=x_start,\n",
    "#     y=y_start,\n",
    "#     z=z_start,\n",
    "#     u=np.array(x_end_z) - np.array(x_start),\n",
    "#     v=np.array(y_end_z) - np.array(y_start),\n",
    "#     w=np.array(z_end_z) - np.array(z_start),\n",
    "#     sizemode=\"scaled\",\n",
    "#     sizeref=0.5,\n",
    "#     anchor=\"tail\",\n",
    "#     colorscale=[[0, 'blue'], [1, 'blue']],\n",
    "#     name='Z-axis'\n",
    "# )\n",
    "\n",
    "# # Combine plots\n",
    "# fig = go.Figure(data=[scatter, quiver_x, quiver_y, quiver_z])\n",
    "\n",
    "# # Customize the layout\n",
    "# fig.update_layout(\n",
    "#     scene=dict(\n",
    "#         xaxis_title='X',\n",
    "#         yaxis_title='Y',\n",
    "#         zaxis_title='Z'\n",
    "#     ),\n",
    "#     title='Interactive 3D Visualization of Positions and Orientations'\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_helpers.build_training import NumpyEncoder\n",
    "from notebook_helpers.constants import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from arm.utils import get_gripper_render_pose\n",
    "from arm.utils import visualise_voxel\n",
    "\n",
    "\n",
    "model_run_iter = natsorted([run for run in os.listdir(run_dir) if \"run\" in run])[-1]\n",
    "\n",
    "# Choose the loss metric at which model is saved\n",
    "# chosen_model = \"best_model_train\"\n",
    "# chosen_model = \"best_model_test\"\n",
    "# chosen_model = \"best_model_general\"\n",
    "chosen_model = \"last_model\"\n",
    "\n",
    "# Save results\n",
    "model_run_scored = dict()\n",
    "model_run_distances = dict()\n",
    "# model_run_rotations = dict() # Ignore rotation, location matters most\n",
    "\n",
    "# Load model if available\n",
    "try:\n",
    "    peract_agent.load_weights(os.path.join(run_dir, model_run_iter, chosen_model))\n",
    "except:\n",
    "    print(f\"Model {model_run_iter} not found, skipping.\")\n",
    "\n",
    "fig, axs = plt.subplots(2,1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "for analyzed_episode in TEST_INDEXES:\n",
    "    \n",
    "\n",
    "    all_frames_episode = dict_test_episodes_frames[analyzed_episode]\n",
    "    \n",
    "    episode_frame_length = len(all_frames_episode)-1\n",
    "    episode_frames_frame_point = []\n",
    "    episode_frames_dist_expert = []\n",
    "    episode_frames_rot_expert = []\n",
    "    episode_frames_dist = []\n",
    "    episode_frames_rot = []\n",
    "\n",
    "    # fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    for episode_frame_i, analyzed_frame_episode in enumerate(all_frames_episode): # Loop through all available frames of the replay buffer\n",
    "\n",
    "        while True: # Find the frames allocated to the episode\n",
    "            batch = next(test_data_iter)\n",
    "            lang_goal = batch['lang_goal'][0][0][0]\n",
    "            if lang_goal == analyzed_frame_episode:\n",
    "                batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "                break\n",
    "\n",
    "        update_dict = peract_agent.update(None, batch, backprop=False)\n",
    "\n",
    "        # Get predictions\n",
    "        pred_trans = update_dict[\"pred_action\"][\"trans\"]\n",
    "        pred_rot_grip = update_dict[\"pred_action\"][\"rot_and_grip\"]\n",
    "\n",
    "        # Get expert action and convert to real coordinates\n",
    "        gt_trans_expert = update_dict[\"expert_action\"][\"action_trans\"][0]\n",
    "        gt_rot_and_grip_expert = update_dict[\"expert_action\"][\"rot_and_grip\"][0]\n",
    "        gt_quat_expert = discrete_euler_to_quaternion(gt_rot_and_grip_expert[:3], ROTATION_RESOLUTION)\n",
    "        pred_quat = discrete_euler_to_quaternion(pred_rot_grip[:,:3], ROTATION_RESOLUTION)\n",
    "\n",
    "        # Calculate distance/angular error: pred vs. expert\n",
    "        dist_expert = np.round(np.linalg.norm(pred_trans-gt_trans_expert, axis=1), 4)\n",
    "        angle_dist_expert = 1.0 - np.abs(np.sum(normalize_quaternion(gt_quat_expert) * normalize_quaternion(pred_quat)))\n",
    "        print(f\"prediction: {pred_trans}, {pred_rot_grip[:,:3]}\")\n",
    "        print(f\"expert: {gt_trans_expert}, {gt_rot_and_grip_expert[:3]} - trans-diff: {dist_expert} - ang-diff: {angle_dist_expert}\")\n",
    "        episode_frames_dist_expert.append(dist_expert)\n",
    "        episode_frames_rot_expert.append(angle_dist_expert)\n",
    "\n",
    "        # Get all ground truth grasps to compare\n",
    "        gt_voxel_all, gt_all = get_all_available_grasp_episode(analyzed_episode)\n",
    "        gt_trans_all = gt_voxel_all[:,:3]\n",
    "        gt_rot_all = gt_voxel_all[:,3:]\n",
    "        gt_quat_all = discrete_euler_to_quaternion(gt_rot_all, 5)\n",
    "        gt_quat_all_flipped = compose_qq(gt_all, np.array([0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0]))[:,3:] # Also obtain the flipped (wrist) ground truth\n",
    "\n",
    "        # Calcualte distance/angular error: pred vs. any-GT\n",
    "        dist_all = np.round(np.linalg.norm(pred_trans-gt_trans_all, axis=1), 4)\n",
    "        min_dist_idx = np.argmin(dist_all)\n",
    "        dist = dist_all[min_dist_idx]\n",
    "        angle_dist = 1.0 - np.abs(np.sum(gt_quat_all[min_dist_idx] * pred_quat))\n",
    "        angle_dist_flipped = 1.0 - np.abs(np.sum(gt_quat_all_flipped[min_dist_idx] * pred_quat))\n",
    "        angle_dist = min(angle_dist, angle_dist_flipped)\n",
    "        print(f\"ground-truth: {gt_trans_all[min_dist_idx]}, {gt_rot_all[min_dist_idx]} - trans-diff: {dist} - ang-diff: {angle_dist}\")\n",
    "        print(f\"Quaternion ground-truth: {gt_quat_all[min_dist_idx]} {pred_quat}\")\n",
    "        episode_frames_dist.append(dist)\n",
    "        episode_frames_rot.append(angle_dist)\n",
    "\n",
    "        episode_frames_frame_point.append(episode_frame_i/episode_frame_length)\n",
    "\n",
    "        # Plot the corresponding Prediction vs. Expert vs. Closest-GT\n",
    "        # fig_voxel = plt.figure(figsize=(16, 8))\n",
    "        voxel_size = 0.045\n",
    "        render_gripper = True\n",
    "        voxel_scale = voxel_size * 100\n",
    "\n",
    "        for i in range(3):\n",
    "            vis_voxel_grid = update_dict['voxel_grid'][0].cpu().numpy()\n",
    "            vis_trans_coord = pred_trans\n",
    "            if i == 0:\n",
    "                vis_gt_coord = pred_trans[0]\n",
    "                continuous_quat = pred_quat[0]\n",
    "            if i == 1:\n",
    "                vis_gt_coord = gt_trans_expert\n",
    "                continuous_quat = gt_quat_expert\n",
    "            if i == 2:\n",
    "                vis_gt_coord = gt_trans_all[min_dist_idx]\n",
    "                continuous_quat = gt_quat_all[min_dist_idx]\n",
    "            \n",
    "            scene_bounds = np.array(SCENE_BOUNDS)\n",
    "            continuous_trans = vis_gt_coord*0.01 + scene_bounds[:3]\n",
    "\n",
    "            gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
    "                                                       SCENE_BOUNDS[:3],\n",
    "                                                       continuous_trans,\n",
    "                                                       continuous_quat)\n",
    "\n",
    "            rendered_img = visualise_voxel(vis_voxel_grid,\n",
    "                                           None,\n",
    "                                           vis_trans_coord,\n",
    "                                           vis_gt_coord,\n",
    "                                           voxel_size=voxel_size,\n",
    "                                           rotation_amount=np.deg2rad(45),\n",
    "                                           render_gripper=render_gripper,\n",
    "                                           gripper_pose=gripper_pose_mat,\n",
    "                                           gripper_mesh_scale=voxel_scale)\n",
    "            # fig_voxel.add_subplot(1, 3, i+1)\n",
    "            # plt.imshow(rendered_img)\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "    axs[0].plot(episode_frames_frame_point, episode_frames_dist_expert, label=f'episode (expert): {analyzed_episode}', linestyle='dashed')\n",
    "    axs[0].plot(episode_frames_frame_point, episode_frames_dist, label=f'episode (any): {analyzed_episode}')\n",
    "\n",
    "    print(episode_frames_frame_point, episode_frames_rot_expert, episode_frames_rot)\n",
    "    axs[1].plot(episode_frames_frame_point, episode_frames_rot_expert, label=f'episode (expert): {analyzed_episode}', linestyle='dashed')\n",
    "    axs[1].plot(episode_frames_frame_point, episode_frames_rot, label=f'episode (any): {analyzed_episode}')\n",
    "\n",
    "axs[0].set_xlabel(f\"Episode length [timestep / frame length] (@t=1, timestamp is at approach)\")\n",
    "axs[0].set_ylabel(f\"Translation error [voxels]\")\n",
    "axs[1].set_xlabel(f\"Episode length [timestep / frame length] (@t=1, timestamp is at approach)\")\n",
    "axs[1].set_ylabel(f\"Rotaion error\")\n",
    "\n",
    "plt.title(f\"{TASK}\\n{settings}\")\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
