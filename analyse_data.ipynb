{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_ # bad trick to fix numpy version issue :(\n",
    "import os\n",
    "import sys\n",
    "from natsort import natsorted\n",
    "\n",
    "sys.path = [p for p in sys.path if '/peract/' not in p]\n",
    "\n",
    "# Set `PYOPENGL_PLATFORM=egl` for pyrender visualizations\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\" # Depends on your computer and available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Depending on your workspace, you may already have this repository installe, otherwise clone once again\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'peract_colab')):\n",
    "    !git clone https://github.com/yuki1003/peract_colab.git\n",
    "\n",
    "!cd peract_colab && git pull origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if modules in peract_colab repository are recognized.\n",
    "try: # test import\n",
    "    from rlbench.utils import get_stored_demo\n",
    "except ImportError as error_message:\n",
    "    print(error_message)\n",
    "    print(\"Adding peract_colab repository to system path.\")\n",
    "    sys.path.append('peract_colab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "from arm.replay_buffer import create_replay, fill_replay, uniform_fill_replay, fill_replay_copy_with_crop_from_approach\n",
    "from yarr.replay_buffer.wrappers.pytorch_replay_buffer import PyTorchReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STATIC VALUES USED IN BELOW FUNCTION: SETTING THEM AS GLOBAL FOR FURTHER USE\n",
    "\n",
    "#___DATA___\n",
    "TASK = 'handing_over_banana'\n",
    "\n",
    "# Data Constants\n",
    "WORKSPACE_DIR = os.getcwd()\n",
    "DATA_FOLDER = os.path.join(WORKSPACE_DIR, \"task_data\", \"handoversim\")\n",
    "EPISODES_FOLDER = os.path.join(TASK, \"all_variations\", \"episodes\")\n",
    "\n",
    "EPISODE_FOLDER = 'episode%d'\n",
    "SETUP = \"s1\" # Options: \"s1\"\n",
    "train_data_path = os.path.join(DATA_FOLDER, f\"train_{SETUP}\", EPISODES_FOLDER)\n",
    "TRAIN_INDEXES = [int(episode_nr.replace(\"episode\", \"\")) for episode_nr in natsorted(os.listdir(train_data_path))][:3]\n",
    "test_data_path = os.path.join(DATA_FOLDER, f\"val_{SETUP}\", EPISODES_FOLDER)\n",
    "TEST_INDEXES = [int(episode_nr.replace(\"episode\", \"\")) for episode_nr in natsorted(os.listdir(test_data_path))][:3]\n",
    "\n",
    "print(f\"TRAIN | Total #: {len(TRAIN_INDEXES)}, indices: {TRAIN_INDEXES}\")\n",
    "print(f\"TEST | Total #: {TEST_INDEXES}\")\n",
    "\n",
    "# Replaybuffer related constants\n",
    "LOW_DIM_SIZE = 4    # 4 dimensions - proprioception: {gripper_open, left_finger_joint, right_finger_joint, timestep}\n",
    "IMAGE_SIZE =  128  # 128x128 - if you want to use higher voxel resolutions like 200^3, you might want to regenerate the dataset with larger images\n",
    "DEMO_AUGMENTATION_EVERY_N = 10 # Only select every n-th frame to use for replaybuffer from demo\n",
    "ROTATION_RESOLUTION = 5 # degree increments per axis\n",
    "TARGET_OBJ_KEYPOINTS=False # Real - (changed later)\n",
    "TARGET_OBJ_USE_LAST_KP=False # Real - (changed later)\n",
    "TARGET_OBJ_IS_AVAIL = True # HandoverSim - (changed later)\n",
    "\n",
    "DEPTH_SCALE = 1000\n",
    "STOPPING_DELTA = 0.001\n",
    "SCENE_BOUNDS = [0.11, -0.5, 0.8, 1.11, 0.5, 1.8]  # Must be 1m each\n",
    "\n",
    "# Training Settings Constants\n",
    "BATCH_SIZE = 1\n",
    "VOXEL_SIZES = [100]  # 100x100x100 voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_peract_agent(settings):\n",
    "\n",
    "    # BATCH SETTINGS\n",
    "    FILL_REPLAY_SETTING = settings['fill_replay_setting']\n",
    "    CAMERAS = settings['cameras']\n",
    "    USE_APPROACH = settings['keypoint_approach']\n",
    "\n",
    "    # Summary of run properties\n",
    "    print(\"\\nExperiment Setup\")\n",
    "    print(f\"Task: {TASK} - SETUP: {SETUP} - Cameras: {len(CAMERAS)}\")\n",
    "    print(\"Run Properties\")\n",
    "    print(f\"Fill replay setting: {FILL_REPLAY_SETTING}\")\n",
    "\n",
    "    #___REPLAY-BUFFER___\n",
    "    train_replay_storage_dir = os.path.join(WORKSPACE_DIR,'replay_train')\n",
    "    if os.path.exists(train_replay_storage_dir):\n",
    "        print(f\"Emptying {train_replay_storage_dir}\")\n",
    "        shutil.rmtree(train_replay_storage_dir)\n",
    "    if not os.path.exists(train_replay_storage_dir):\n",
    "        print(f\"Could not find {train_replay_storage_dir}, creating directory.\")\n",
    "        os.mkdir(train_replay_storage_dir)\n",
    "\n",
    "    test_replay_storage_dir = os.path.join(WORKSPACE_DIR,'replay_test')\n",
    "    if os.path.exists(test_replay_storage_dir):\n",
    "        print(f\"Emptying {test_replay_storage_dir}\")\n",
    "        shutil.rmtree(test_replay_storage_dir)\n",
    "    if not os.path.exists(test_replay_storage_dir):\n",
    "        print(f\"Could not find {test_replay_storage_dir}, creating directory.\")\n",
    "        os.mkdir(test_replay_storage_dir)\n",
    "\n",
    "    train_replay_buffer = create_replay(batch_size=BATCH_SIZE,\n",
    "                                        timesteps=1,\n",
    "                                        save_dir=train_replay_storage_dir,\n",
    "                                        cameras=CAMERAS,\n",
    "                                        voxel_sizes=VOXEL_SIZES,\n",
    "                                        image_size=IMAGE_SIZE,\n",
    "                                        low_dim_size=LOW_DIM_SIZE)\n",
    "\n",
    "    test_replay_buffer = create_replay(batch_size=BATCH_SIZE,\n",
    "                                    timesteps=1,\n",
    "                                    save_dir=test_replay_storage_dir,\n",
    "                                    cameras=CAMERAS,\n",
    "                                    voxel_sizes=VOXEL_SIZES,\n",
    "                                    image_size=IMAGE_SIZE,\n",
    "                                    low_dim_size=LOW_DIM_SIZE)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    clip_model, preprocess = clip.load(\"RN50\", device=device) # CLIP-ResNet50\n",
    "\n",
    "    print(\"-- Train Buffer --\")\n",
    "    if FILL_REPLAY_SETTING.lower() == \"uniform\":\n",
    "        uniform_fill_replay(\n",
    "            data_path=train_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=train_replay_buffer,\n",
    "            # start_idx=0,\n",
    "            # num_demos=NUM_DEMOS,\n",
    "            d_indexes=TRAIN_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    elif FILL_REPLAY_SETTING.lower() == \"crop\":\n",
    "        fill_replay_copy_with_crop_from_approach(\n",
    "            data_path=train_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=train_replay_buffer,\n",
    "            # start_idx=0,\n",
    "            # num_demos=NUM_DEMOS,\n",
    "            d_indexes=TRAIN_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    elif FILL_REPLAY_SETTING.lower() == \"standard\":\n",
    "        fill_replay(\n",
    "            data_path=train_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=train_replay_buffer,\n",
    "            # start_idx=0,\n",
    "            # num_demos=NUM_DEMOS,\n",
    "            d_indexes=TRAIN_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\"Unkown setting for fill replay buffer\")\n",
    "\n",
    "        \n",
    "    print(\"-- Test Buffer --\")\n",
    "    if FILL_REPLAY_SETTING.lower() == \"uniform\":\n",
    "        uniform_fill_replay(\n",
    "            data_path=test_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=test_replay_buffer,\n",
    "            # start_idx=start_idx,\n",
    "            # num_demos=num_demos,\n",
    "            d_indexes=TEST_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    elif FILL_REPLAY_SETTING.lower() == \"crop\":\n",
    "        fill_replay_copy_with_crop_from_approach(\n",
    "            data_path=test_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=test_replay_buffer,\n",
    "            # start_idx=start_idx,\n",
    "            # num_demos=num_demos,\n",
    "            d_indexes=TEST_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    elif FILL_REPLAY_SETTING.lower() == \"standard\":\n",
    "        fill_replay(\n",
    "            data_path=test_data_path,\n",
    "            episode_folder=EPISODE_FOLDER,\n",
    "            replay=test_replay_buffer,\n",
    "            # start_idx=start_idx,\n",
    "            # num_demos=num_demos,\n",
    "            d_indexes=TEST_INDEXES,\n",
    "            demo_augmentation=True,\n",
    "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
    "            cameras=CAMERAS,\n",
    "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
    "            voxel_sizes=VOXEL_SIZES,\n",
    "            rotation_resolution=ROTATION_RESOLUTION,\n",
    "            crop_augmentation=False,\n",
    "            depth_scale=DEPTH_SCALE,\n",
    "            use_approach=USE_APPROACH,\n",
    "            approach_distance=0.3,\n",
    "            stopping_delta=STOPPING_DELTA,\n",
    "            target_obj_keypoint=TARGET_OBJ_KEYPOINTS,\n",
    "            target_obj_use_last_kp=TARGET_OBJ_USE_LAST_KP,\n",
    "            target_obj_is_avail=TARGET_OBJ_IS_AVAIL,\n",
    "            clip_model=clip_model,\n",
    "            device=device,\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\"Unkown setting for fill replay buffer\")\n",
    "\n",
    "\n",
    "    # delete the CLIP model since we have already extracted language features\n",
    "    del clip_model\n",
    "\n",
    "    # wrap buffer with PyTorch dataset and make iterator\n",
    "    train_wrapped_replay = PyTorchReplayBuffer(train_replay_buffer)\n",
    "    train_dataset = train_wrapped_replay.dataset()\n",
    "    train_data_iter = iter(train_dataset)\n",
    "\n",
    "    test_wrapped_replay = PyTorchReplayBuffer(test_replay_buffer)\n",
    "    test_dataset = test_wrapped_replay.dataset()\n",
    "    test_data_iter = iter(test_dataset)\n",
    "\n",
    "    return train_data_iter, test_data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "available_cameras = [f\"view_{camera_i}\" for camera_i in range(3)]\n",
    "# Grid search\n",
    "# grid = {\n",
    "#     'fill_replay_setting': [\"\", \"\"],\n",
    "#     'cameras': [available_cameras],\n",
    "#     'RGB_AUGMENTATION': ['None'],\n",
    "#     'keypoint_approach': [True, False]\n",
    "# }\n",
    "\n",
    "grid = {\n",
    "    'fill_replay_setting': [\"crop\"],\n",
    "    'cameras': [available_cameras],\n",
    "    'RGB_AUGMENTATION': ['None'],\n",
    "    'keypoint_approach': [True]\n",
    "}\n",
    "# Loop over al grid search combinations\n",
    "counter = 0\n",
    "lst_settings = []\n",
    "for values in itertools.product(*grid.values()):\n",
    "    \n",
    "    point = dict(zip(grid.keys(), values))\n",
    "    # merge the general settings\n",
    "    settings = {**point}\n",
    "    lst_settings.append(settings)\n",
    "    print(counter, settings)\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_SETTING = 0\n",
    "settings = lst_settings[CHOSEN_SETTING]\n",
    "\n",
    "# FILL_REPLAY_SETTING = settings['fill_replay_uniform']\n",
    "# CAMERAS = settings['cameras']\n",
    "# USE_APPROACH = settings['keypoint_approach']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_data_iter, test_data_iter = inspect_peract_agent(lst_settings[CHOSEN_SETTING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from agent.utils import _preprocess_inputs\n",
    "from agent.voxel_grid import VoxelGrid\n",
    "from arm.utils import get_gripper_render_pose, visualise_voxel_video\n",
    "\n",
    "## First find analysis\n",
    "analyzed_episode = 46\n",
    "\n",
    "lang_goals = set()\n",
    "all_frames = set()\n",
    "for i in range(1000):\n",
    "    # sample from dataset\n",
    "    batch = next(train_data_iter)\n",
    "    lang_goal = batch['lang_goal'][0][0][0]\n",
    "    task, episode, frame, kp = lang_goal.split('-')\n",
    "    if analyzed_episode == int(episode.replace('episode_', '')):\n",
    "        lang_goals.add(lang_goal)\n",
    "        all_frames.add(frame)\n",
    "\n",
    "replay_buffer = natsorted(lang_goals)\n",
    "all_frames = natsorted(all_frames)\n",
    "\n",
    "print(replay_buffer, all_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Directory to store the frames\n",
    "output_dir = \"temp_frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Video parameters\n",
    "fps = 10\n",
    "print(settings)\n",
    "output_video = f\"episode_{analyzed_episode}-uniform_{settings['fill_replay_setting']}-approach_{settings['keypoint_approach']}.mp4\"\n",
    "print(output_video)\n",
    "video_writer = None\n",
    "\n",
    "# initialize voxelizer\n",
    "vox_grid = VoxelGrid(\n",
    "    coord_bounds=SCENE_BOUNDS,\n",
    "    voxel_size=VOXEL_SIZES[0],\n",
    "    device=device,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    feature_size=3,\n",
    "    max_num_coords=np.prod([IMAGE_SIZE, IMAGE_SIZE]) * len(settings[\"cameras\"]),\n",
    ")\n",
    "\n",
    "# metric scene bounds\n",
    "bounds = torch.tensor(SCENE_BOUNDS,device=device).unsqueeze(0)\n",
    "\n",
    "for analyzed_frame in all_frames: # Loop through all available frames of the replay buffer\n",
    "    frame_language_goals = []\n",
    "    for replay_buffer_i in replay_buffer:\n",
    "        if analyzed_frame in replay_buffer_i:\n",
    "            frame_language_goals.append(replay_buffer_i) # Select ones from replay buffer matching frame number\n",
    "\n",
    "    frame_language_goal = frame_language_goals[0] # Use as foundation\n",
    "    while True:\n",
    "        batch = next(train_data_iter)\n",
    "        lang_goal = batch['lang_goal'][0][0][0]\n",
    "        if lang_goal == frame_language_goal:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "            # sample\n",
    "            action_trans = batch['trans_action_indicies'][:, -1, :3].int()\n",
    "            action_rot_grip = batch['rot_grip_action_indicies'][:, -1].int()\n",
    "            action_gripper_pose = batch['gripper_pose'][:, -1]\n",
    "            break\n",
    "\n",
    "    if len(frame_language_goals) > 1: # If frame with second keypoint/action available, find!\n",
    "        frame_language_goal = frame_language_goals[1]\n",
    "        while True:\n",
    "            batch_temp = next(train_data_iter)\n",
    "            lang_goal = batch_temp['lang_goal'][0][0][0]\n",
    "            \n",
    "            if lang_goal == frame_language_goal:\n",
    "                batch_temp = {k: v.to(device) for k, v in batch_temp.items() if type(v) == torch.Tensor}\n",
    "                next_action_gripper_pose = batch_temp['gripper_pose'][:, -1]\n",
    "                # sample\n",
    "                action_gripper_pose = torch.cat([action_gripper_pose, next_action_gripper_pose], dim=0)\n",
    "                break\n",
    "\n",
    "    # preprocess observations\n",
    "    rgbs_pcds, _ = _preprocess_inputs(batch, settings[\"cameras\"])\n",
    "    pcds = [rp[1] for rp in rgbs_pcds]\n",
    "\n",
    "    # batch_size\n",
    "    bs = rgbs_pcds[0][0].shape[0]\n",
    "\n",
    "    # identity matrix\n",
    "    identity_4x4 = torch.eye(4).unsqueeze(0).repeat(bs, 1, 1).to(device=device)\n",
    "\n",
    "    # flatten observations\n",
    "    pcd_flat = torch.cat([p.permute(0, 2, 3, 1).reshape(bs, -1, 3) for p in pcds], 1)\n",
    "    rgb = [rp[0] for rp in rgbs_pcds] # Loop per camera\n",
    "    feat_size = rgb[0].shape[1]\n",
    "    flat_imag_features = torch.cat(\n",
    "        [p.permute(0, 2, 3, 1).reshape(bs, -1, feat_size) for p in rgb], 1)\n",
    "\n",
    "    # voxelize!\n",
    "    voxel_grid = vox_grid.coords_to_bounding_voxel_grid(pcd_flat,\n",
    "                                                        flat_imag_features,\n",
    "                                                        coord_bounds=bounds)\n",
    "    # swap to channels fist\n",
    "    voxel_grid = voxel_grid.permute(0, 4, 1, 2, 3).detach().cpu().numpy()\n",
    "\n",
    "    # expert action voxel indicies and coord\n",
    "    coords_indicies = action_trans\n",
    "    # discrete to continuous\n",
    "    continuous_trans = action_gripper_pose[:,:3].detach().cpu().numpy()\n",
    "    continuous_quat = action_gripper_pose[:,3:].detach().cpu().numpy()\n",
    "\n",
    "    # gripper visualization pose\n",
    "    voxel_size = 0.045\n",
    "    voxel_scale = voxel_size * 100\n",
    "    gripper_pose_mat = []\n",
    "    for continuous_trans_i, continuous_quat_i in zip(continuous_trans, continuous_quat):\n",
    "        gripper_pose_mat_i = get_gripper_render_pose(voxel_scale,\n",
    "                                                SCENE_BOUNDS[:3],\n",
    "                                                continuous_trans_i,\n",
    "                                                continuous_quat_i)\n",
    "        gripper_pose_mat.append(gripper_pose_mat_i)\n",
    "\n",
    "    gripper_pose_mat = np.squeeze(np.array(gripper_pose_mat))\n",
    "\n",
    "    rendered_img_0 = visualise_voxel_video(voxel_grid[0],\n",
    "                                None,\n",
    "                                None,\n",
    "                                coords_indicies[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(0),\n",
    "                                render_gripper=True,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale,\n",
    "                                perspective = False)\n",
    "\n",
    "    rendered_img_90 = visualise_voxel_video(voxel_grid[0],\n",
    "                                None,\n",
    "                                None,\n",
    "                                coords_indicies[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(90),\n",
    "                                render_gripper=True,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale,\n",
    "                                perspective = False)\n",
    "\n",
    "    rendered_img_180 = visualise_voxel_video(voxel_grid[0],\n",
    "                                None,\n",
    "                                None,\n",
    "                                coords_indicies[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(180),\n",
    "                                render_gripper=True,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale,\n",
    "                                perspective = False)\n",
    "\n",
    "    rendered_img_270 = visualise_voxel_video(voxel_grid[0],\n",
    "                                None,\n",
    "                                None,\n",
    "                                coords_indicies[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(270),\n",
    "                                render_gripper=True,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale,\n",
    "                                perspective = False)\n",
    "\n",
    "    rendered_img_0_persp = visualise_voxel_video(voxel_grid[0],\n",
    "                                None,\n",
    "                                None,\n",
    "                                coords_indicies[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(0),\n",
    "                                render_gripper=True,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "    rendered_img_side_persp = visualise_voxel_video(voxel_grid[0],\n",
    "                                None,\n",
    "                                None,\n",
    "                                coords_indicies[0],\n",
    "                                highlight_alpha=1.0,\n",
    "                                voxel_size=voxel_size,\n",
    "                                rotation_amount=np.deg2rad(45),\n",
    "                                render_gripper=True,\n",
    "                                gripper_pose=gripper_pose_mat,\n",
    "                                gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "                    \n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.add_subplot(3, 2, 1)\n",
    "    plt.imshow(rendered_img_0)\n",
    "    plt.title(\"0-degree view\")\n",
    "    fig.add_subplot(3, 2, 2)\n",
    "    plt.imshow(rendered_img_90)\n",
    "    plt.title(\"90-degree view\")\n",
    "    fig.add_subplot(3, 2, 3)\n",
    "    plt.imshow(rendered_img_180)\n",
    "    plt.title(\"180-degree view\")\n",
    "    fig.add_subplot(3, 2, 4)\n",
    "    plt.imshow(rendered_img_270)\n",
    "    plt.title(\"270-degree view\")\n",
    "    fig.add_subplot(3, 2, 5)\n",
    "    plt.imshow(rendered_img_0_persp)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"00-degree view\")\n",
    "    fig.add_subplot(3, 2, 6)\n",
    "    plt.imshow(rendered_img_side_persp)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"side view\")\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    # Save the figure as an image\n",
    "    frame_path = os.path.join(output_dir, f\"frame_{analyzed_frame}.png\")\n",
    "    plt.savefig(frame_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Add the frame to the video\n",
    "    if video_writer is None:\n",
    "        # Initialize video writer with the first frame's dimensions\n",
    "        sample_frame = cv2.imread(frame_path)\n",
    "        height, width, _ = sample_frame.shape\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "        video_writer = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "    video_writer.write(cv2.imread(frame_path))\n",
    "\n",
    "# Release the video writer and cleanup\n",
    "if video_writer:\n",
    "    video_writer.release()\n",
    "\n",
    "# Remove temporary frames directory\n",
    "for file in os.listdir(output_dir):\n",
    "    os.remove(os.path.join(output_dir, file))\n",
    "os.rmdir(output_dir)\n",
    "\n",
    "print(f\"Video saved as {output_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
