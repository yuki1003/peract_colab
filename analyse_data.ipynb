{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_ # bad trick to fix numpy version issue :(\n",
    "import os\n",
    "import sys\n",
    "from natsort import natsorted\n",
    "\n",
    "\n",
    "sys.path = [p for p in sys.path if '/peract/' not in p]\n",
    "\n",
    "# Set `PYOPENGL_PLATFORM=egl` for pyrender visualizations\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\" # Depends on your computer and available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DISPLAY ALL SETTINGS\n",
    "import itertools\n",
    "\n",
    "\n",
    "available_cameras = [f\"view_{camera_i}\" for camera_i in range(6)]# + [\"wrist\"]\n",
    "# Grid search\n",
    "# grid = {\n",
    "#     'fill_replay_setting': [\"\", \"\"],\n",
    "#     'cameras': [available_cameras],\n",
    "#     'keypoint_approach': [True, False]\n",
    "# }\n",
    "\n",
    "grid = {\n",
    "    'fill_replay_setting': [\"standard\"],\n",
    "    'cameras': [available_cameras, [available_cameras[0]]],\n",
    "    'keypoint_approach': [True],\n",
    "    'demo_augm_n': [5] # Not used for fill_replay_setting == \"uniform\"\n",
    "}\n",
    "# Loop over al grid search combinations\n",
    "counter = 0\n",
    "lst_settings = []\n",
    "for values in itertools.product(*grid.values()):\n",
    "    \n",
    "    point = dict(zip(grid.keys(), values))\n",
    "    # merge the general settings\n",
    "    settings = {**point}\n",
    "    lst_settings.append(settings)\n",
    "    print(counter, settings)\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE SETTING TO VISUALIZE (CREATE VIDEO OUT OF)\n",
    "from notebook_helpers.build_replay import load_replay_buffer\n",
    "from notebook_helpers.constants import * # Load global constant variables from constants.py\n",
    "\n",
    "CHOSEN_SETTING = 0\n",
    "settings = lst_settings[CHOSEN_SETTING]\n",
    "\n",
    "# FILL_REPLAY_SETTING = settings['fill_replay_uniform']\n",
    "# CAMERAS = settings['cameras']\n",
    "# USE_APPROACH = settings['keypoint_approach']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_data_iter, test_data_iter = load_replay_buffer(lst_settings[CHOSEN_SETTING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COLLECT ALL FRAMES USED FOR ANALYSIS\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from agent.utils import _preprocess_inputs\n",
    "from agent.voxel_grid import VoxelGrid\n",
    "from arm.utils import get_gripper_render_pose, visualise_voxel_video\n",
    "\n",
    "## First find analysis\n",
    "analyzed_episode = 645\n",
    "\n",
    "lang_goals = set()\n",
    "all_frames = set()\n",
    "for i in range(1000):\n",
    "    # sample from dataset\n",
    "    batch = next(train_data_iter)\n",
    "    lang_goal = batch['lang_goal'][0][0][0]\n",
    "    task, episode, frame, kp = lang_goal.split('-')\n",
    "    if analyzed_episode == int(episode.replace('episode_', '')):\n",
    "        lang_goals.add(lang_goal)\n",
    "        all_frames.add(frame)\n",
    "\n",
    "replay_buffer = natsorted(lang_goals)\n",
    "all_frames = natsorted(all_frames)\n",
    "\n",
    "print(replay_buffer, all_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE VIDEO USING COLLECTED FRAMES\n",
    "\n",
    "import imageio\n",
    "\n",
    "\n",
    "# Video parameters\n",
    "print(settings)\n",
    "if settings.get('fill_replay_setting', None) in [\"crop\", \"standard\"]:\n",
    "    video_output_path = f\"episode_{analyzed_episode}-fill_replay_{settings['fill_replay_setting']}-skip_{settings['demo_augm_n']}-approach_{settings['keypoint_approach']}.mp4\"\n",
    "elif settings.get('fill_replay_setting', None) in [\"uniform\"]:\n",
    "    video_output_path = f\"episode_{analyzed_episode}-fill_replay_{settings['fill_replay_setting']}-sample_{5}-approach_{settings['keypoint_approach']}.mp4\" # NOTE: Hardcoded setting\n",
    "else:\n",
    "    raise ValueError(\"Unkown parameter for settings['fill_replay_setting']. Cannot analyze input data.\")\n",
    "# video_output_path = f\"TEST_episode_{analyzed_episode}-fill_replay_{settings['fill_replay_setting']}-skip_{settings['demo_augm_n']}-approach_only.mp4\"\n",
    "\n",
    "# initialize voxelizer\n",
    "vox_grid = VoxelGrid(\n",
    "    coord_bounds=SCENE_BOUNDS,\n",
    "    voxel_size=VOXEL_SIZES[0],\n",
    "    device=device,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    feature_size=3,\n",
    "    max_num_coords=np.prod([IMAGE_SIZE, IMAGE_SIZE]) * len(settings[\"cameras\"]),\n",
    ")\n",
    "\n",
    "# metric scene bounds\n",
    "bounds = torch.tensor(SCENE_BOUNDS,device=device).unsqueeze(0)\n",
    "\n",
    "# Open a video writer\n",
    "with imageio.get_writer(video_output_path, fps=10) as video_writer:\n",
    "    for analyzed_frame in all_frames: # Loop through all available frames of the replay buffer\n",
    "        frame_language_goals = []\n",
    "        for replay_buffer_i in replay_buffer:\n",
    "            if analyzed_frame in replay_buffer_i:\n",
    "                frame_language_goals.append(replay_buffer_i) # Select ones from replay buffer matching frame number\n",
    "\n",
    "        frame_language_goal = frame_language_goals[0] # Use as foundation\n",
    "        while True:\n",
    "            batch = next(train_data_iter)\n",
    "            lang_goal = batch['lang_goal'][0][0][0]\n",
    "            if lang_goal == frame_language_goal:\n",
    "                batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
    "                # sample\n",
    "                action_trans = batch['trans_action_indicies'][:, -1, :3].int()\n",
    "                action_rot_grip = batch['rot_grip_action_indicies'][:, -1].int()\n",
    "                action_gripper_pose = batch['gripper_pose'][:, -1]\n",
    "                break\n",
    "\n",
    "        if len(frame_language_goals) > 1: # If frame with second keypoint/action available, find!\n",
    "            frame_language_goal = frame_language_goals[1]\n",
    "            while True:\n",
    "                batch_temp = next(train_data_iter)\n",
    "                lang_goal = batch_temp['lang_goal'][0][0][0]\n",
    "                \n",
    "                if lang_goal == frame_language_goal:\n",
    "                    batch_temp = {k: v.to(device) for k, v in batch_temp.items() if type(v) == torch.Tensor}\n",
    "                    next_action_gripper_pose = batch_temp['gripper_pose'][:, -1]\n",
    "                    # sample\n",
    "                    action_gripper_pose = torch.cat([action_gripper_pose, next_action_gripper_pose], dim=0)\n",
    "                    break\n",
    "\n",
    "        # preprocess observations\n",
    "        rgbs_pcds, _ = _preprocess_inputs(batch, settings[\"cameras\"])\n",
    "        pcds = [rp[1] for rp in rgbs_pcds]\n",
    "\n",
    "        # batch_size\n",
    "        bs = rgbs_pcds[0][0].shape[0]\n",
    "\n",
    "        # identity matrix\n",
    "        identity_4x4 = torch.eye(4).unsqueeze(0).repeat(bs, 1, 1).to(device=device)\n",
    "\n",
    "        # flatten observations\n",
    "        pcd_flat = torch.cat([p.permute(0, 2, 3, 1).reshape(bs, -1, 3) for p in pcds], 1)\n",
    "        rgb = [rp[0] for rp in rgbs_pcds] # Loop per camera\n",
    "        feat_size = rgb[0].shape[1]\n",
    "        flat_imag_features = torch.cat(\n",
    "            [p.permute(0, 2, 3, 1).reshape(bs, -1, feat_size) for p in rgb], 1)\n",
    "\n",
    "        # voxelize!\n",
    "        voxel_grid = vox_grid.coords_to_bounding_voxel_grid(pcd_flat,\n",
    "                                                            flat_imag_features,\n",
    "                                                            coord_bounds=bounds)\n",
    "        # swap to channels fist\n",
    "        voxel_grid = voxel_grid.permute(0, 4, 1, 2, 3).detach().cpu().numpy()\n",
    "\n",
    "        # expert action voxel indicies and coord\n",
    "        coords_indicies = action_trans\n",
    "        # discrete to continuous\n",
    "        continuous_trans = action_gripper_pose[:,:3].detach().cpu().numpy()\n",
    "        continuous_quat = action_gripper_pose[:,3:].detach().cpu().numpy()\n",
    "\n",
    "        # gripper visualization pose\n",
    "        voxel_size = 0.045\n",
    "        voxel_scale = voxel_size * 100\n",
    "        gripper_pose_mat = []\n",
    "        for continuous_trans_i, continuous_quat_i in zip(continuous_trans, continuous_quat):\n",
    "            gripper_pose_mat_i = get_gripper_render_pose(voxel_scale,\n",
    "                                                    SCENE_BOUNDS[:3],\n",
    "                                                    continuous_trans_i,\n",
    "                                                    continuous_quat_i)\n",
    "            gripper_pose_mat.append(gripper_pose_mat_i)\n",
    "\n",
    "        gripper_pose_mat = np.squeeze(np.array(gripper_pose_mat))\n",
    "\n",
    "        rendered_img_0 = visualise_voxel_video(voxel_grid[0],\n",
    "                                    None,\n",
    "                                    None,\n",
    "                                    coords_indicies[0],\n",
    "                                    highlight_alpha=1.0,\n",
    "                                    voxel_size=voxel_size,\n",
    "                                    rotation_amount=np.deg2rad(0),\n",
    "                                    render_gripper=True,\n",
    "                                    gripper_pose=gripper_pose_mat,\n",
    "                                    gripper_mesh_scale=voxel_scale,\n",
    "                                    perspective = False)\n",
    "\n",
    "        rendered_img_90 = visualise_voxel_video(voxel_grid[0],\n",
    "                                    None,\n",
    "                                    None,\n",
    "                                    coords_indicies[0],\n",
    "                                    highlight_alpha=1.0,\n",
    "                                    voxel_size=voxel_size,\n",
    "                                    rotation_amount=np.deg2rad(90),\n",
    "                                    render_gripper=True,\n",
    "                                    gripper_pose=gripper_pose_mat,\n",
    "                                    gripper_mesh_scale=voxel_scale,\n",
    "                                    perspective = False)\n",
    "\n",
    "        rendered_img_180 = visualise_voxel_video(voxel_grid[0],\n",
    "                                    None,\n",
    "                                    None,\n",
    "                                    coords_indicies[0],\n",
    "                                    highlight_alpha=1.0,\n",
    "                                    voxel_size=voxel_size,\n",
    "                                    rotation_amount=np.deg2rad(180),\n",
    "                                    render_gripper=True,\n",
    "                                    gripper_pose=gripper_pose_mat,\n",
    "                                    gripper_mesh_scale=voxel_scale,\n",
    "                                    perspective = False)\n",
    "\n",
    "        rendered_img_270 = visualise_voxel_video(voxel_grid[0],\n",
    "                                    None,\n",
    "                                    None,\n",
    "                                    coords_indicies[0],\n",
    "                                    highlight_alpha=1.0,\n",
    "                                    voxel_size=voxel_size,\n",
    "                                    rotation_amount=np.deg2rad(270),\n",
    "                                    render_gripper=True,\n",
    "                                    gripper_pose=gripper_pose_mat,\n",
    "                                    gripper_mesh_scale=voxel_scale,\n",
    "                                    perspective = False)\n",
    "\n",
    "        rendered_img_0_persp = visualise_voxel_video(voxel_grid[0],\n",
    "                                    None,\n",
    "                                    None,\n",
    "                                    coords_indicies[0],\n",
    "                                    highlight_alpha=1.0,\n",
    "                                    voxel_size=voxel_size,\n",
    "                                    rotation_amount=np.deg2rad(0),\n",
    "                                    render_gripper=True,\n",
    "                                    gripper_pose=gripper_pose_mat,\n",
    "                                    gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "        rendered_img_side_persp = visualise_voxel_video(voxel_grid[0],\n",
    "                                    None,\n",
    "                                    None,\n",
    "                                    coords_indicies[0],\n",
    "                                    highlight_alpha=1.0,\n",
    "                                    voxel_size=voxel_size,\n",
    "                                    rotation_amount=np.deg2rad(45),\n",
    "                                    render_gripper=True,\n",
    "                                    gripper_pose=gripper_pose_mat,\n",
    "                                    gripper_mesh_scale=voxel_scale)\n",
    "\n",
    "                        \n",
    "\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        fig.add_subplot(3, 2, 1)\n",
    "        plt.imshow(rendered_img_0)\n",
    "        plt.title(\"0-degree view\")\n",
    "        fig.add_subplot(3, 2, 2)\n",
    "        plt.imshow(rendered_img_90)\n",
    "        plt.title(\"90-degree view\")\n",
    "        fig.add_subplot(3, 2, 3)\n",
    "        plt.imshow(rendered_img_180)\n",
    "        plt.title(\"180-degree view\")\n",
    "        fig.add_subplot(3, 2, 4)\n",
    "        plt.imshow(rendered_img_270)\n",
    "        plt.title(\"270-degree view\")\n",
    "        fig.add_subplot(3, 2, 5)\n",
    "        plt.imshow(rendered_img_0_persp)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"00-degree view\")\n",
    "        fig.add_subplot(3, 2, 6)\n",
    "        plt.imshow(rendered_img_side_persp)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"side view\")\n",
    "\n",
    "        # Add timestamp as text with white font and black background\n",
    "        fig.text(0.02, 0.95, f\"Timestep: {analyzed_frame}\", ha='left', fontsize=16, color='white', weight='bold',\n",
    "                 bbox=dict(facecolor='black', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "\n",
    "        # Convert the matplotlib figure to a NumPy array\n",
    "        fig.canvas.draw()\n",
    "        img_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        \n",
    "        video_writer.append_data(img_array)  # Add frame to video\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "print(f\"Video saved as {video_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
